{"cells":[{"cell_type":"markdown","metadata":{"id":"SceK5n2PKVMV"},"source":["### Extracting embeddings"]},{"cell_type":"markdown","metadata":{"id":"hlD6nxUKKRHH"},"source":["Importing packages."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716635887387,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"ZbaSGqT6PzvE"},"outputs":[],"source":["import os\n","import librosa\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model"]},{"cell_type":"markdown","metadata":{"id":"AlYZ-tjJKN-e"},"source":["Mounting the data from drive"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716635888353,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"umZPI89LQB1y","outputId":"867699b1-de20-4328-a31e-0922829a7056"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n"]}],"source":["directory_path = '/content/drive/MyDrive/enhancing_speaker_recognition_evaluation/data'\n","\n","print(len(os.listdir(directory_path)))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["58\n"]}],"source":["directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers\")\n","\n","print(len(os.listdir(directory_path)))"]},{"cell_type":"markdown","metadata":{"id":"hZpXM5PeKHwv"},"source":["Defining device."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716635889877,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"zIP9ARAGQpWZ","outputId":"41b0bc89-6d03-4971-ab87-b86f50ef009d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"zCWEomYBJQ8g"},"source":["Now we're extracting the vector representations of the audio files in different stages of the encoder."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370,"referenced_widgets":["91a74ab240a54735a18da7e1cf8cde5a","922f60e005c242a29e00de5b56d9eff2","0c0b00113d9d4d17a9324aac1a1b7812","73cfdb8db08f4c62a0199b21da0890d6","8ecdc35e441c4e0b9515446383caf7c1","a93d7b6215cb42e790b2bb08c7fa910c","5ee8cc88f6f8429da17d627c5bcfbaee","f8512d416bd84a0e8ccff509c4aa998b","dfa5aba7c0b84a5bad48b6505ecb5246","6e9ae6e783a04f1492df54d198e9a844","300ac59f66cd4ba19a8e05558766d56e","5f89d5eddaa542498f9e22f51dd0379e","8f00321cd01948d081f90d5fb0399ac7","6c38047dff0b49c3917b73465362d015","202535d409cb4df4b9f36f79a56a444f","402a6e29fa5d4216842716be09bdec0b","7f8c3d92d1cb4e1db403794993823d65","362b4b76ecf74aefbd85b27a41daf87d","9d2f7f474e9b491ca0172dc850ab005d","862426b3ac80408584610f2067df9c55","5ed129a6b9834f12a8c8b2155e09ca23","a27dff3aa8a34489bd6cd47ebabc7d27","dd399b2c6d3a4d4ab667b6c91753f6e0","93dc414ffd09438db90797e3d50c5883","9e5d01709684425988f881a95f1ff96e","92d3680ff8ff49c0a3f21a13201bfecc","08103e90e897474fa82bf032221e11fb","8497181edc074e65b048e1f70a664a46","13dcfcdb1bb548be992630d634303121","f62ab26c3d144c7c850a8eb6e35dfc64","b147449ceb4a4f289a7fac9283e497f4","0c11e4c65a9c42deb48b73e70e421873","d1f43d48550f49cc89cb1305ad3120d6"]},"executionInfo":{"elapsed":809506,"status":"ok","timestamp":1716636716494,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"L15GvisEPtQl","outputId":"0eff16ec-5591-437a-f5d3-bcd9bd7dc698"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/50 [00:00<?, ?it/s]/home/rag/base_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n","100%|██████████| 50/50 [00:10<00:00,  4.94it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.24it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.75it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.41it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.84it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.47it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.86it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.87it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.56it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.90it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.81it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.89it/s]\n","100%|██████████| 50/50 [00:02<00:00, 16.70it/s]\n","100%|██████████| 50/50 [00:02<00:00, 17.21it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.42it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.98it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.90it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.98it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.05it/s]\n","100%|██████████| 50/50 [00:02<00:00, 17.58it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.71it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.89it/s]\n","100%|██████████| 50/50 [00:02<00:00, 21.34it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.26it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.97it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.28it/s]\n","100%|██████████| 50/50 [00:03<00:00, 16.04it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.73it/s]\n","100%|██████████| 50/50 [00:02<00:00, 16.68it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.86it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.90it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.17it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.99it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.04it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.84it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.96it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.48it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.32it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.54it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.14it/s]\n","100%|██████████| 50/50 [00:03<00:00, 16.63it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.96it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.83it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.67it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.17it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.83it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.30it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.03it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.73it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.54it/s]\n"]}],"source":["import os\n","import librosa\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n","\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", output_hidden_states=True)\n","model.to(device)\n","\n","def check_directories_exist(directory, layer_indices):\n","    \"\"\"Prüft, ob die benötigten Verzeichnisse für jede Schicht bereits existieren.\"\"\"\n","    all_exist = True\n","    for index in layer_indices:\n","        layer_dir = os.path.join(directory, f\"layer_{index}\")\n","        if not os.path.exists(layer_dir):\n","            all_exist = False\n","            break\n","    return all_exist\n","\n","def load_audio_files(directory, layer_indices=[-1]):\n","    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n","    for filename in tqdm(os.listdir(directory)):\n","        if filename.endswith(\".mp3\"):\n","            file_path = os.path.join(directory, filename)\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            input_values = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=sr).input_values\n","            input_values = input_values.to(device)\n","            with torch.no_grad():\n","                outputs = model(input_values)\n","                for index in layer_indices:\n","                    hidden_states = outputs.hidden_states[index]\n","                    # creating sub directory for each layer in speaker directory\n","                    layer_dir = os.path.join(directory, f\"layer_{index}\")\n","                    os.makedirs(layer_dir, exist_ok=True)\n","                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n","                    np.save(save_path, hidden_states.cpu().numpy())\n","\n","def process_audio_directory(base_directory, layer_indices=range(25)):\n","    \"\"\"Verarbeitet Audio-Dateien in den angegebenen Verzeichnissen, falls die Ziellayer-Verzeichnisse noch nicht existieren.\"\"\"\n","    for d in os.listdir(base_directory):\n","        dir_path = os.path.join(base_directory, d)\n","        if os.path.isdir(dir_path) and not check_directories_exist(dir_path, layer_indices):\n","            load_audio_files(dir_path, layer_indices)\n","\n","directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_xls_r_300m\")\n","\n","process_audio_directory(directory_path)"]},{"cell_type":"markdown","metadata":{},"source":["# fine tuning von XLS R"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["on the voxceleb dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from transformers import Wav2Vec2ForCTC, Trainer, TrainingArguments\n","from transformers import Wav2Vec2FeatureExtractor\n","from torch.nn.functional import cross_entropy\n","\n","# Load dataset\n","dataset = load_dataset(\"voxceleb1\")\n","\n","# Prepare feature extractor\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","\n","# Define dataset preprocessing\n","def prepare_dataset(batch):\n","    # Process audio files\n","    audio = batch[\"audio\"]\n","    inputs = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n","    batch[\"input_values\"] = inputs.input_values.squeeze(0)\n","    batch[\"labels\"] = batch[\"speaker_id\"]\n","    return batch\n","\n","# Apply preprocessing\n","dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], batch_size=8, num_proc=4, batched=True)\n","\n","# Model\n","model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", num_labels=dataset[\"train\"].features[\"speaker_id\"].num_classes)\n","\n","# Define Training Arguments\n","training_args = TrainingArguments(\n","  output_dir=\"./results\",\n","  group_by_length=True,\n","  per_device_train_batch_shift_size=16,\n","  evaluation_strategy=\"steps\",\n","  num_train_epochs=3,\n","  save_steps=500,\n","  eval_steps=500,\n","  logging_steps=10,\n","  learning_rate=1e-4,\n","  save_total_limit=2,\n",")\n","\n","# Trainer with a custom compute_loss function\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = cross_entropy(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Define Trainer\n","trainer = CustomTrainer(\n","  model=model,\n","  args=training_args,\n","  train_dataset=dataset[\"train\"],\n","  eval_dataset=dataset[\"test\"],\n","  tokenizer=feature_extractor,\n",")\n","\n","# Start training\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{},"source":["fine tuning on our own curated dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import torch\n","import librosa\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model, Trainer, TrainingArguments, TrainerCallback, Wav2Vec2FeatureExtractor\n","import math\n","from datasets import load_metric\n","from datetime import datetime\n","import torch.nn as nn\n","\n","# Load the processor\n","processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","\n","# Define the custom dataset class using pandas\n","class LocalAudioDataset(Dataset):\n","    def __init__(self, csv_file, processor, subset, noise_factor=0.0):\n","        self.processor = processor\n","        self.data = pd.read_csv(csv_file)\n","        self.data = self.data[self.data['subset'] == subset]\n","        self.speaker_ids = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n","        self.data['label'] = self.data['label'].map(self.speaker_ids)\n","        self.noise_factor = noise_factor\n","        \n","        print(f\"Loaded {len(self.speaker_ids)} speakers: {self.speaker_ids}\")\n","        print(f\"Total files in {subset}: {len(self.data)}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx, retry_count=0):\n","        file_path = self.data.iloc[idx]['path']\n","        label = self.data.iloc[idx]['label']\n","        \n","        try:\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            audio = librosa.to_mono(audio)\n","            audio = self._pad_or_truncate(audio, max_length=16000)\n","            if self.noise_factor > 0:\n","                audio = self._add_noise(audio)\n","            input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.squeeze(0)\n","            return {\"input_values\": input_values, \"labels\": label}\n","        except Exception as e:\n","            if retry_count < 3:  # Retry up to 3 times\n","                return self.__getitem__((idx + 1) % len(self), retry_count + 1)\n","            else:\n","                print(f\"Error loading {file_path}: {e}\", file=sys.stderr)\n","                raise e  # Raise exception if retry limit is reached\n","\n","    def _pad_or_truncate(self, audio, max_length):\n","        if len(audio) < max_length:\n","            pad_size = max_length - len(audio)\n","            audio = np.pad(audio, (0, pad_size), 'constant', constant_values=(0, 0))\n","        else:\n","            audio = audio[:max_length]\n","        return audio\n","\n","    def _add_noise(self, audio):\n","        noise = np.random.randn(len(audio))\n","        augmented_audio = audio + self.noise_factor * noise\n","        augmented_audio = augmented_audio.astype(type(audio[0]))\n","        return augmented_audio\n","\n","# Paths to dataset CSV file\n","csv_file = 'dataset_large.csv'\n","train_dataset = LocalAudioDataset(csv_file, processor, 'train')\n","validate_dataset = LocalAudioDataset(csv_file, processor, 'validate')\n","test_dataset = LocalAudioDataset(csv_file, processor, 'test')\n","\n","num_speakers = len(train_dataset.speaker_ids)\n","print(f\"Number of unique speakers: {num_speakers}\")\n","\n","print(f\"Labels in train dataset: {train_dataset.data['label'].tolist()}\")\n","print(f\"Labels in test dataset: {test_dataset.data['label'].tolist()}\")\n","\n","# Define a custom classification head on top of the base Wav2Vec2 model\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    def __init__(self, config, num_labels):\n","        super().__init__()\n","        self.dropout = nn.Dropout(config.hidden_dropout)\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take the mean of the hidden states of the first token\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","# Define the full model by combining Wav2Vec2Model with the classification head\n","class Wav2Vec2ForCustomClassification(nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","        self.classifier = Wav2Vec2ClassificationHead(self.wav2vec2.config, num_labels)\n","\n","    def forward(self, input_values, attention_mask=None, labels=None):\n","        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state\n","        logits = self.classifier(hidden_states)\n","        \n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n","        \n","        return (loss, logits) if loss is not None else logits\n","\n","# Instantiate the model with the custom classification head\n","model = Wav2Vec2ForCustomClassification(num_labels=num_speakers)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","model = model.to(device)\n","\n","def validate_labels(dataset):\n","    for item in dataset:\n","        label = item['labels']\n","        if label >= num_speakers or label < 0:\n","            print(f\"Invalid label {label} for item: {item}\")\n","            raise ValueError(f\"Invalid label {label} found in dataset.\")\n","    print(\"All labels are valid.\")\n","\n","validate_labels(train_dataset)\n","validate_labels(validate_dataset)\n","validate_labels(test_dataset)\n","\n","batch_size = 8\n","steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n","logging_steps = steps_per_epoch // 5\n","eval_steps = steps_per_epoch // 5\n","\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return accuracy_metric.compute(predictions=predictions, references=labels)\n","\n","log_dir = \"/home/rag/experimental_trial/results/training_logs\"\n","os.makedirs(log_dir, exist_ok=True)\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","log_file = os.path.join(log_dir, f\"training_logxlsr_finetuning_{timestamp}.csv\")\n","with open(log_file, \"w\") as f:\n","    f.write(\"Timestamp,Step,Training Loss,Validation Loss,Accuracy\\n\")\n","\n","class SaveMetricsCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            with open(log_file, \"a\") as f:\n","                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","                step = state.global_step\n","                training_loss = logs.get(\"loss\", \"\")\n","                validation_loss = logs.get(\"eval_loss\", \"\")\n","                accuracy = logs.get(\"eval_accuracy\", \"\")\n","                f.write(f\"{timestamp},{step},{training_loss},{validation_loss},{accuracy}\\n\")\n","\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, early_stopping_patience=100, early_stopping_threshold=0.0):\n","        self.early_stopping_patience = early_stopping_patience\n","        self.early_stopping_threshold = early_stopping_threshold\n","        self.best_metric = None\n","        self.patience_counter = 0\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        metric = kwargs.get(\"metrics\", {}).get(\"eval_loss\")\n","        if metric is None:\n","            return\n","        \n","        if self.best_metric is None or metric < self.best_metric - self.early_stopping_threshold:\n","            self.best_metric = metric\n","            self.patience_counter = 0\n","        else:\n","            self.patience_counter += 1\n","        \n","        if self.patience_counter >= self.early_stopping_patience:\n","            print(f\"Early stopping at step {state.global_step}\")\n","            control.should_training_stop = True\n","\n","# Ensure 'no_cuda' parameter aligns with device availability\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    group_by_length=True,\n","    per_device_train_batch_size=batch_size,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=100,\n","    save_steps=logging_steps,\n","    eval_steps=eval_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=5e-6,\n","    save_total_limit=2,\n","    no_cuda=not torch.cuda.is_available(),\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,  # lower eval_loss is better\n","    save_strategy=\"steps\"  # Save checkpoints every `save_steps`\n",")\n","\n","# Add early stopping callback to the trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validate_dataset,\n","    tokenizer=processor,\n","    compute_metrics=compute_metrics,\n","    callbacks=[SaveMetricsCallback(), EarlyStoppingCallback()]  # Include early stopping\n",")\n","\n","# Train and evaluate\n","trainer.train()\n","\n","metrics = trainer.evaluate(test_dataset)\n","\n","print(f\"Test set evaluation metrics: {metrics}\")\n","print(\"Training and evaluation completed successfully!\")\n","\n","best_model_dir = \"./results/best_model_xlsr_finetuning\"\n","os.makedirs(best_model_dir, exist_ok=True)\n","\n","trainer.save_model(best_model_dir)\n","processor.save_pretrained(best_model_dir)\n","\n","print(f\"Best model saved to {best_model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["now we extract hidden states"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2FeatureExtractor\n","from tqdm import tqdm\n","import librosa\n","from safetensors.torch import load_file as safe_load\n","from torch import nn\n","\n","# Initialize the processor and model for xlsr\n","processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","finetuned_model_path = \"/home/rag/experimental_trial/results/best_model_xlsr_finetuning/model.safetensors\"\n","\n","# Define a custom classification head on top of the base Wav2Vec2 model\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    def __init__(self, config, num_labels):\n","        super().__init__()\n","        self.dropout = nn.Dropout(config.hidden_dropout)\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take the mean of the hidden states of the first token\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","# Define the full model by combining Wav2Vec2Model with the classification head\n","class Wav2Vec2ForCustomClassification(nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", output_hidden_states=True)\n","        self.classifier = Wav2Vec2ClassificationHead(self.wav2vec2.config, num_labels)\n","\n","    def forward(self, input_values, attention_mask=None, labels=None):\n","        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n","        hidden_states = outputs.hidden_states\n","        logits = self.classifier(hidden_states[-1])\n","        \n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n","        \n","        return (loss, logits, hidden_states) if loss is not None else (logits, hidden_states)\n","\n","# Instantiate the model with the custom classification head\n","model = Wav2Vec2ForCustomClassification(num_labels=111)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","state_dict = safe_load(finetuned_model_path)\n","model.load_state_dict(state_dict)\n","model.to(device)\n","\n","def check_directories_exist(directory, layer_indices):\n","    \"\"\"Prüft, ob die benötigten Verzeichnisse für jede Schicht bereits existieren.\"\"\"\n","    all_exist = True\n","    for index in layer_indices:\n","        layer_dir = os.path.join(directory, f\"layer_{index}\")\n","        if not os.path.exists(layer_dir):\n","            all_exist = False\n","            break\n","    return all_exist\n","\n","def load_audio_files(directory, layer_indices=[-1]):\n","    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n","    for filename in tqdm(os.listdir(directory)):\n","        if filename.endswith(\".mp3\"):\n","            file_path = os.path.join(directory, filename)\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n","            input_values = inputs[\"input_values\"].to(device)\n","            \n","            with torch.no_grad():\n","                logits, hidden_states = model(input_values)\n","                for index in layer_indices:\n","                    hidden_state = hidden_states[index]\n","                    # creating sub directory for each layer in speaker directory\n","                    layer_dir = os.path.join(directory, f\"layer_{index}\")\n","                    os.makedirs(layer_dir, exist_ok=True)\n","                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n","                    np.save(save_path, hidden_state.cpu().numpy())\n","\n","def process_audio_directory(base_directory, layer_indices=range(25)):\n","    \"\"\"Verarbeitet Audio-Dateien in den angegebenen Verzeichnissen, falls die Ziellayer-Verzeichnisse noch nicht existieren.\"\"\"\n","    for d in os.listdir(base_directory):\n","        dir_path = os.path.join(base_directory, d)\n","        if os.path.isdir(dir_path) and not check_directories_exist(dir_path, layer_indices):\n","            load_audio_files(dir_path, layer_indices)\n","\n","directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_xlrs_finetuned\")\n","\n","process_audio_directory(directory_path)\n"]},{"cell_type":"markdown","metadata":{},"source":["# now we use optuna to optimize the number of parameter used "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import sys\n","import torch\n","import librosa\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model, Trainer, TrainingArguments, TrainerCallback, Wav2Vec2FeatureExtractor, Wav2Vec2Config\n","import math\n","from datasets import load_metric\n","from datetime import datetime\n","import torch.nn as nn\n","import optuna\n","\n","# Load the processor\n","processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","\n","# Define the custom dataset class using pandas\n","class LocalAudioDataset(Dataset):\n","    def __init__(self, csv_file, processor, subset, noise_factor=0.0):\n","        self.processor = processor\n","        self.data = pd.read_csv(csv_file)\n","        self.data = self.data[self.data['subset'] == subset]\n","        self.speaker_ids = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n","        self.data['label'] = self.data['label'].map(self.speaker_ids)\n","        self.noise_factor = noise_factor\n","        \n","        print(f\"Loaded {len(self.speaker_ids)} speakers: {self.speaker_ids}\")\n","        print(f\"Total files in {subset}: {len(self.data)}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx, retry_count=0):\n","        file_path = self.data.iloc[idx]['path']\n","        label = self.data.iloc[idx]['label']\n","        \n","        try:\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            audio = librosa.to_mono(audio)\n","            audio = self._pad_or_truncate(audio, max_length=16000)\n","            if self.noise_factor > 0:\n","                audio = self._add_noise(audio)\n","            input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.squeeze(0)\n","            return {\"input_values\": input_values, \"labels\": label}\n","        except Exception as e:\n","            if retry_count < 3:  # Retry up to 3 times\n","                return self.__getitem__((idx + 1) % len(self), retry_count + 1)\n","            else:\n","                print(f\"Error loading {file_path}: {e}\", file=sys.stderr)\n","                raise e  # Raise exception if retry limit is reached\n","\n","    def _pad_or_truncate(self, audio, max_length):\n","        if len(audio) < max_length:\n","            pad_size = max_length - len(audio)\n","            audio = np.pad(audio, (0, pad_size), 'constant', constant_values=(0, 0))\n","        else:\n","            audio = audio[:max_length]\n","        return audio\n","\n","    def _add_noise(self, audio):\n","        noise = np.random.randn(len(audio))\n","        augmented_audio = audio + self.noise_factor * noise\n","        augmented_audio = augmented_audio.astype(type(audio[0]))\n","        return augmented_audio\n","\n","# Paths to dataset CSV file\n","csv_file = 'dataset_large.csv'\n","train_dataset = LocalAudioDataset(csv_file, processor, 'train')\n","validate_dataset = LocalAudioDataset(csv_file, processor, 'validate')\n","test_dataset = LocalAudioDataset(csv_file, processor, 'test')\n","\n","num_speakers = len(train_dataset.speaker_ids)\n","print(f\"Number of unique speakers: {num_speakers}\")\n","\n","print(f\"Labels in train dataset: {train_dataset.data['label'].tolist()}\")\n","print(f\"Labels in test dataset: {test_dataset.data['label'].tolist()}\")\n","\n","# Define a custom classification head on top of the base Wav2Vec2 model\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    def __init__(self, config, num_labels):\n","        super().__init__()\n","        self.dropout = nn.Dropout(config.hidden_dropout)\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take the mean of the hidden states of the first token\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","# Define the full model by combining Wav2Vec2Model with the classification head\n","class Wav2Vec2ForCustomClassification(nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","        self.classifier = Wav2Vec2ClassificationHead(self.wav2vec2.config, num_labels)\n","\n","    def forward(self, input_values, attention_mask=None, labels=None):\n","        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state\n","        logits = self.classifier(hidden_states)\n","        \n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n","        \n","        return (loss, logits) if loss is not None else logits\n","\n","# Instantiate the model with the custom classification head\n","model = Wav2Vec2ForCustomClassification(num_labels=num_speakers)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","model = model.to(device)\n","\n","def validate_labels(dataset):\n","    for item in dataset:\n","        label = item['labels']\n","        if label >= num_speakers or label < 0:\n","            print(f\"Invalid label {label} for item: {item}\")\n","            raise ValueError(f\"Invalid label {label} found in dataset.\")\n","    print(\"All labels are valid.\")\n","\n","validate_labels(train_dataset)\n","validate_labels(validate_dataset)\n","validate_labels(test_dataset)\n","\n","batch_size = 8\n","steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n","logging_steps = steps_per_epoch // 5\n","eval_steps = steps_per_epoch // 5\n","\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return accuracy_metric.compute(predictions=predictions, references=labels)\n","\n","log_dir = \"/home/rag/experimental_trial/results/training_logs\"\n","os.makedirs(log_dir, exist_ok=True)\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","log_file = os.path.join(log_dir, f\"training_logxlsr_finetuning_optimizing_layers_{timestamp}.csv\")\n","with open(log_file, \"w\") as f:\n","    f.write(\"Timestamp,Step,Training Loss,Validation Loss,Accuracy\\n\")\n","\n","class SaveMetricsCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            with open(log_file, \"a\") as f:\n","                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","                step = state.global_step\n","                training_loss = logs.get(\"loss\", \"\")\n","                validation_loss = logs.get(\"eval_loss\", \"\")\n","                accuracy = logs.get(\"eval_accuracy\", \"\")\n","                f.write(f\"{timestamp},{step},{training_loss},{validation_loss},{accuracy}\\n\")\n","\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, early_stopping_patience=100, early_stopping_threshold=0.0):\n","        self.early_stopping_patience = early_stopping_patience\n","        self.early_stopping_threshold = early_stopping_threshold\n","        self.best_metric = None\n","        self.patience_counter = 0\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        metric = kwargs.get(\"metrics\", {}).get(\"eval_loss\")\n","        if metric is None:\n","            return\n","        \n","        if self.best_metric is None or metric < self.best_metric - self.early_stopping_threshold:\n","            self.best_metric = metric\n","            self.patience_counter = 0\n","        else:\n","            self.patience_counter += 1\n","        \n","        if self.patience_counter >= self.early_stopping_patience:\n","            print(f\"Early stopping at step {state.global_step}\")\n","            control.should_training_stop = True\n","\n","# Define the Optuna objective function\n","def objective(trial):\n","    # Suggest the number of layers\n","    num_layers = trial.suggest_int('num_layers', 1, 24)\n","\n","    # Load the model configuration with the suggested number of layers\n","    config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", num_labels=num_speakers)\n","    config.num_hidden_layers = num_layers\n","\n","    # Instantiate the model with the custom classification head\n","    model = Wav2Vec2ForCustomClassification(num_labels=num_speakers)\n","    model.wav2vec2.encoder.layers = model.wav2vec2.encoder.layers[:num_layers]\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    # Ensure 'no_cuda' parameter aligns with device availability\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",\n","        group_by_length=True,\n","        per_device_train_batch_size=batch_size,\n","        evaluation_strategy=\"steps\",\n","        num_train_epochs=10,  # Use 10 epochs\n","        save_steps=logging_steps,\n","        eval_steps=eval_steps,\n","        logging_steps=logging_steps,\n","        learning_rate=5e-6,  # Fixed learning rate\n","        save_total_limit=2,\n","        no_cuda=not torch.cuda.is_available(),\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"eval_loss\",\n","        greater_is_better=False,\n","        save_strategy=\"steps\"\n","    )\n","\n","    # Add early stopping callback to the trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=validate_dataset,\n","        tokenizer=processor,\n","        compute_metrics=compute_metrics,\n","        callbacks=[SaveMetricsCallback(), EarlyStoppingCallback()]\n","    )\n","\n","    # Train and evaluate\n","    trainer.train()\n","    \n","    metrics = trainer.evaluate(validate_dataset)\n","    \n","    # Return the evaluation loss for Optuna to minimize\n","    return metrics[\"eval_loss\"]\n","\n","# Create an Optuna study and optimize the objective function\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=25)\n","\n","# Print the best hyperparameters found\n","print(f\"Best hyperparameters: {study.best_params}\")\n","\n","# Save the best model and processor\n","best_model_dir = \"./results/best_model_xlsr_finetuning_optuna_layer_optimized\"\n","os.makedirs(best_model_dir, exist_ok=True)\n","\n","trainer.save_model(best_model_dir)\n","processor.save_pretrained(best_model_dir)\n","\n","print(f\"Best model saved to {best_model_dir}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN+yvvv6PVerTjdhNTDh9OE","gpuType":"T4","mount_file_id":"1WM9o59Rmu2-KuQu0mfCOrhA_e8D1TdAj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08103e90e897474fa82bf032221e11fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c0b00113d9d4d17a9324aac1a1b7812":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8512d416bd84a0e8ccff509c4aa998b","max":212,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfa5aba7c0b84a5bad48b6505ecb5246","value":212}},"0c11e4c65a9c42deb48b73e70e421873":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13dcfcdb1bb548be992630d634303121":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"202535d409cb4df4b9f36f79a56a444f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ed129a6b9834f12a8c8b2155e09ca23","placeholder":"​","style":"IPY_MODEL_a27dff3aa8a34489bd6cd47ebabc7d27","value":" 1.57k/1.57k [00:00&lt;00:00, 90.4kB/s]"}},"300ac59f66cd4ba19a8e05558766d56e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"362b4b76ecf74aefbd85b27a41daf87d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"402a6e29fa5d4216842716be09bdec0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ed129a6b9834f12a8c8b2155e09ca23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee8cc88f6f8429da17d627c5bcfbaee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f89d5eddaa542498f9e22f51dd0379e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f00321cd01948d081f90d5fb0399ac7","IPY_MODEL_6c38047dff0b49c3917b73465362d015","IPY_MODEL_202535d409cb4df4b9f36f79a56a444f"],"layout":"IPY_MODEL_402a6e29fa5d4216842716be09bdec0b"}},"6c38047dff0b49c3917b73465362d015":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2f7f474e9b491ca0172dc850ab005d","max":1568,"min":0,"orientation":"horizontal","style":"IPY_MODEL_862426b3ac80408584610f2067df9c55","value":1568}},"6e9ae6e783a04f1492df54d198e9a844":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73cfdb8db08f4c62a0199b21da0890d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e9ae6e783a04f1492df54d198e9a844","placeholder":"​","style":"IPY_MODEL_300ac59f66cd4ba19a8e05558766d56e","value":" 212/212 [00:00&lt;00:00, 15.3kB/s]"}},"7f8c3d92d1cb4e1db403794993823d65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8497181edc074e65b048e1f70a664a46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862426b3ac80408584610f2067df9c55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ecdc35e441c4e0b9515446383caf7c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f00321cd01948d081f90d5fb0399ac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f8c3d92d1cb4e1db403794993823d65","placeholder":"​","style":"IPY_MODEL_362b4b76ecf74aefbd85b27a41daf87d","value":"config.json: 100%"}},"91a74ab240a54735a18da7e1cf8cde5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_922f60e005c242a29e00de5b56d9eff2","IPY_MODEL_0c0b00113d9d4d17a9324aac1a1b7812","IPY_MODEL_73cfdb8db08f4c62a0199b21da0890d6"],"layout":"IPY_MODEL_8ecdc35e441c4e0b9515446383caf7c1"}},"922f60e005c242a29e00de5b56d9eff2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a93d7b6215cb42e790b2bb08c7fa910c","placeholder":"​","style":"IPY_MODEL_5ee8cc88f6f8429da17d627c5bcfbaee","value":"preprocessor_config.json: 100%"}},"92d3680ff8ff49c0a3f21a13201bfecc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c11e4c65a9c42deb48b73e70e421873","placeholder":"​","style":"IPY_MODEL_d1f43d48550f49cc89cb1305ad3120d6","value":" 1.27G/1.27G [00:12&lt;00:00, 167MB/s]"}},"93dc414ffd09438db90797e3d50c5883":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8497181edc074e65b048e1f70a664a46","placeholder":"​","style":"IPY_MODEL_13dcfcdb1bb548be992630d634303121","value":"pytorch_model.bin: 100%"}},"9d2f7f474e9b491ca0172dc850ab005d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e5d01709684425988f881a95f1ff96e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f62ab26c3d144c7c850a8eb6e35dfc64","max":1269737156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b147449ceb4a4f289a7fac9283e497f4","value":1269737156}},"a27dff3aa8a34489bd6cd47ebabc7d27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a93d7b6215cb42e790b2bb08c7fa910c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b147449ceb4a4f289a7fac9283e497f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1f43d48550f49cc89cb1305ad3120d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd399b2c6d3a4d4ab667b6c91753f6e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93dc414ffd09438db90797e3d50c5883","IPY_MODEL_9e5d01709684425988f881a95f1ff96e","IPY_MODEL_92d3680ff8ff49c0a3f21a13201bfecc"],"layout":"IPY_MODEL_08103e90e897474fa82bf032221e11fb"}},"dfa5aba7c0b84a5bad48b6505ecb5246":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f62ab26c3d144c7c850a8eb6e35dfc64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8512d416bd84a0e8ccff509c4aa998b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
