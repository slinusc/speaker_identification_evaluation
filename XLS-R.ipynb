{"cells":[{"cell_type":"markdown","metadata":{"id":"SceK5n2PKVMV"},"source":["### Extracting embeddings"]},{"cell_type":"markdown","metadata":{"id":"hlD6nxUKKRHH"},"source":["Importing packages."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716635887387,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"ZbaSGqT6PzvE"},"outputs":[],"source":["import os\n","import librosa\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model"]},{"cell_type":"markdown","metadata":{"id":"AlYZ-tjJKN-e"},"source":["Mounting the data from drive"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1716635888353,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"umZPI89LQB1y","outputId":"867699b1-de20-4328-a31e-0922829a7056"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n"]}],"source":["directory_path = '/content/drive/MyDrive/enhancing_speaker_recognition_evaluation/data'\n","\n","print(len(os.listdir(directory_path)))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["58\n"]}],"source":["directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers\")\n","\n","print(len(os.listdir(directory_path)))"]},{"cell_type":"markdown","metadata":{"id":"hZpXM5PeKHwv"},"source":["Defining device."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1716635889877,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"zIP9ARAGQpWZ","outputId":"41b0bc89-6d03-4971-ab87-b86f50ef009d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"zCWEomYBJQ8g"},"source":["Now we're extracting the vector representations of the audio files in different stages of the encoder."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370,"referenced_widgets":["91a74ab240a54735a18da7e1cf8cde5a","922f60e005c242a29e00de5b56d9eff2","0c0b00113d9d4d17a9324aac1a1b7812","73cfdb8db08f4c62a0199b21da0890d6","8ecdc35e441c4e0b9515446383caf7c1","a93d7b6215cb42e790b2bb08c7fa910c","5ee8cc88f6f8429da17d627c5bcfbaee","f8512d416bd84a0e8ccff509c4aa998b","dfa5aba7c0b84a5bad48b6505ecb5246","6e9ae6e783a04f1492df54d198e9a844","300ac59f66cd4ba19a8e05558766d56e","5f89d5eddaa542498f9e22f51dd0379e","8f00321cd01948d081f90d5fb0399ac7","6c38047dff0b49c3917b73465362d015","202535d409cb4df4b9f36f79a56a444f","402a6e29fa5d4216842716be09bdec0b","7f8c3d92d1cb4e1db403794993823d65","362b4b76ecf74aefbd85b27a41daf87d","9d2f7f474e9b491ca0172dc850ab005d","862426b3ac80408584610f2067df9c55","5ed129a6b9834f12a8c8b2155e09ca23","a27dff3aa8a34489bd6cd47ebabc7d27","dd399b2c6d3a4d4ab667b6c91753f6e0","93dc414ffd09438db90797e3d50c5883","9e5d01709684425988f881a95f1ff96e","92d3680ff8ff49c0a3f21a13201bfecc","08103e90e897474fa82bf032221e11fb","8497181edc074e65b048e1f70a664a46","13dcfcdb1bb548be992630d634303121","f62ab26c3d144c7c850a8eb6e35dfc64","b147449ceb4a4f289a7fac9283e497f4","0c11e4c65a9c42deb48b73e70e421873","d1f43d48550f49cc89cb1305ad3120d6"]},"executionInfo":{"elapsed":809506,"status":"ok","timestamp":1716636716494,"user":{"displayName":"Linus Stuhlmann","userId":"07279173608860907824"},"user_tz":-120},"id":"L15GvisEPtQl","outputId":"0eff16ec-5591-437a-f5d3-bcd9bd7dc698"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/50 [00:00<?, ?it/s]/home/rag/base_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n","100%|██████████| 50/50 [00:10<00:00,  4.94it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.24it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.75it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.41it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.84it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.47it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.86it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.87it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.56it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.90it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.81it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.89it/s]\n","100%|██████████| 50/50 [00:02<00:00, 16.70it/s]\n","100%|██████████| 50/50 [00:02<00:00, 17.21it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.42it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.98it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.90it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.98it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.05it/s]\n","100%|██████████| 50/50 [00:02<00:00, 17.58it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.71it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.89it/s]\n","100%|██████████| 50/50 [00:02<00:00, 21.34it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.26it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.97it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.28it/s]\n","100%|██████████| 50/50 [00:03<00:00, 16.04it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.73it/s]\n","100%|██████████| 50/50 [00:02<00:00, 16.68it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.86it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.90it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.17it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.99it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.04it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.84it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.96it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.48it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.32it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.54it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.14it/s]\n","100%|██████████| 50/50 [00:03<00:00, 16.63it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.96it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.83it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.67it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.17it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.83it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.30it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.03it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.73it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.54it/s]\n"]}],"source":["import os\n","import librosa\n","import torch\n","from tqdm import tqdm\n","import numpy as np\n","from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n","\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", output_hidden_states=True)\n","model.to(device)\n","\n","def check_directories_exist(directory, layer_indices):\n","    \"\"\"Prüft, ob die benötigten Verzeichnisse für jede Schicht bereits existieren.\"\"\"\n","    all_exist = True\n","    for index in layer_indices:\n","        layer_dir = os.path.join(directory, f\"layer_{index}\")\n","        if not os.path.exists(layer_dir):\n","            all_exist = False\n","            break\n","    return all_exist\n","\n","def load_audio_files(directory, layer_indices=[-1]):\n","    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n","    for filename in tqdm(os.listdir(directory)):\n","        if filename.endswith(\".mp3\"):\n","            file_path = os.path.join(directory, filename)\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            input_values = feature_extractor(audio, return_tensors=\"pt\", sampling_rate=sr).input_values\n","            input_values = input_values.to(device)\n","            with torch.no_grad():\n","                outputs = model(input_values)\n","                for index in layer_indices:\n","                    hidden_states = outputs.hidden_states[index]\n","                    # creating sub directory for each layer in speaker directory\n","                    layer_dir = os.path.join(directory, f\"layer_{index}\")\n","                    os.makedirs(layer_dir, exist_ok=True)\n","                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n","                    np.save(save_path, hidden_states.cpu().numpy())\n","\n","def process_audio_directory(base_directory, layer_indices=range(25)):\n","    \"\"\"Verarbeitet Audio-Dateien in den angegebenen Verzeichnissen, falls die Ziellayer-Verzeichnisse noch nicht existieren.\"\"\"\n","    for d in os.listdir(base_directory):\n","        dir_path = os.path.join(base_directory, d)\n","        if os.path.isdir(dir_path) and not check_directories_exist(dir_path, layer_indices):\n","            load_audio_files(dir_path, layer_indices)\n","\n","directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_xls_r_300m\")\n","\n","process_audio_directory(directory_path)"]},{"cell_type":"markdown","metadata":{},"source":["# fine tuning von XLS R"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from transformers import Wav2Vec2ForCTC, Trainer, TrainingArguments\n","from transformers import Wav2Vec2FeatureExtractor\n","from torch.nn.functional import cross_entropy\n","\n","# Load dataset\n","dataset = load_dataset(\"voxceleb1\")\n","\n","# Prepare feature extractor\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","\n","# Define dataset preprocessing\n","def prepare_dataset(batch):\n","    # Process audio files\n","    audio = batch[\"audio\"]\n","    inputs = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n","    batch[\"input_values\"] = inputs.input_values.squeeze(0)\n","    batch[\"labels\"] = batch[\"speaker_id\"]\n","    return batch\n","\n","# Apply preprocessing\n","dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names[\"train\"], batch_size=8, num_proc=4, batched=True)\n","\n","# Model\n","model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", num_labels=dataset[\"train\"].features[\"speaker_id\"].num_classes)\n","\n","# Define Training Arguments\n","training_args = TrainingArguments(\n","  output_dir=\"./results\",\n","  group_by_length=True,\n","  per_device_train_batch_shift_size=16,\n","  evaluation_strategy=\"steps\",\n","  num_train_epochs=3,\n","  save_steps=500,\n","  eval_steps=500,\n","  logging_steps=10,\n","  learning_rate=1e-4,\n","  save_total_limit=2,\n",")\n","\n","# Trainer with a custom compute_loss function\n","class CustomTrainer(Trainer):\n","    def compute_loss(self, model, inputs, return_outputs=False):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss = cross_entropy(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n","        return (loss, outputs) if return_outputs else loss\n","\n","# Define Trainer\n","trainer = CustomTrainer(\n","  model=model,\n","  args=training_args,\n","  train_dataset=dataset[\"train\"],\n","  eval_dataset=dataset[\"test\"],\n","  tokenizer=feature_extractor,\n",")\n","\n","# Start training\n","trainer.train()\n"]},{"cell_type":"markdown","metadata":{},"source":["fine tuning on our dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 111 speakers: {'speaker_6': 0, 'speaker_156': 1, 'speaker_22': 2, 'speaker_19': 3, 'speaker_91': 4, 'speaker_27': 5, 'speaker_94': 6, 'speaker_34': 7, 'speaker_97': 8, 'speaker_100': 9, 'speaker_36': 10, 'speaker_128': 11, 'speaker_134': 12, 'speaker_68': 13, 'speaker_9': 14, 'speaker_17': 15, 'speaker_73': 16, 'speaker_42': 17, 'speaker_52': 18, 'speaker_151': 19, 'speaker_150': 20, 'speaker_141': 21, 'speaker_82': 22, 'speaker_130': 23, 'speaker_75': 24, 'speaker_58': 25, 'speaker_74': 26, 'speaker_104': 27, 'speaker_47': 28, 'speaker_135': 29, 'speaker_71': 30, 'speaker_83': 31, 'speaker_116': 32, 'speaker_99': 33, 'speaker_108': 34, 'speaker_31': 35, 'speaker_106': 36, 'speaker_28': 37, 'speaker_65': 38, 'speaker_48': 39, 'speaker_49': 40, 'speaker_53': 41, 'speaker_3': 42, 'speaker_63': 43, 'speaker_138': 44, 'speaker_98': 45, 'speaker_92': 46, 'speaker_123': 47, 'speaker_32': 48, 'speaker_10': 49, 'speaker_155': 50, 'speaker_153': 51, 'speaker_23': 52, 'speaker_59': 53, 'speaker_56': 54, 'speaker_101': 55, 'speaker_26': 56, 'speaker_30': 57, 'speaker_140': 58, 'speaker_35': 59, 'speaker_93': 60, 'speaker_66': 61, 'speaker_62': 62, 'speaker_137': 63, 'speaker_125': 64, 'speaker_157': 65, 'speaker_13': 66, 'speaker_152': 67, 'speaker_25': 68, 'speaker_89': 69, 'speaker_118': 70, 'speaker_16': 71, 'speaker_70': 72, 'speaker_144': 73, 'speaker_102': 74, 'speaker_43': 75, 'speaker_96': 76, 'speaker_131': 77, 'speaker_87': 78, 'speaker_39': 79, 'speaker_8': 80, 'speaker_51': 81, 'speaker_115': 82, 'speaker_158': 83, 'speaker_107': 84, 'speaker_119': 85, 'speaker_77': 86, 'speaker_2': 87, 'speaker_46': 88, 'speaker_84': 89, 'speaker_126': 90, 'speaker_85': 91, 'speaker_109': 92, 'speaker_11': 93, 'speaker_129': 94, 'speaker_86': 95, 'speaker_121': 96, 'speaker_41': 97, 'speaker_113': 98, 'speaker_76': 99, 'speaker_127': 100, 'speaker_154': 101, 'speaker_120': 102, 'speaker_78': 103, 'speaker_122': 104, 'speaker_117': 105, 'speaker_64': 106, 'speaker_133': 107, 'speaker_80': 108, 'speaker_124': 109, 'speaker_50': 110}\n","Total files in train: 8880\n","Loaded 111 speakers: {'speaker_6': 0, 'speaker_156': 1, 'speaker_22': 2, 'speaker_19': 3, 'speaker_91': 4, 'speaker_27': 5, 'speaker_94': 6, 'speaker_34': 7, 'speaker_97': 8, 'speaker_100': 9, 'speaker_36': 10, 'speaker_128': 11, 'speaker_134': 12, 'speaker_68': 13, 'speaker_9': 14, 'speaker_17': 15, 'speaker_73': 16, 'speaker_42': 17, 'speaker_52': 18, 'speaker_151': 19, 'speaker_150': 20, 'speaker_141': 21, 'speaker_82': 22, 'speaker_130': 23, 'speaker_75': 24, 'speaker_58': 25, 'speaker_74': 26, 'speaker_104': 27, 'speaker_47': 28, 'speaker_135': 29, 'speaker_71': 30, 'speaker_83': 31, 'speaker_116': 32, 'speaker_99': 33, 'speaker_108': 34, 'speaker_31': 35, 'speaker_106': 36, 'speaker_28': 37, 'speaker_65': 38, 'speaker_48': 39, 'speaker_49': 40, 'speaker_53': 41, 'speaker_3': 42, 'speaker_63': 43, 'speaker_138': 44, 'speaker_98': 45, 'speaker_92': 46, 'speaker_123': 47, 'speaker_32': 48, 'speaker_10': 49, 'speaker_155': 50, 'speaker_153': 51, 'speaker_23': 52, 'speaker_59': 53, 'speaker_56': 54, 'speaker_101': 55, 'speaker_26': 56, 'speaker_30': 57, 'speaker_140': 58, 'speaker_35': 59, 'speaker_93': 60, 'speaker_66': 61, 'speaker_62': 62, 'speaker_137': 63, 'speaker_125': 64, 'speaker_157': 65, 'speaker_13': 66, 'speaker_152': 67, 'speaker_25': 68, 'speaker_89': 69, 'speaker_118': 70, 'speaker_16': 71, 'speaker_70': 72, 'speaker_144': 73, 'speaker_102': 74, 'speaker_43': 75, 'speaker_96': 76, 'speaker_131': 77, 'speaker_87': 78, 'speaker_39': 79, 'speaker_8': 80, 'speaker_51': 81, 'speaker_115': 82, 'speaker_158': 83, 'speaker_107': 84, 'speaker_119': 85, 'speaker_77': 86, 'speaker_2': 87, 'speaker_46': 88, 'speaker_84': 89, 'speaker_126': 90, 'speaker_85': 91, 'speaker_109': 92, 'speaker_11': 93, 'speaker_129': 94, 'speaker_86': 95, 'speaker_121': 96, 'speaker_41': 97, 'speaker_113': 98, 'speaker_76': 99, 'speaker_127': 100, 'speaker_154': 101, 'speaker_120': 102, 'speaker_78': 103, 'speaker_122': 104, 'speaker_117': 105, 'speaker_64': 106, 'speaker_133': 107, 'speaker_80': 108, 'speaker_124': 109, 'speaker_50': 110}\n","Total files in validate: 1110\n","Loaded 111 speakers: {'speaker_6': 0, 'speaker_156': 1, 'speaker_22': 2, 'speaker_19': 3, 'speaker_91': 4, 'speaker_27': 5, 'speaker_94': 6, 'speaker_34': 7, 'speaker_97': 8, 'speaker_100': 9, 'speaker_36': 10, 'speaker_128': 11, 'speaker_134': 12, 'speaker_68': 13, 'speaker_9': 14, 'speaker_17': 15, 'speaker_73': 16, 'speaker_42': 17, 'speaker_52': 18, 'speaker_151': 19, 'speaker_150': 20, 'speaker_141': 21, 'speaker_82': 22, 'speaker_130': 23, 'speaker_75': 24, 'speaker_58': 25, 'speaker_74': 26, 'speaker_104': 27, 'speaker_47': 28, 'speaker_135': 29, 'speaker_71': 30, 'speaker_83': 31, 'speaker_116': 32, 'speaker_99': 33, 'speaker_108': 34, 'speaker_31': 35, 'speaker_106': 36, 'speaker_28': 37, 'speaker_65': 38, 'speaker_48': 39, 'speaker_49': 40, 'speaker_53': 41, 'speaker_3': 42, 'speaker_63': 43, 'speaker_138': 44, 'speaker_98': 45, 'speaker_92': 46, 'speaker_123': 47, 'speaker_32': 48, 'speaker_10': 49, 'speaker_155': 50, 'speaker_153': 51, 'speaker_23': 52, 'speaker_59': 53, 'speaker_56': 54, 'speaker_101': 55, 'speaker_26': 56, 'speaker_30': 57, 'speaker_140': 58, 'speaker_35': 59, 'speaker_93': 60, 'speaker_66': 61, 'speaker_62': 62, 'speaker_137': 63, 'speaker_125': 64, 'speaker_157': 65, 'speaker_13': 66, 'speaker_152': 67, 'speaker_25': 68, 'speaker_89': 69, 'speaker_118': 70, 'speaker_16': 71, 'speaker_70': 72, 'speaker_144': 73, 'speaker_102': 74, 'speaker_43': 75, 'speaker_96': 76, 'speaker_131': 77, 'speaker_87': 78, 'speaker_39': 79, 'speaker_8': 80, 'speaker_51': 81, 'speaker_115': 82, 'speaker_158': 83, 'speaker_107': 84, 'speaker_119': 85, 'speaker_77': 86, 'speaker_2': 87, 'speaker_46': 88, 'speaker_84': 89, 'speaker_126': 90, 'speaker_85': 91, 'speaker_109': 92, 'speaker_11': 93, 'speaker_129': 94, 'speaker_86': 95, 'speaker_121': 96, 'speaker_41': 97, 'speaker_113': 98, 'speaker_76': 99, 'speaker_127': 100, 'speaker_154': 101, 'speaker_120': 102, 'speaker_78': 103, 'speaker_122': 104, 'speaker_117': 105, 'speaker_64': 106, 'speaker_133': 107, 'speaker_80': 108, 'speaker_124': 109, 'speaker_50': 110}\n","Total files in test: 1110\n","Number of unique speakers: 111\n","Labels in train dataset: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110]\n","Labels in test dataset: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110]\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","All labels are valid.\n","All labels are valid.\n","All labels are valid.\n"]},{"name":"stderr","output_type":"stream","text":["/home/rag/base_venv/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='95904' max='111000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 95904/111000 8:15:49 < 1:18:02, 3.22 it/s, Epoch 86/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.706100</td>\n","      <td>4.708409</td>\n","      <td>0.008108</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.705700</td>\n","      <td>4.706166</td>\n","      <td>0.012613</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.699800</td>\n","      <td>4.701221</td>\n","      <td>0.013514</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.695000</td>\n","      <td>4.695594</td>\n","      <td>0.012613</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.684800</td>\n","      <td>4.686562</td>\n","      <td>0.017117</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.668900</td>\n","      <td>4.672047</td>\n","      <td>0.020721</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.653900</td>\n","      <td>4.652225</td>\n","      <td>0.033333</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.635800</td>\n","      <td>4.627883</td>\n","      <td>0.063063</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.610900</td>\n","      <td>4.597482</td>\n","      <td>0.081081</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.581800</td>\n","      <td>4.540682</td>\n","      <td>0.109910</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.532000</td>\n","      <td>4.497216</td>\n","      <td>0.112613</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.486100</td>\n","      <td>4.450745</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.448100</td>\n","      <td>4.388031</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.389000</td>\n","      <td>4.332085</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.336400</td>\n","      <td>4.278473</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.287000</td>\n","      <td>4.224349</td>\n","      <td>0.142342</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.229800</td>\n","      <td>4.174204</td>\n","      <td>0.135135</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.162700</td>\n","      <td>4.125351</td>\n","      <td>0.154054</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.108000</td>\n","      <td>4.064117</td>\n","      <td>0.166667</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.085500</td>\n","      <td>4.017149</td>\n","      <td>0.160360</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.024500</td>\n","      <td>3.977946</td>\n","      <td>0.173874</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>3.977900</td>\n","      <td>3.933484</td>\n","      <td>0.197297</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>3.939800</td>\n","      <td>3.885463</td>\n","      <td>0.204505</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>3.871500</td>\n","      <td>3.819177</td>\n","      <td>0.230631</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>3.834000</td>\n","      <td>3.789062</td>\n","      <td>0.228829</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>3.781600</td>\n","      <td>3.745749</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>3.776200</td>\n","      <td>3.703835</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>3.714100</td>\n","      <td>3.658749</td>\n","      <td>0.265766</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>3.654000</td>\n","      <td>3.652572</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>3.609600</td>\n","      <td>3.581869</td>\n","      <td>0.283784</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.586900</td>\n","      <td>3.586178</td>\n","      <td>0.261261</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.533800</td>\n","      <td>3.485783</td>\n","      <td>0.309009</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.486300</td>\n","      <td>3.441972</td>\n","      <td>0.312613</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.465800</td>\n","      <td>3.443608</td>\n","      <td>0.302703</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.398900</td>\n","      <td>3.374289</td>\n","      <td>0.309009</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.356700</td>\n","      <td>3.382798</td>\n","      <td>0.323423</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.324600</td>\n","      <td>3.287828</td>\n","      <td>0.325225</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.277200</td>\n","      <td>3.296731</td>\n","      <td>0.316216</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.219100</td>\n","      <td>3.232816</td>\n","      <td>0.339640</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.228800</td>\n","      <td>3.210834</td>\n","      <td>0.353153</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.141600</td>\n","      <td>3.154826</td>\n","      <td>0.374775</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.105400</td>\n","      <td>3.117202</td>\n","      <td>0.379279</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.080400</td>\n","      <td>3.075830</td>\n","      <td>0.401802</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.048600</td>\n","      <td>3.047723</td>\n","      <td>0.429730</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>2.999200</td>\n","      <td>3.008173</td>\n","      <td>0.431532</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>2.937200</td>\n","      <td>2.960706</td>\n","      <td>0.436036</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>2.919100</td>\n","      <td>2.949397</td>\n","      <td>0.438739</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>2.881300</td>\n","      <td>2.897842</td>\n","      <td>0.462162</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>2.843200</td>\n","      <td>2.849986</td>\n","      <td>0.490090</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>2.810700</td>\n","      <td>2.815048</td>\n","      <td>0.476577</td>\n","    </tr>\n","    <tr>\n","      <td>11322</td>\n","      <td>2.741000</td>\n","      <td>2.794790</td>\n","      <td>0.478378</td>\n","    </tr>\n","    <tr>\n","      <td>11544</td>\n","      <td>2.753300</td>\n","      <td>2.739130</td>\n","      <td>0.470270</td>\n","    </tr>\n","    <tr>\n","      <td>11766</td>\n","      <td>2.711500</td>\n","      <td>2.742891</td>\n","      <td>0.478378</td>\n","    </tr>\n","    <tr>\n","      <td>11988</td>\n","      <td>2.669100</td>\n","      <td>2.730546</td>\n","      <td>0.446847</td>\n","    </tr>\n","    <tr>\n","      <td>12210</td>\n","      <td>2.607500</td>\n","      <td>2.652412</td>\n","      <td>0.511712</td>\n","    </tr>\n","    <tr>\n","      <td>12432</td>\n","      <td>2.586300</td>\n","      <td>2.622035</td>\n","      <td>0.511712</td>\n","    </tr>\n","    <tr>\n","      <td>12654</td>\n","      <td>2.575300</td>\n","      <td>2.583959</td>\n","      <td>0.521622</td>\n","    </tr>\n","    <tr>\n","      <td>12876</td>\n","      <td>2.503800</td>\n","      <td>2.570900</td>\n","      <td>0.534234</td>\n","    </tr>\n","    <tr>\n","      <td>13098</td>\n","      <td>2.489700</td>\n","      <td>2.511948</td>\n","      <td>0.546847</td>\n","    </tr>\n","    <tr>\n","      <td>13320</td>\n","      <td>2.438700</td>\n","      <td>2.529216</td>\n","      <td>0.531532</td>\n","    </tr>\n","    <tr>\n","      <td>13542</td>\n","      <td>2.398900</td>\n","      <td>2.467642</td>\n","      <td>0.563063</td>\n","    </tr>\n","    <tr>\n","      <td>13764</td>\n","      <td>2.374500</td>\n","      <td>2.464670</td>\n","      <td>0.547748</td>\n","    </tr>\n","    <tr>\n","      <td>13986</td>\n","      <td>2.365700</td>\n","      <td>2.413849</td>\n","      <td>0.567568</td>\n","    </tr>\n","    <tr>\n","      <td>14208</td>\n","      <td>2.296500</td>\n","      <td>2.357843</td>\n","      <td>0.576577</td>\n","    </tr>\n","    <tr>\n","      <td>14430</td>\n","      <td>2.274000</td>\n","      <td>2.366031</td>\n","      <td>0.558559</td>\n","    </tr>\n","    <tr>\n","      <td>14652</td>\n","      <td>2.210000</td>\n","      <td>2.326669</td>\n","      <td>0.567568</td>\n","    </tr>\n","    <tr>\n","      <td>14874</td>\n","      <td>2.247800</td>\n","      <td>2.316789</td>\n","      <td>0.560360</td>\n","    </tr>\n","    <tr>\n","      <td>15096</td>\n","      <td>2.175100</td>\n","      <td>2.275383</td>\n","      <td>0.598198</td>\n","    </tr>\n","    <tr>\n","      <td>15318</td>\n","      <td>2.153500</td>\n","      <td>2.193776</td>\n","      <td>0.604505</td>\n","    </tr>\n","    <tr>\n","      <td>15540</td>\n","      <td>2.102400</td>\n","      <td>2.203442</td>\n","      <td>0.592793</td>\n","    </tr>\n","    <tr>\n","      <td>15762</td>\n","      <td>2.093000</td>\n","      <td>2.149029</td>\n","      <td>0.630631</td>\n","    </tr>\n","    <tr>\n","      <td>15984</td>\n","      <td>2.105300</td>\n","      <td>2.138828</td>\n","      <td>0.611712</td>\n","    </tr>\n","    <tr>\n","      <td>16206</td>\n","      <td>1.971000</td>\n","      <td>2.119666</td>\n","      <td>0.598198</td>\n","    </tr>\n","    <tr>\n","      <td>16428</td>\n","      <td>2.000100</td>\n","      <td>2.087193</td>\n","      <td>0.625225</td>\n","    </tr>\n","    <tr>\n","      <td>16650</td>\n","      <td>1.967700</td>\n","      <td>2.078001</td>\n","      <td>0.606306</td>\n","    </tr>\n","    <tr>\n","      <td>16872</td>\n","      <td>1.901000</td>\n","      <td>2.044186</td>\n","      <td>0.629730</td>\n","    </tr>\n","    <tr>\n","      <td>17094</td>\n","      <td>1.878500</td>\n","      <td>2.057939</td>\n","      <td>0.605405</td>\n","    </tr>\n","    <tr>\n","      <td>17316</td>\n","      <td>1.870100</td>\n","      <td>1.951925</td>\n","      <td>0.669369</td>\n","    </tr>\n","    <tr>\n","      <td>17538</td>\n","      <td>1.841100</td>\n","      <td>1.934683</td>\n","      <td>0.639640</td>\n","    </tr>\n","    <tr>\n","      <td>17760</td>\n","      <td>1.838200</td>\n","      <td>1.955320</td>\n","      <td>0.638739</td>\n","    </tr>\n","    <tr>\n","      <td>17982</td>\n","      <td>1.734200</td>\n","      <td>1.932326</td>\n","      <td>0.632432</td>\n","    </tr>\n","    <tr>\n","      <td>18204</td>\n","      <td>1.757000</td>\n","      <td>1.901089</td>\n","      <td>0.663964</td>\n","    </tr>\n","    <tr>\n","      <td>18426</td>\n","      <td>1.738500</td>\n","      <td>1.872921</td>\n","      <td>0.659459</td>\n","    </tr>\n","    <tr>\n","      <td>18648</td>\n","      <td>1.712900</td>\n","      <td>1.858081</td>\n","      <td>0.661261</td>\n","    </tr>\n","    <tr>\n","      <td>18870</td>\n","      <td>1.667600</td>\n","      <td>1.857022</td>\n","      <td>0.654054</td>\n","    </tr>\n","    <tr>\n","      <td>19092</td>\n","      <td>1.659500</td>\n","      <td>1.805856</td>\n","      <td>0.654054</td>\n","    </tr>\n","    <tr>\n","      <td>19314</td>\n","      <td>1.581000</td>\n","      <td>1.777840</td>\n","      <td>0.683784</td>\n","    </tr>\n","    <tr>\n","      <td>19536</td>\n","      <td>1.588000</td>\n","      <td>1.764695</td>\n","      <td>0.651351</td>\n","    </tr>\n","    <tr>\n","      <td>19758</td>\n","      <td>1.556700</td>\n","      <td>1.741909</td>\n","      <td>0.683784</td>\n","    </tr>\n","    <tr>\n","      <td>19980</td>\n","      <td>1.579600</td>\n","      <td>1.713268</td>\n","      <td>0.689189</td>\n","    </tr>\n","    <tr>\n","      <td>20202</td>\n","      <td>1.490000</td>\n","      <td>1.712843</td>\n","      <td>0.673874</td>\n","    </tr>\n","    <tr>\n","      <td>20424</td>\n","      <td>1.540900</td>\n","      <td>1.744035</td>\n","      <td>0.652252</td>\n","    </tr>\n","    <tr>\n","      <td>20646</td>\n","      <td>1.480200</td>\n","      <td>1.681797</td>\n","      <td>0.683784</td>\n","    </tr>\n","    <tr>\n","      <td>20868</td>\n","      <td>1.429000</td>\n","      <td>1.663025</td>\n","      <td>0.672072</td>\n","    </tr>\n","    <tr>\n","      <td>21090</td>\n","      <td>1.390700</td>\n","      <td>1.612423</td>\n","      <td>0.692793</td>\n","    </tr>\n","    <tr>\n","      <td>21312</td>\n","      <td>1.376300</td>\n","      <td>1.615953</td>\n","      <td>0.687387</td>\n","    </tr>\n","    <tr>\n","      <td>21534</td>\n","      <td>1.360900</td>\n","      <td>1.598583</td>\n","      <td>0.696396</td>\n","    </tr>\n","    <tr>\n","      <td>21756</td>\n","      <td>1.352300</td>\n","      <td>1.582296</td>\n","      <td>0.682883</td>\n","    </tr>\n","    <tr>\n","      <td>21978</td>\n","      <td>1.345300</td>\n","      <td>1.557399</td>\n","      <td>0.704505</td>\n","    </tr>\n","    <tr>\n","      <td>22200</td>\n","      <td>1.377800</td>\n","      <td>1.573445</td>\n","      <td>0.677477</td>\n","    </tr>\n","    <tr>\n","      <td>22422</td>\n","      <td>1.281400</td>\n","      <td>1.509295</td>\n","      <td>0.704505</td>\n","    </tr>\n","    <tr>\n","      <td>22644</td>\n","      <td>1.294800</td>\n","      <td>1.546738</td>\n","      <td>0.681081</td>\n","    </tr>\n","    <tr>\n","      <td>22866</td>\n","      <td>1.252100</td>\n","      <td>1.477123</td>\n","      <td>0.704505</td>\n","    </tr>\n","    <tr>\n","      <td>23088</td>\n","      <td>1.218200</td>\n","      <td>1.462702</td>\n","      <td>0.712613</td>\n","    </tr>\n","    <tr>\n","      <td>23310</td>\n","      <td>1.193000</td>\n","      <td>1.452007</td>\n","      <td>0.708108</td>\n","    </tr>\n","    <tr>\n","      <td>23532</td>\n","      <td>1.223200</td>\n","      <td>1.417527</td>\n","      <td>0.730631</td>\n","    </tr>\n","    <tr>\n","      <td>23754</td>\n","      <td>1.128600</td>\n","      <td>1.405955</td>\n","      <td>0.721622</td>\n","    </tr>\n","    <tr>\n","      <td>23976</td>\n","      <td>1.146000</td>\n","      <td>1.432261</td>\n","      <td>0.718018</td>\n","    </tr>\n","    <tr>\n","      <td>24198</td>\n","      <td>1.141100</td>\n","      <td>1.396743</td>\n","      <td>0.716216</td>\n","    </tr>\n","    <tr>\n","      <td>24420</td>\n","      <td>1.150400</td>\n","      <td>1.357688</td>\n","      <td>0.734234</td>\n","    </tr>\n","    <tr>\n","      <td>24642</td>\n","      <td>1.074300</td>\n","      <td>1.346126</td>\n","      <td>0.730631</td>\n","    </tr>\n","    <tr>\n","      <td>24864</td>\n","      <td>1.058200</td>\n","      <td>1.345419</td>\n","      <td>0.724324</td>\n","    </tr>\n","    <tr>\n","      <td>25086</td>\n","      <td>1.077000</td>\n","      <td>1.400505</td>\n","      <td>0.703604</td>\n","    </tr>\n","    <tr>\n","      <td>25308</td>\n","      <td>1.049400</td>\n","      <td>1.321716</td>\n","      <td>0.734234</td>\n","    </tr>\n","    <tr>\n","      <td>25530</td>\n","      <td>1.027800</td>\n","      <td>1.308866</td>\n","      <td>0.734234</td>\n","    </tr>\n","    <tr>\n","      <td>25752</td>\n","      <td>0.994200</td>\n","      <td>1.269092</td>\n","      <td>0.733333</td>\n","    </tr>\n","    <tr>\n","      <td>25974</td>\n","      <td>1.015700</td>\n","      <td>1.250573</td>\n","      <td>0.746847</td>\n","    </tr>\n","    <tr>\n","      <td>26196</td>\n","      <td>0.915700</td>\n","      <td>1.264460</td>\n","      <td>0.736036</td>\n","    </tr>\n","    <tr>\n","      <td>26418</td>\n","      <td>0.951100</td>\n","      <td>1.275172</td>\n","      <td>0.729730</td>\n","    </tr>\n","    <tr>\n","      <td>26640</td>\n","      <td>0.965200</td>\n","      <td>1.274846</td>\n","      <td>0.727027</td>\n","    </tr>\n","    <tr>\n","      <td>26862</td>\n","      <td>0.920500</td>\n","      <td>1.269598</td>\n","      <td>0.729730</td>\n","    </tr>\n","    <tr>\n","      <td>27084</td>\n","      <td>0.903800</td>\n","      <td>1.245521</td>\n","      <td>0.724324</td>\n","    </tr>\n","    <tr>\n","      <td>27306</td>\n","      <td>0.876900</td>\n","      <td>1.219371</td>\n","      <td>0.736036</td>\n","    </tr>\n","    <tr>\n","      <td>27528</td>\n","      <td>0.889500</td>\n","      <td>1.192448</td>\n","      <td>0.748649</td>\n","    </tr>\n","    <tr>\n","      <td>27750</td>\n","      <td>0.888600</td>\n","      <td>1.183203</td>\n","      <td>0.756757</td>\n","    </tr>\n","    <tr>\n","      <td>27972</td>\n","      <td>0.860300</td>\n","      <td>1.172967</td>\n","      <td>0.748649</td>\n","    </tr>\n","    <tr>\n","      <td>28194</td>\n","      <td>0.816900</td>\n","      <td>1.183486</td>\n","      <td>0.739640</td>\n","    </tr>\n","    <tr>\n","      <td>28416</td>\n","      <td>0.807300</td>\n","      <td>1.174016</td>\n","      <td>0.746847</td>\n","    </tr>\n","    <tr>\n","      <td>28638</td>\n","      <td>0.776700</td>\n","      <td>1.150585</td>\n","      <td>0.744144</td>\n","    </tr>\n","    <tr>\n","      <td>28860</td>\n","      <td>0.812400</td>\n","      <td>1.171660</td>\n","      <td>0.742342</td>\n","    </tr>\n","    <tr>\n","      <td>29082</td>\n","      <td>0.762600</td>\n","      <td>1.109252</td>\n","      <td>0.756757</td>\n","    </tr>\n","    <tr>\n","      <td>29304</td>\n","      <td>0.732700</td>\n","      <td>1.084014</td>\n","      <td>0.768468</td>\n","    </tr>\n","    <tr>\n","      <td>29526</td>\n","      <td>0.768300</td>\n","      <td>1.117223</td>\n","      <td>0.748649</td>\n","    </tr>\n","    <tr>\n","      <td>29748</td>\n","      <td>0.767800</td>\n","      <td>1.117015</td>\n","      <td>0.747748</td>\n","    </tr>\n","    <tr>\n","      <td>29970</td>\n","      <td>0.792300</td>\n","      <td>1.096115</td>\n","      <td>0.748649</td>\n","    </tr>\n","    <tr>\n","      <td>30192</td>\n","      <td>0.654200</td>\n","      <td>1.061617</td>\n","      <td>0.763063</td>\n","    </tr>\n","    <tr>\n","      <td>30414</td>\n","      <td>0.683900</td>\n","      <td>1.060281</td>\n","      <td>0.773874</td>\n","    </tr>\n","    <tr>\n","      <td>30636</td>\n","      <td>0.742600</td>\n","      <td>1.025859</td>\n","      <td>0.777477</td>\n","    </tr>\n","    <tr>\n","      <td>30858</td>\n","      <td>0.692700</td>\n","      <td>1.047149</td>\n","      <td>0.761261</td>\n","    </tr>\n","    <tr>\n","      <td>31080</td>\n","      <td>0.666200</td>\n","      <td>1.023172</td>\n","      <td>0.781982</td>\n","    </tr>\n","    <tr>\n","      <td>31302</td>\n","      <td>0.657800</td>\n","      <td>0.986669</td>\n","      <td>0.781081</td>\n","    </tr>\n","    <tr>\n","      <td>31524</td>\n","      <td>0.668800</td>\n","      <td>0.997861</td>\n","      <td>0.777477</td>\n","    </tr>\n","    <tr>\n","      <td>31746</td>\n","      <td>0.630600</td>\n","      <td>1.002839</td>\n","      <td>0.780180</td>\n","    </tr>\n","    <tr>\n","      <td>31968</td>\n","      <td>0.655000</td>\n","      <td>1.020434</td>\n","      <td>0.772973</td>\n","    </tr>\n","    <tr>\n","      <td>32190</td>\n","      <td>0.611400</td>\n","      <td>0.980714</td>\n","      <td>0.783784</td>\n","    </tr>\n","    <tr>\n","      <td>32412</td>\n","      <td>0.584400</td>\n","      <td>1.001126</td>\n","      <td>0.765766</td>\n","    </tr>\n","    <tr>\n","      <td>32634</td>\n","      <td>0.600900</td>\n","      <td>1.038433</td>\n","      <td>0.767568</td>\n","    </tr>\n","    <tr>\n","      <td>32856</td>\n","      <td>0.571600</td>\n","      <td>0.947422</td>\n","      <td>0.797297</td>\n","    </tr>\n","    <tr>\n","      <td>33078</td>\n","      <td>0.581300</td>\n","      <td>0.935484</td>\n","      <td>0.793694</td>\n","    </tr>\n","    <tr>\n","      <td>33300</td>\n","      <td>0.570800</td>\n","      <td>1.004139</td>\n","      <td>0.774775</td>\n","    </tr>\n","    <tr>\n","      <td>33522</td>\n","      <td>0.558500</td>\n","      <td>0.987444</td>\n","      <td>0.779279</td>\n","    </tr>\n","    <tr>\n","      <td>33744</td>\n","      <td>0.526100</td>\n","      <td>0.998581</td>\n","      <td>0.767568</td>\n","    </tr>\n","    <tr>\n","      <td>33966</td>\n","      <td>0.580000</td>\n","      <td>0.918119</td>\n","      <td>0.791892</td>\n","    </tr>\n","    <tr>\n","      <td>34188</td>\n","      <td>0.558800</td>\n","      <td>0.929574</td>\n","      <td>0.790991</td>\n","    </tr>\n","    <tr>\n","      <td>34410</td>\n","      <td>0.542100</td>\n","      <td>0.885316</td>\n","      <td>0.796396</td>\n","    </tr>\n","    <tr>\n","      <td>34632</td>\n","      <td>0.508300</td>\n","      <td>0.924948</td>\n","      <td>0.787387</td>\n","    </tr>\n","    <tr>\n","      <td>34854</td>\n","      <td>0.507800</td>\n","      <td>0.907033</td>\n","      <td>0.795495</td>\n","    </tr>\n","    <tr>\n","      <td>35076</td>\n","      <td>0.539500</td>\n","      <td>0.928706</td>\n","      <td>0.774775</td>\n","    </tr>\n","    <tr>\n","      <td>35298</td>\n","      <td>0.504500</td>\n","      <td>0.883035</td>\n","      <td>0.800901</td>\n","    </tr>\n","    <tr>\n","      <td>35520</td>\n","      <td>0.507600</td>\n","      <td>0.946605</td>\n","      <td>0.772973</td>\n","    </tr>\n","    <tr>\n","      <td>35742</td>\n","      <td>0.482700</td>\n","      <td>0.902783</td>\n","      <td>0.794595</td>\n","    </tr>\n","    <tr>\n","      <td>35964</td>\n","      <td>0.488100</td>\n","      <td>0.839473</td>\n","      <td>0.804505</td>\n","    </tr>\n","    <tr>\n","      <td>36186</td>\n","      <td>0.480600</td>\n","      <td>0.854860</td>\n","      <td>0.790090</td>\n","    </tr>\n","    <tr>\n","      <td>36408</td>\n","      <td>0.440500</td>\n","      <td>0.839990</td>\n","      <td>0.809910</td>\n","    </tr>\n","    <tr>\n","      <td>36630</td>\n","      <td>0.463900</td>\n","      <td>0.871733</td>\n","      <td>0.798198</td>\n","    </tr>\n","    <tr>\n","      <td>36852</td>\n","      <td>0.458900</td>\n","      <td>0.805667</td>\n","      <td>0.818018</td>\n","    </tr>\n","    <tr>\n","      <td>37074</td>\n","      <td>0.472900</td>\n","      <td>0.855570</td>\n","      <td>0.805405</td>\n","    </tr>\n","    <tr>\n","      <td>37296</td>\n","      <td>0.416700</td>\n","      <td>0.830120</td>\n","      <td>0.811712</td>\n","    </tr>\n","    <tr>\n","      <td>37518</td>\n","      <td>0.435900</td>\n","      <td>0.807058</td>\n","      <td>0.815315</td>\n","    </tr>\n","    <tr>\n","      <td>37740</td>\n","      <td>0.415400</td>\n","      <td>0.838946</td>\n","      <td>0.804505</td>\n","    </tr>\n","    <tr>\n","      <td>37962</td>\n","      <td>0.449500</td>\n","      <td>0.879892</td>\n","      <td>0.792793</td>\n","    </tr>\n","    <tr>\n","      <td>38184</td>\n","      <td>0.424300</td>\n","      <td>0.846261</td>\n","      <td>0.803604</td>\n","    </tr>\n","    <tr>\n","      <td>38406</td>\n","      <td>0.378400</td>\n","      <td>0.886152</td>\n","      <td>0.789189</td>\n","    </tr>\n","    <tr>\n","      <td>38628</td>\n","      <td>0.379500</td>\n","      <td>0.849038</td>\n","      <td>0.798198</td>\n","    </tr>\n","    <tr>\n","      <td>38850</td>\n","      <td>0.428200</td>\n","      <td>0.788073</td>\n","      <td>0.821622</td>\n","    </tr>\n","    <tr>\n","      <td>39072</td>\n","      <td>0.372600</td>\n","      <td>0.884360</td>\n","      <td>0.783784</td>\n","    </tr>\n","    <tr>\n","      <td>39294</td>\n","      <td>0.382100</td>\n","      <td>0.771813</td>\n","      <td>0.819820</td>\n","    </tr>\n","    <tr>\n","      <td>39516</td>\n","      <td>0.382100</td>\n","      <td>0.816273</td>\n","      <td>0.799099</td>\n","    </tr>\n","    <tr>\n","      <td>39738</td>\n","      <td>0.375800</td>\n","      <td>0.825420</td>\n","      <td>0.797297</td>\n","    </tr>\n","    <tr>\n","      <td>39960</td>\n","      <td>0.360800</td>\n","      <td>0.866221</td>\n","      <td>0.792793</td>\n","    </tr>\n","    <tr>\n","      <td>40182</td>\n","      <td>0.386800</td>\n","      <td>0.776869</td>\n","      <td>0.812613</td>\n","    </tr>\n","    <tr>\n","      <td>40404</td>\n","      <td>0.364200</td>\n","      <td>0.853047</td>\n","      <td>0.787387</td>\n","    </tr>\n","    <tr>\n","      <td>40626</td>\n","      <td>0.351600</td>\n","      <td>0.844278</td>\n","      <td>0.799099</td>\n","    </tr>\n","    <tr>\n","      <td>40848</td>\n","      <td>0.372400</td>\n","      <td>0.828846</td>\n","      <td>0.790991</td>\n","    </tr>\n","    <tr>\n","      <td>41070</td>\n","      <td>0.362700</td>\n","      <td>0.828997</td>\n","      <td>0.796396</td>\n","    </tr>\n","    <tr>\n","      <td>41292</td>\n","      <td>0.313900</td>\n","      <td>0.767226</td>\n","      <td>0.818018</td>\n","    </tr>\n","    <tr>\n","      <td>41514</td>\n","      <td>0.322600</td>\n","      <td>0.805532</td>\n","      <td>0.800901</td>\n","    </tr>\n","    <tr>\n","      <td>41736</td>\n","      <td>0.361000</td>\n","      <td>0.821702</td>\n","      <td>0.801802</td>\n","    </tr>\n","    <tr>\n","      <td>41958</td>\n","      <td>0.315700</td>\n","      <td>0.798490</td>\n","      <td>0.795495</td>\n","    </tr>\n","    <tr>\n","      <td>42180</td>\n","      <td>0.323900</td>\n","      <td>0.783900</td>\n","      <td>0.813514</td>\n","    </tr>\n","    <tr>\n","      <td>42402</td>\n","      <td>0.340000</td>\n","      <td>0.782243</td>\n","      <td>0.806306</td>\n","    </tr>\n","    <tr>\n","      <td>42624</td>\n","      <td>0.294400</td>\n","      <td>0.794127</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>42846</td>\n","      <td>0.321700</td>\n","      <td>0.798452</td>\n","      <td>0.804505</td>\n","    </tr>\n","    <tr>\n","      <td>43068</td>\n","      <td>0.280400</td>\n","      <td>0.771978</td>\n","      <td>0.807207</td>\n","    </tr>\n","    <tr>\n","      <td>43290</td>\n","      <td>0.303500</td>\n","      <td>0.717251</td>\n","      <td>0.826126</td>\n","    </tr>\n","    <tr>\n","      <td>43512</td>\n","      <td>0.343000</td>\n","      <td>0.759952</td>\n","      <td>0.815315</td>\n","    </tr>\n","    <tr>\n","      <td>43734</td>\n","      <td>0.310400</td>\n","      <td>0.703962</td>\n","      <td>0.826126</td>\n","    </tr>\n","    <tr>\n","      <td>43956</td>\n","      <td>0.292100</td>\n","      <td>0.747924</td>\n","      <td>0.817117</td>\n","    </tr>\n","    <tr>\n","      <td>44178</td>\n","      <td>0.290700</td>\n","      <td>0.716933</td>\n","      <td>0.823423</td>\n","    </tr>\n","    <tr>\n","      <td>44400</td>\n","      <td>0.320100</td>\n","      <td>0.748936</td>\n","      <td>0.806306</td>\n","    </tr>\n","    <tr>\n","      <td>44622</td>\n","      <td>0.294100</td>\n","      <td>0.751379</td>\n","      <td>0.810811</td>\n","    </tr>\n","    <tr>\n","      <td>44844</td>\n","      <td>0.257900</td>\n","      <td>0.760582</td>\n","      <td>0.813514</td>\n","    </tr>\n","    <tr>\n","      <td>45066</td>\n","      <td>0.290600</td>\n","      <td>0.732450</td>\n","      <td>0.816216</td>\n","    </tr>\n","    <tr>\n","      <td>45288</td>\n","      <td>0.265500</td>\n","      <td>0.760710</td>\n","      <td>0.810811</td>\n","    </tr>\n","    <tr>\n","      <td>45510</td>\n","      <td>0.278100</td>\n","      <td>0.709348</td>\n","      <td>0.823423</td>\n","    </tr>\n","    <tr>\n","      <td>45732</td>\n","      <td>0.268200</td>\n","      <td>0.669880</td>\n","      <td>0.836036</td>\n","    </tr>\n","    <tr>\n","      <td>45954</td>\n","      <td>0.246500</td>\n","      <td>0.672116</td>\n","      <td>0.835135</td>\n","    </tr>\n","    <tr>\n","      <td>46176</td>\n","      <td>0.257200</td>\n","      <td>0.698842</td>\n","      <td>0.817117</td>\n","    </tr>\n","    <tr>\n","      <td>46398</td>\n","      <td>0.271100</td>\n","      <td>0.707206</td>\n","      <td>0.814414</td>\n","    </tr>\n","    <tr>\n","      <td>46620</td>\n","      <td>0.230400</td>\n","      <td>0.656521</td>\n","      <td>0.839640</td>\n","    </tr>\n","    <tr>\n","      <td>46842</td>\n","      <td>0.247500</td>\n","      <td>0.712790</td>\n","      <td>0.824324</td>\n","    </tr>\n","    <tr>\n","      <td>47064</td>\n","      <td>0.231200</td>\n","      <td>0.723691</td>\n","      <td>0.812613</td>\n","    </tr>\n","    <tr>\n","      <td>47286</td>\n","      <td>0.262800</td>\n","      <td>0.686844</td>\n","      <td>0.818018</td>\n","    </tr>\n","    <tr>\n","      <td>47508</td>\n","      <td>0.217000</td>\n","      <td>0.737019</td>\n","      <td>0.817117</td>\n","    </tr>\n","    <tr>\n","      <td>47730</td>\n","      <td>0.246300</td>\n","      <td>0.726864</td>\n","      <td>0.814414</td>\n","    </tr>\n","    <tr>\n","      <td>47952</td>\n","      <td>0.230900</td>\n","      <td>0.679181</td>\n","      <td>0.838739</td>\n","    </tr>\n","    <tr>\n","      <td>48174</td>\n","      <td>0.253100</td>\n","      <td>0.719807</td>\n","      <td>0.820721</td>\n","    </tr>\n","    <tr>\n","      <td>48396</td>\n","      <td>0.218500</td>\n","      <td>0.713008</td>\n","      <td>0.819820</td>\n","    </tr>\n","    <tr>\n","      <td>48618</td>\n","      <td>0.234000</td>\n","      <td>0.696392</td>\n","      <td>0.823423</td>\n","    </tr>\n","    <tr>\n","      <td>48840</td>\n","      <td>0.235600</td>\n","      <td>0.749351</td>\n","      <td>0.809009</td>\n","    </tr>\n","    <tr>\n","      <td>49062</td>\n","      <td>0.238700</td>\n","      <td>0.696792</td>\n","      <td>0.820721</td>\n","    </tr>\n","    <tr>\n","      <td>49284</td>\n","      <td>0.241300</td>\n","      <td>0.700669</td>\n","      <td>0.821622</td>\n","    </tr>\n","    <tr>\n","      <td>49506</td>\n","      <td>0.179200</td>\n","      <td>0.708816</td>\n","      <td>0.813514</td>\n","    </tr>\n","    <tr>\n","      <td>49728</td>\n","      <td>0.236500</td>\n","      <td>0.742018</td>\n","      <td>0.800000</td>\n","    </tr>\n","    <tr>\n","      <td>49950</td>\n","      <td>0.221600</td>\n","      <td>0.653510</td>\n","      <td>0.835135</td>\n","    </tr>\n","    <tr>\n","      <td>50172</td>\n","      <td>0.210900</td>\n","      <td>0.658369</td>\n","      <td>0.826126</td>\n","    </tr>\n","    <tr>\n","      <td>50394</td>\n","      <td>0.241300</td>\n","      <td>0.662616</td>\n","      <td>0.831532</td>\n","    </tr>\n","    <tr>\n","      <td>50616</td>\n","      <td>0.200500</td>\n","      <td>0.663687</td>\n","      <td>0.836036</td>\n","    </tr>\n","    <tr>\n","      <td>50838</td>\n","      <td>0.205500</td>\n","      <td>0.666294</td>\n","      <td>0.829730</td>\n","    </tr>\n","    <tr>\n","      <td>51060</td>\n","      <td>0.217800</td>\n","      <td>0.663058</td>\n","      <td>0.830631</td>\n","    </tr>\n","    <tr>\n","      <td>51282</td>\n","      <td>0.177800</td>\n","      <td>0.682414</td>\n","      <td>0.824324</td>\n","    </tr>\n","    <tr>\n","      <td>51504</td>\n","      <td>0.215300</td>\n","      <td>0.677315</td>\n","      <td>0.836937</td>\n","    </tr>\n","    <tr>\n","      <td>51726</td>\n","      <td>0.230000</td>\n","      <td>0.642180</td>\n","      <td>0.842342</td>\n","    </tr>\n","    <tr>\n","      <td>51948</td>\n","      <td>0.180900</td>\n","      <td>0.669744</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <td>52170</td>\n","      <td>0.204900</td>\n","      <td>0.636171</td>\n","      <td>0.844144</td>\n","    </tr>\n","    <tr>\n","      <td>52392</td>\n","      <td>0.193400</td>\n","      <td>0.697533</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <td>52614</td>\n","      <td>0.192800</td>\n","      <td>0.617394</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>52836</td>\n","      <td>0.212400</td>\n","      <td>0.668679</td>\n","      <td>0.829730</td>\n","    </tr>\n","    <tr>\n","      <td>53058</td>\n","      <td>0.208400</td>\n","      <td>0.664745</td>\n","      <td>0.835135</td>\n","    </tr>\n","    <tr>\n","      <td>53280</td>\n","      <td>0.177500</td>\n","      <td>0.666349</td>\n","      <td>0.836036</td>\n","    </tr>\n","    <tr>\n","      <td>53502</td>\n","      <td>0.159300</td>\n","      <td>0.609076</td>\n","      <td>0.850450</td>\n","    </tr>\n","    <tr>\n","      <td>53724</td>\n","      <td>0.181900</td>\n","      <td>0.643712</td>\n","      <td>0.844144</td>\n","    </tr>\n","    <tr>\n","      <td>53946</td>\n","      <td>0.157200</td>\n","      <td>0.661849</td>\n","      <td>0.837838</td>\n","    </tr>\n","    <tr>\n","      <td>54168</td>\n","      <td>0.186700</td>\n","      <td>0.676476</td>\n","      <td>0.832432</td>\n","    </tr>\n","    <tr>\n","      <td>54390</td>\n","      <td>0.189500</td>\n","      <td>0.654614</td>\n","      <td>0.828829</td>\n","    </tr>\n","    <tr>\n","      <td>54612</td>\n","      <td>0.177500</td>\n","      <td>0.627989</td>\n","      <td>0.845946</td>\n","    </tr>\n","    <tr>\n","      <td>54834</td>\n","      <td>0.181700</td>\n","      <td>0.663586</td>\n","      <td>0.831532</td>\n","    </tr>\n","    <tr>\n","      <td>55056</td>\n","      <td>0.174000</td>\n","      <td>0.665360</td>\n","      <td>0.832432</td>\n","    </tr>\n","    <tr>\n","      <td>55278</td>\n","      <td>0.157800</td>\n","      <td>0.629915</td>\n","      <td>0.847748</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>0.153700</td>\n","      <td>0.693210</td>\n","      <td>0.818018</td>\n","    </tr>\n","    <tr>\n","      <td>55722</td>\n","      <td>0.193600</td>\n","      <td>0.699464</td>\n","      <td>0.817117</td>\n","    </tr>\n","    <tr>\n","      <td>55944</td>\n","      <td>0.168600</td>\n","      <td>0.662470</td>\n","      <td>0.831532</td>\n","    </tr>\n","    <tr>\n","      <td>56166</td>\n","      <td>0.156900</td>\n","      <td>0.657120</td>\n","      <td>0.836036</td>\n","    </tr>\n","    <tr>\n","      <td>56388</td>\n","      <td>0.170800</td>\n","      <td>0.678401</td>\n","      <td>0.824324</td>\n","    </tr>\n","    <tr>\n","      <td>56610</td>\n","      <td>0.179200</td>\n","      <td>0.657212</td>\n","      <td>0.837838</td>\n","    </tr>\n","    <tr>\n","      <td>56832</td>\n","      <td>0.177900</td>\n","      <td>0.653498</td>\n","      <td>0.840541</td>\n","    </tr>\n","    <tr>\n","      <td>57054</td>\n","      <td>0.123400</td>\n","      <td>0.658038</td>\n","      <td>0.838739</td>\n","    </tr>\n","    <tr>\n","      <td>57276</td>\n","      <td>0.148700</td>\n","      <td>0.614741</td>\n","      <td>0.851351</td>\n","    </tr>\n","    <tr>\n","      <td>57498</td>\n","      <td>0.134400</td>\n","      <td>0.638814</td>\n","      <td>0.844144</td>\n","    </tr>\n","    <tr>\n","      <td>57720</td>\n","      <td>0.149100</td>\n","      <td>0.630665</td>\n","      <td>0.845946</td>\n","    </tr>\n","    <tr>\n","      <td>57942</td>\n","      <td>0.176600</td>\n","      <td>0.627356</td>\n","      <td>0.840541</td>\n","    </tr>\n","    <tr>\n","      <td>58164</td>\n","      <td>0.152100</td>\n","      <td>0.637911</td>\n","      <td>0.839640</td>\n","    </tr>\n","    <tr>\n","      <td>58386</td>\n","      <td>0.150700</td>\n","      <td>0.705032</td>\n","      <td>0.825225</td>\n","    </tr>\n","    <tr>\n","      <td>58608</td>\n","      <td>0.125200</td>\n","      <td>0.632735</td>\n","      <td>0.842342</td>\n","    </tr>\n","    <tr>\n","      <td>58830</td>\n","      <td>0.172700</td>\n","      <td>0.636837</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>59052</td>\n","      <td>0.152400</td>\n","      <td>0.644210</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>59274</td>\n","      <td>0.170500</td>\n","      <td>0.631586</td>\n","      <td>0.847748</td>\n","    </tr>\n","    <tr>\n","      <td>59496</td>\n","      <td>0.160200</td>\n","      <td>0.652517</td>\n","      <td>0.838739</td>\n","    </tr>\n","    <tr>\n","      <td>59718</td>\n","      <td>0.170400</td>\n","      <td>0.642401</td>\n","      <td>0.843243</td>\n","    </tr>\n","    <tr>\n","      <td>59940</td>\n","      <td>0.150100</td>\n","      <td>0.646409</td>\n","      <td>0.845045</td>\n","    </tr>\n","    <tr>\n","      <td>60162</td>\n","      <td>0.131200</td>\n","      <td>0.651206</td>\n","      <td>0.842342</td>\n","    </tr>\n","    <tr>\n","      <td>60384</td>\n","      <td>0.132500</td>\n","      <td>0.657178</td>\n","      <td>0.836937</td>\n","    </tr>\n","    <tr>\n","      <td>60606</td>\n","      <td>0.138000</td>\n","      <td>0.647632</td>\n","      <td>0.836036</td>\n","    </tr>\n","    <tr>\n","      <td>60828</td>\n","      <td>0.167700</td>\n","      <td>0.679915</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <td>61050</td>\n","      <td>0.150100</td>\n","      <td>0.630649</td>\n","      <td>0.851351</td>\n","    </tr>\n","    <tr>\n","      <td>61272</td>\n","      <td>0.141600</td>\n","      <td>0.653696</td>\n","      <td>0.836937</td>\n","    </tr>\n","    <tr>\n","      <td>61494</td>\n","      <td>0.101600</td>\n","      <td>0.651023</td>\n","      <td>0.839640</td>\n","    </tr>\n","    <tr>\n","      <td>61716</td>\n","      <td>0.147000</td>\n","      <td>0.631413</td>\n","      <td>0.839640</td>\n","    </tr>\n","    <tr>\n","      <td>61938</td>\n","      <td>0.138000</td>\n","      <td>0.641546</td>\n","      <td>0.840541</td>\n","    </tr>\n","    <tr>\n","      <td>62160</td>\n","      <td>0.157300</td>\n","      <td>0.615663</td>\n","      <td>0.848649</td>\n","    </tr>\n","    <tr>\n","      <td>62382</td>\n","      <td>0.138900</td>\n","      <td>0.635530</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>62604</td>\n","      <td>0.109300</td>\n","      <td>0.657391</td>\n","      <td>0.843243</td>\n","    </tr>\n","    <tr>\n","      <td>62826</td>\n","      <td>0.137700</td>\n","      <td>0.644860</td>\n","      <td>0.841441</td>\n","    </tr>\n","    <tr>\n","      <td>63048</td>\n","      <td>0.137700</td>\n","      <td>0.679510</td>\n","      <td>0.835135</td>\n","    </tr>\n","    <tr>\n","      <td>63270</td>\n","      <td>0.145000</td>\n","      <td>0.651868</td>\n","      <td>0.837838</td>\n","    </tr>\n","    <tr>\n","      <td>63492</td>\n","      <td>0.120600</td>\n","      <td>0.632704</td>\n","      <td>0.837838</td>\n","    </tr>\n","    <tr>\n","      <td>63714</td>\n","      <td>0.148400</td>\n","      <td>0.619919</td>\n","      <td>0.847748</td>\n","    </tr>\n","    <tr>\n","      <td>63936</td>\n","      <td>0.134600</td>\n","      <td>0.675867</td>\n","      <td>0.840541</td>\n","    </tr>\n","    <tr>\n","      <td>64158</td>\n","      <td>0.112300</td>\n","      <td>0.660892</td>\n","      <td>0.831532</td>\n","    </tr>\n","    <tr>\n","      <td>64380</td>\n","      <td>0.120600</td>\n","      <td>0.666347</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <td>64602</td>\n","      <td>0.136700</td>\n","      <td>0.639375</td>\n","      <td>0.853153</td>\n","    </tr>\n","    <tr>\n","      <td>64824</td>\n","      <td>0.125000</td>\n","      <td>0.666436</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <td>65046</td>\n","      <td>0.108600</td>\n","      <td>0.634153</td>\n","      <td>0.833333</td>\n","    </tr>\n","    <tr>\n","      <td>65268</td>\n","      <td>0.139300</td>\n","      <td>0.591031</td>\n","      <td>0.854054</td>\n","    </tr>\n","    <tr>\n","      <td>65490</td>\n","      <td>0.120400</td>\n","      <td>0.648436</td>\n","      <td>0.841441</td>\n","    </tr>\n","    <tr>\n","      <td>65712</td>\n","      <td>0.138100</td>\n","      <td>0.610622</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>65934</td>\n","      <td>0.123200</td>\n","      <td>0.611151</td>\n","      <td>0.845946</td>\n","    </tr>\n","    <tr>\n","      <td>66156</td>\n","      <td>0.156900</td>\n","      <td>0.614920</td>\n","      <td>0.848649</td>\n","    </tr>\n","    <tr>\n","      <td>66378</td>\n","      <td>0.127300</td>\n","      <td>0.603340</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>66600</td>\n","      <td>0.108000</td>\n","      <td>0.652169</td>\n","      <td>0.837838</td>\n","    </tr>\n","    <tr>\n","      <td>66822</td>\n","      <td>0.173100</td>\n","      <td>0.616609</td>\n","      <td>0.845946</td>\n","    </tr>\n","    <tr>\n","      <td>67044</td>\n","      <td>0.098700</td>\n","      <td>0.664275</td>\n","      <td>0.843243</td>\n","    </tr>\n","    <tr>\n","      <td>67266</td>\n","      <td>0.156600</td>\n","      <td>0.639776</td>\n","      <td>0.842342</td>\n","    </tr>\n","    <tr>\n","      <td>67488</td>\n","      <td>0.122300</td>\n","      <td>0.629427</td>\n","      <td>0.836937</td>\n","    </tr>\n","    <tr>\n","      <td>67710</td>\n","      <td>0.118000</td>\n","      <td>0.647931</td>\n","      <td>0.841441</td>\n","    </tr>\n","    <tr>\n","      <td>67932</td>\n","      <td>0.112000</td>\n","      <td>0.618246</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>68154</td>\n","      <td>0.118700</td>\n","      <td>0.712688</td>\n","      <td>0.822523</td>\n","    </tr>\n","    <tr>\n","      <td>68376</td>\n","      <td>0.104000</td>\n","      <td>0.628003</td>\n","      <td>0.850450</td>\n","    </tr>\n","    <tr>\n","      <td>68598</td>\n","      <td>0.125500</td>\n","      <td>0.680912</td>\n","      <td>0.836036</td>\n","    </tr>\n","    <tr>\n","      <td>68820</td>\n","      <td>0.131800</td>\n","      <td>0.589483</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>69042</td>\n","      <td>0.102700</td>\n","      <td>0.597193</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>69264</td>\n","      <td>0.127500</td>\n","      <td>0.625717</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>69486</td>\n","      <td>0.076800</td>\n","      <td>0.626814</td>\n","      <td>0.851351</td>\n","    </tr>\n","    <tr>\n","      <td>69708</td>\n","      <td>0.135500</td>\n","      <td>0.636905</td>\n","      <td>0.839640</td>\n","    </tr>\n","    <tr>\n","      <td>69930</td>\n","      <td>0.139300</td>\n","      <td>0.636055</td>\n","      <td>0.843243</td>\n","    </tr>\n","    <tr>\n","      <td>70152</td>\n","      <td>0.085100</td>\n","      <td>0.640685</td>\n","      <td>0.847748</td>\n","    </tr>\n","    <tr>\n","      <td>70374</td>\n","      <td>0.108100</td>\n","      <td>0.648035</td>\n","      <td>0.847748</td>\n","    </tr>\n","    <tr>\n","      <td>70596</td>\n","      <td>0.114200</td>\n","      <td>0.589471</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>70818</td>\n","      <td>0.135700</td>\n","      <td>0.685462</td>\n","      <td>0.832432</td>\n","    </tr>\n","    <tr>\n","      <td>71040</td>\n","      <td>0.093700</td>\n","      <td>0.610313</td>\n","      <td>0.851351</td>\n","    </tr>\n","    <tr>\n","      <td>71262</td>\n","      <td>0.108600</td>\n","      <td>0.607719</td>\n","      <td>0.852252</td>\n","    </tr>\n","    <tr>\n","      <td>71484</td>\n","      <td>0.122100</td>\n","      <td>0.620447</td>\n","      <td>0.854054</td>\n","    </tr>\n","    <tr>\n","      <td>71706</td>\n","      <td>0.094900</td>\n","      <td>0.607256</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>71928</td>\n","      <td>0.097900</td>\n","      <td>0.568852</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>72150</td>\n","      <td>0.125800</td>\n","      <td>0.618659</td>\n","      <td>0.858559</td>\n","    </tr>\n","    <tr>\n","      <td>72372</td>\n","      <td>0.100800</td>\n","      <td>0.592099</td>\n","      <td>0.853153</td>\n","    </tr>\n","    <tr>\n","      <td>72594</td>\n","      <td>0.088100</td>\n","      <td>0.602192</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>72816</td>\n","      <td>0.105200</td>\n","      <td>0.613032</td>\n","      <td>0.851351</td>\n","    </tr>\n","    <tr>\n","      <td>73038</td>\n","      <td>0.091500</td>\n","      <td>0.579158</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>73260</td>\n","      <td>0.088200</td>\n","      <td>0.588231</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>73482</td>\n","      <td>0.103200</td>\n","      <td>0.623544</td>\n","      <td>0.849550</td>\n","    </tr>\n","    <tr>\n","      <td>73704</td>\n","      <td>0.100100</td>\n","      <td>0.568774</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>73926</td>\n","      <td>0.081600</td>\n","      <td>0.594625</td>\n","      <td>0.850450</td>\n","    </tr>\n","    <tr>\n","      <td>74148</td>\n","      <td>0.123100</td>\n","      <td>0.591191</td>\n","      <td>0.862162</td>\n","    </tr>\n","    <tr>\n","      <td>74370</td>\n","      <td>0.113400</td>\n","      <td>0.586522</td>\n","      <td>0.859459</td>\n","    </tr>\n","    <tr>\n","      <td>74592</td>\n","      <td>0.129400</td>\n","      <td>0.592037</td>\n","      <td>0.860360</td>\n","    </tr>\n","    <tr>\n","      <td>74814</td>\n","      <td>0.087300</td>\n","      <td>0.634277</td>\n","      <td>0.849550</td>\n","    </tr>\n","    <tr>\n","      <td>75036</td>\n","      <td>0.124400</td>\n","      <td>0.608762</td>\n","      <td>0.852252</td>\n","    </tr>\n","    <tr>\n","      <td>75258</td>\n","      <td>0.104100</td>\n","      <td>0.590562</td>\n","      <td>0.862162</td>\n","    </tr>\n","    <tr>\n","      <td>75480</td>\n","      <td>0.095000</td>\n","      <td>0.617031</td>\n","      <td>0.851351</td>\n","    </tr>\n","    <tr>\n","      <td>75702</td>\n","      <td>0.102300</td>\n","      <td>0.576755</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>75924</td>\n","      <td>0.096400</td>\n","      <td>0.661629</td>\n","      <td>0.837838</td>\n","    </tr>\n","    <tr>\n","      <td>76146</td>\n","      <td>0.110100</td>\n","      <td>0.607087</td>\n","      <td>0.854054</td>\n","    </tr>\n","    <tr>\n","      <td>76368</td>\n","      <td>0.106000</td>\n","      <td>0.631692</td>\n","      <td>0.848649</td>\n","    </tr>\n","    <tr>\n","      <td>76590</td>\n","      <td>0.116400</td>\n","      <td>0.600818</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>76812</td>\n","      <td>0.084600</td>\n","      <td>0.635768</td>\n","      <td>0.845946</td>\n","    </tr>\n","    <tr>\n","      <td>77034</td>\n","      <td>0.123100</td>\n","      <td>0.641317</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>77256</td>\n","      <td>0.117400</td>\n","      <td>0.612402</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>77478</td>\n","      <td>0.115200</td>\n","      <td>0.604570</td>\n","      <td>0.860360</td>\n","    </tr>\n","    <tr>\n","      <td>77700</td>\n","      <td>0.082900</td>\n","      <td>0.626909</td>\n","      <td>0.854054</td>\n","    </tr>\n","    <tr>\n","      <td>77922</td>\n","      <td>0.093400</td>\n","      <td>0.635021</td>\n","      <td>0.854054</td>\n","    </tr>\n","    <tr>\n","      <td>78144</td>\n","      <td>0.073900</td>\n","      <td>0.598641</td>\n","      <td>0.866667</td>\n","    </tr>\n","    <tr>\n","      <td>78366</td>\n","      <td>0.124200</td>\n","      <td>0.573397</td>\n","      <td>0.873874</td>\n","    </tr>\n","    <tr>\n","      <td>78588</td>\n","      <td>0.117400</td>\n","      <td>0.635283</td>\n","      <td>0.852252</td>\n","    </tr>\n","    <tr>\n","      <td>78810</td>\n","      <td>0.111700</td>\n","      <td>0.645011</td>\n","      <td>0.848649</td>\n","    </tr>\n","    <tr>\n","      <td>79032</td>\n","      <td>0.106700</td>\n","      <td>0.585286</td>\n","      <td>0.858559</td>\n","    </tr>\n","    <tr>\n","      <td>79254</td>\n","      <td>0.111300</td>\n","      <td>0.639455</td>\n","      <td>0.846847</td>\n","    </tr>\n","    <tr>\n","      <td>79476</td>\n","      <td>0.063100</td>\n","      <td>0.607925</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>79698</td>\n","      <td>0.106300</td>\n","      <td>0.606248</td>\n","      <td>0.860360</td>\n","    </tr>\n","    <tr>\n","      <td>79920</td>\n","      <td>0.108000</td>\n","      <td>0.655279</td>\n","      <td>0.852252</td>\n","    </tr>\n","    <tr>\n","      <td>80142</td>\n","      <td>0.082800</td>\n","      <td>0.619933</td>\n","      <td>0.867568</td>\n","    </tr>\n","    <tr>\n","      <td>80364</td>\n","      <td>0.083600</td>\n","      <td>0.615728</td>\n","      <td>0.858559</td>\n","    </tr>\n","    <tr>\n","      <td>80586</td>\n","      <td>0.081100</td>\n","      <td>0.602062</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>80808</td>\n","      <td>0.080100</td>\n","      <td>0.638837</td>\n","      <td>0.858559</td>\n","    </tr>\n","    <tr>\n","      <td>81030</td>\n","      <td>0.099800</td>\n","      <td>0.622831</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>81252</td>\n","      <td>0.083500</td>\n","      <td>0.596528</td>\n","      <td>0.867568</td>\n","    </tr>\n","    <tr>\n","      <td>81474</td>\n","      <td>0.109700</td>\n","      <td>0.620818</td>\n","      <td>0.861261</td>\n","    </tr>\n","    <tr>\n","      <td>81696</td>\n","      <td>0.119100</td>\n","      <td>0.599942</td>\n","      <td>0.861261</td>\n","    </tr>\n","    <tr>\n","      <td>81918</td>\n","      <td>0.063100</td>\n","      <td>0.598768</td>\n","      <td>0.861261</td>\n","    </tr>\n","    <tr>\n","      <td>82140</td>\n","      <td>0.095900</td>\n","      <td>0.616210</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>82362</td>\n","      <td>0.075300</td>\n","      <td>0.614195</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>82584</td>\n","      <td>0.103100</td>\n","      <td>0.595069</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>82806</td>\n","      <td>0.091100</td>\n","      <td>0.622524</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>83028</td>\n","      <td>0.084600</td>\n","      <td>0.620654</td>\n","      <td>0.854955</td>\n","    </tr>\n","    <tr>\n","      <td>83250</td>\n","      <td>0.092000</td>\n","      <td>0.629537</td>\n","      <td>0.854955</td>\n","    </tr>\n","    <tr>\n","      <td>83472</td>\n","      <td>0.129700</td>\n","      <td>0.627344</td>\n","      <td>0.861261</td>\n","    </tr>\n","    <tr>\n","      <td>83694</td>\n","      <td>0.095600</td>\n","      <td>0.626648</td>\n","      <td>0.853153</td>\n","    </tr>\n","    <tr>\n","      <td>83916</td>\n","      <td>0.086400</td>\n","      <td>0.606679</td>\n","      <td>0.864865</td>\n","    </tr>\n","    <tr>\n","      <td>84138</td>\n","      <td>0.088900</td>\n","      <td>0.577749</td>\n","      <td>0.868468</td>\n","    </tr>\n","    <tr>\n","      <td>84360</td>\n","      <td>0.101500</td>\n","      <td>0.600367</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>84582</td>\n","      <td>0.067700</td>\n","      <td>0.611637</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>84804</td>\n","      <td>0.093200</td>\n","      <td>0.636786</td>\n","      <td>0.859459</td>\n","    </tr>\n","    <tr>\n","      <td>85026</td>\n","      <td>0.086200</td>\n","      <td>0.629759</td>\n","      <td>0.853153</td>\n","    </tr>\n","    <tr>\n","      <td>85248</td>\n","      <td>0.076400</td>\n","      <td>0.601971</td>\n","      <td>0.868468</td>\n","    </tr>\n","    <tr>\n","      <td>85470</td>\n","      <td>0.086900</td>\n","      <td>0.637653</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>85692</td>\n","      <td>0.096200</td>\n","      <td>0.638628</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>85914</td>\n","      <td>0.076300</td>\n","      <td>0.663347</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>86136</td>\n","      <td>0.082600</td>\n","      <td>0.641798</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>86358</td>\n","      <td>0.090100</td>\n","      <td>0.632045</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>86580</td>\n","      <td>0.080500</td>\n","      <td>0.625323</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>86802</td>\n","      <td>0.073800</td>\n","      <td>0.657588</td>\n","      <td>0.847748</td>\n","    </tr>\n","    <tr>\n","      <td>87024</td>\n","      <td>0.087100</td>\n","      <td>0.667595</td>\n","      <td>0.853153</td>\n","    </tr>\n","    <tr>\n","      <td>87246</td>\n","      <td>0.127200</td>\n","      <td>0.640041</td>\n","      <td>0.854955</td>\n","    </tr>\n","    <tr>\n","      <td>87468</td>\n","      <td>0.080000</td>\n","      <td>0.613588</td>\n","      <td>0.866667</td>\n","    </tr>\n","    <tr>\n","      <td>87690</td>\n","      <td>0.087400</td>\n","      <td>0.586666</td>\n","      <td>0.872973</td>\n","    </tr>\n","    <tr>\n","      <td>87912</td>\n","      <td>0.077000</td>\n","      <td>0.616222</td>\n","      <td>0.864865</td>\n","    </tr>\n","    <tr>\n","      <td>88134</td>\n","      <td>0.089300</td>\n","      <td>0.625595</td>\n","      <td>0.860360</td>\n","    </tr>\n","    <tr>\n","      <td>88356</td>\n","      <td>0.070800</td>\n","      <td>0.603192</td>\n","      <td>0.871171</td>\n","    </tr>\n","    <tr>\n","      <td>88578</td>\n","      <td>0.092100</td>\n","      <td>0.613191</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>88800</td>\n","      <td>0.085300</td>\n","      <td>0.601441</td>\n","      <td>0.867568</td>\n","    </tr>\n","    <tr>\n","      <td>89022</td>\n","      <td>0.082500</td>\n","      <td>0.625960</td>\n","      <td>0.866667</td>\n","    </tr>\n","    <tr>\n","      <td>89244</td>\n","      <td>0.097400</td>\n","      <td>0.614611</td>\n","      <td>0.868468</td>\n","    </tr>\n","    <tr>\n","      <td>89466</td>\n","      <td>0.085500</td>\n","      <td>0.615501</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>89688</td>\n","      <td>0.066800</td>\n","      <td>0.619112</td>\n","      <td>0.861261</td>\n","    </tr>\n","    <tr>\n","      <td>89910</td>\n","      <td>0.058200</td>\n","      <td>0.596975</td>\n","      <td>0.874775</td>\n","    </tr>\n","    <tr>\n","      <td>90132</td>\n","      <td>0.107800</td>\n","      <td>0.603680</td>\n","      <td>0.868468</td>\n","    </tr>\n","    <tr>\n","      <td>90354</td>\n","      <td>0.073800</td>\n","      <td>0.627326</td>\n","      <td>0.862162</td>\n","    </tr>\n","    <tr>\n","      <td>90576</td>\n","      <td>0.083100</td>\n","      <td>0.628068</td>\n","      <td>0.862162</td>\n","    </tr>\n","    <tr>\n","      <td>90798</td>\n","      <td>0.066700</td>\n","      <td>0.638839</td>\n","      <td>0.861261</td>\n","    </tr>\n","    <tr>\n","      <td>91020</td>\n","      <td>0.065800</td>\n","      <td>0.645786</td>\n","      <td>0.859459</td>\n","    </tr>\n","    <tr>\n","      <td>91242</td>\n","      <td>0.081700</td>\n","      <td>0.632293</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>91464</td>\n","      <td>0.067500</td>\n","      <td>0.611310</td>\n","      <td>0.863964</td>\n","    </tr>\n","    <tr>\n","      <td>91686</td>\n","      <td>0.073100</td>\n","      <td>0.606847</td>\n","      <td>0.865766</td>\n","    </tr>\n","    <tr>\n","      <td>91908</td>\n","      <td>0.078100</td>\n","      <td>0.621710</td>\n","      <td>0.864865</td>\n","    </tr>\n","    <tr>\n","      <td>92130</td>\n","      <td>0.079500</td>\n","      <td>0.636442</td>\n","      <td>0.860360</td>\n","    </tr>\n","    <tr>\n","      <td>92352</td>\n","      <td>0.068000</td>\n","      <td>0.640421</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>92574</td>\n","      <td>0.122400</td>\n","      <td>0.628721</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>92796</td>\n","      <td>0.069600</td>\n","      <td>0.638495</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>93018</td>\n","      <td>0.071500</td>\n","      <td>0.633799</td>\n","      <td>0.860360</td>\n","    </tr>\n","    <tr>\n","      <td>93240</td>\n","      <td>0.081900</td>\n","      <td>0.624671</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>93462</td>\n","      <td>0.065000</td>\n","      <td>0.642274</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>93684</td>\n","      <td>0.076800</td>\n","      <td>0.656136</td>\n","      <td>0.856757</td>\n","    </tr>\n","    <tr>\n","      <td>93906</td>\n","      <td>0.065100</td>\n","      <td>0.601553</td>\n","      <td>0.866667</td>\n","    </tr>\n","    <tr>\n","      <td>94128</td>\n","      <td>0.067100</td>\n","      <td>0.649750</td>\n","      <td>0.854955</td>\n","    </tr>\n","    <tr>\n","      <td>94350</td>\n","      <td>0.103900</td>\n","      <td>0.643421</td>\n","      <td>0.857658</td>\n","    </tr>\n","    <tr>\n","      <td>94572</td>\n","      <td>0.060100</td>\n","      <td>0.665793</td>\n","      <td>0.855856</td>\n","    </tr>\n","    <tr>\n","      <td>94794</td>\n","      <td>0.071300</td>\n","      <td>0.633792</td>\n","      <td>0.862162</td>\n","    </tr>\n","    <tr>\n","      <td>95016</td>\n","      <td>0.070600</td>\n","      <td>0.644410</td>\n","      <td>0.859459</td>\n","    </tr>\n","    <tr>\n","      <td>95238</td>\n","      <td>0.071300</td>\n","      <td>0.639429</td>\n","      <td>0.862162</td>\n","    </tr>\n","    <tr>\n","      <td>95460</td>\n","      <td>0.084800</td>\n","      <td>0.631447</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>95682</td>\n","      <td>0.083400</td>\n","      <td>0.617902</td>\n","      <td>0.863063</td>\n","    </tr>\n","    <tr>\n","      <td>95904</td>\n","      <td>0.061600</td>\n","      <td>0.648226</td>\n","      <td>0.857658</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Early stopping at step 95904\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:13]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test set evaluation metrics: {'eval_loss': 0.5091816782951355, 'eval_accuracy': 0.8729729729729729, 'eval_runtime': 13.3249, 'eval_samples_per_second': 83.303, 'eval_steps_per_second': 10.432, 'epoch': 86.4}\n","Training and evaluation completed successfully!\n","Best model saved to ./results/best_model_xlsr_finetuning\n"]}],"source":["import os\n","import sys\n","import torch\n","import librosa\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model, Trainer, TrainingArguments, TrainerCallback, Wav2Vec2FeatureExtractor\n","import math\n","from datasets import load_metric\n","from datetime import datetime\n","import torch.nn as nn\n","\n","# Load the processor\n","processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","\n","# Define the custom dataset class using pandas\n","class LocalAudioDataset(Dataset):\n","    def __init__(self, csv_file, processor, subset, noise_factor=0.0):\n","        self.processor = processor\n","        self.data = pd.read_csv(csv_file)\n","        self.data = self.data[self.data['subset'] == subset]\n","        self.speaker_ids = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n","        self.data['label'] = self.data['label'].map(self.speaker_ids)\n","        self.noise_factor = noise_factor\n","        \n","        print(f\"Loaded {len(self.speaker_ids)} speakers: {self.speaker_ids}\")\n","        print(f\"Total files in {subset}: {len(self.data)}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx, retry_count=0):\n","        file_path = self.data.iloc[idx]['path']\n","        label = self.data.iloc[idx]['label']\n","        \n","        try:\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            audio = librosa.to_mono(audio)\n","            audio = self._pad_or_truncate(audio, max_length=16000)\n","            if self.noise_factor > 0:\n","                audio = self._add_noise(audio)\n","            input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.squeeze(0)\n","            return {\"input_values\": input_values, \"labels\": label}\n","        except Exception as e:\n","            if retry_count < 3:  # Retry up to 3 times\n","                return self.__getitem__((idx + 1) % len(self), retry_count + 1)\n","            else:\n","                print(f\"Error loading {file_path}: {e}\", file=sys.stderr)\n","                raise e  # Raise exception if retry limit is reached\n","\n","    def _pad_or_truncate(self, audio, max_length):\n","        if len(audio) < max_length:\n","            pad_size = max_length - len(audio)\n","            audio = np.pad(audio, (0, pad_size), 'constant', constant_values=(0, 0))\n","        else:\n","            audio = audio[:max_length]\n","        return audio\n","\n","    def _add_noise(self, audio):\n","        noise = np.random.randn(len(audio))\n","        augmented_audio = audio + self.noise_factor * noise\n","        augmented_audio = augmented_audio.astype(type(audio[0]))\n","        return augmented_audio\n","\n","# Paths to dataset CSV file\n","csv_file = 'dataset_large.csv'\n","train_dataset = LocalAudioDataset(csv_file, processor, 'train')\n","validate_dataset = LocalAudioDataset(csv_file, processor, 'validate')\n","test_dataset = LocalAudioDataset(csv_file, processor, 'test')\n","\n","num_speakers = len(train_dataset.speaker_ids)\n","print(f\"Number of unique speakers: {num_speakers}\")\n","\n","print(f\"Labels in train dataset: {train_dataset.data['label'].tolist()}\")\n","print(f\"Labels in test dataset: {test_dataset.data['label'].tolist()}\")\n","\n","# Define a custom classification head on top of the base Wav2Vec2 model\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    def __init__(self, config, num_labels):\n","        super().__init__()\n","        self.dropout = nn.Dropout(config.hidden_dropout)\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take the mean of the hidden states of the first token\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","# Define the full model by combining Wav2Vec2Model with the classification head\n","class Wav2Vec2ForCustomClassification(nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","        self.classifier = Wav2Vec2ClassificationHead(self.wav2vec2.config, num_labels)\n","\n","    def forward(self, input_values, attention_mask=None, labels=None):\n","        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state\n","        logits = self.classifier(hidden_states)\n","        \n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n","        \n","        return (loss, logits) if loss is not None else logits\n","\n","# Instantiate the model with the custom classification head\n","model = Wav2Vec2ForCustomClassification(num_labels=num_speakers)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","model = model.to(device)\n","\n","def validate_labels(dataset):\n","    for item in dataset:\n","        label = item['labels']\n","        if label >= num_speakers or label < 0:\n","            print(f\"Invalid label {label} for item: {item}\")\n","            raise ValueError(f\"Invalid label {label} found in dataset.\")\n","    print(\"All labels are valid.\")\n","\n","validate_labels(train_dataset)\n","validate_labels(validate_dataset)\n","validate_labels(test_dataset)\n","\n","batch_size = 8\n","steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n","logging_steps = steps_per_epoch // 5\n","eval_steps = steps_per_epoch // 5\n","\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return accuracy_metric.compute(predictions=predictions, references=labels)\n","\n","log_dir = \"/home/rag/experimental_trial/results/training_logs\"\n","os.makedirs(log_dir, exist_ok=True)\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","log_file = os.path.join(log_dir, f\"training_logxlsr_finetuning_{timestamp}.csv\")\n","with open(log_file, \"w\") as f:\n","    f.write(\"Timestamp,Step,Training Loss,Validation Loss,Accuracy\\n\")\n","\n","class SaveMetricsCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            with open(log_file, \"a\") as f:\n","                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","                step = state.global_step\n","                training_loss = logs.get(\"loss\", \"\")\n","                validation_loss = logs.get(\"eval_loss\", \"\")\n","                accuracy = logs.get(\"eval_accuracy\", \"\")\n","                f.write(f\"{timestamp},{step},{training_loss},{validation_loss},{accuracy}\\n\")\n","\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, early_stopping_patience=100, early_stopping_threshold=0.0):\n","        self.early_stopping_patience = early_stopping_patience\n","        self.early_stopping_threshold = early_stopping_threshold\n","        self.best_metric = None\n","        self.patience_counter = 0\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        metric = kwargs.get(\"metrics\", {}).get(\"eval_loss\")\n","        if metric is None:\n","            return\n","        \n","        if self.best_metric is None or metric < self.best_metric - self.early_stopping_threshold:\n","            self.best_metric = metric\n","            self.patience_counter = 0\n","        else:\n","            self.patience_counter += 1\n","        \n","        if self.patience_counter >= self.early_stopping_patience:\n","            print(f\"Early stopping at step {state.global_step}\")\n","            control.should_training_stop = True\n","\n","# Ensure 'no_cuda' parameter aligns with device availability\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    group_by_length=True,\n","    per_device_train_batch_size=batch_size,\n","    evaluation_strategy=\"steps\",\n","    num_train_epochs=100,\n","    save_steps=logging_steps,\n","    eval_steps=eval_steps,\n","    logging_steps=logging_steps,\n","    learning_rate=5e-6,\n","    save_total_limit=2,\n","    no_cuda=not torch.cuda.is_available(),\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,  # lower eval_loss is better\n","    save_strategy=\"steps\"  # Save checkpoints every `save_steps`\n",")\n","\n","# Add early stopping callback to the trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validate_dataset,\n","    tokenizer=processor,\n","    compute_metrics=compute_metrics,\n","    callbacks=[SaveMetricsCallback(), EarlyStoppingCallback()]  # Include early stopping\n",")\n","\n","# Train and evaluate\n","trainer.train()\n","\n","metrics = trainer.evaluate(test_dataset)\n","\n","print(f\"Test set evaluation metrics: {metrics}\")\n","print(\"Training and evaluation completed successfully!\")\n","\n","best_model_dir = \"./results/best_model_xlsr_finetuning\"\n","os.makedirs(best_model_dir, exist_ok=True)\n","\n","trainer.save_model(best_model_dir)\n","processor.save_pretrained(best_model_dir)\n","\n","print(f\"Best model saved to {best_model_dir}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["now we extract hidden states"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","100%|██████████| 50/50 [00:04<00:00, 11.70it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.26it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.64it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.00it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.15it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.95it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.45it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.66it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.51it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.08it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.36it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.33it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.90it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.38it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.97it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.48it/s]\n","100%|██████████| 50/50 [00:04<00:00, 11.67it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.90it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.96it/s]\n","100%|██████████| 50/50 [00:02<00:00, 17.82it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.46it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.15it/s]\n","100%|██████████| 50/50 [00:02<00:00, 19.33it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.30it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.64it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.32it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.79it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.57it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.32it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.98it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.31it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.85it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.58it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.47it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.59it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.38it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.52it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.31it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.92it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.98it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.08it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.00it/s]\n","100%|██████████| 50/50 [00:03<00:00, 14.09it/s]\n","100%|██████████| 50/50 [00:04<00:00, 10.90it/s]\n","100%|██████████| 50/50 [00:04<00:00, 12.06it/s]\n","100%|██████████| 50/50 [00:03<00:00, 15.38it/s]\n","100%|██████████| 50/50 [00:03<00:00, 12.97it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.82it/s]\n","100%|██████████| 50/50 [00:05<00:00,  9.63it/s]\n","100%|██████████| 50/50 [00:03<00:00, 13.36it/s]\n"]}],"source":["import os\n","import numpy as np\n","import torch\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model, Wav2Vec2FeatureExtractor\n","from tqdm import tqdm\n","import librosa\n","from safetensors.torch import load_file as safe_load\n","from torch import nn\n","\n","# Initialize the processor and model for xlsr\n","processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","finetuned_model_path = \"/home/rag/experimental_trial/results/best_model_xlsr_finetuning/model.safetensors\"\n","\n","# Define a custom classification head on top of the base Wav2Vec2 model\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    def __init__(self, config, num_labels):\n","        super().__init__()\n","        self.dropout = nn.Dropout(config.hidden_dropout)\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take the mean of the hidden states of the first token\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","# Define the full model by combining Wav2Vec2Model with the classification head\n","class Wav2Vec2ForCustomClassification(nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", output_hidden_states=True)\n","        self.classifier = Wav2Vec2ClassificationHead(self.wav2vec2.config, num_labels)\n","\n","    def forward(self, input_values, attention_mask=None, labels=None):\n","        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n","        hidden_states = outputs.hidden_states\n","        logits = self.classifier(hidden_states[-1])\n","        \n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n","        \n","        return (loss, logits, hidden_states) if loss is not None else (logits, hidden_states)\n","\n","# Instantiate the model with the custom classification head\n","model = Wav2Vec2ForCustomClassification(num_labels=111)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","state_dict = safe_load(finetuned_model_path)\n","model.load_state_dict(state_dict)\n","model.to(device)\n","\n","def check_directories_exist(directory, layer_indices):\n","    \"\"\"Prüft, ob die benötigten Verzeichnisse für jede Schicht bereits existieren.\"\"\"\n","    all_exist = True\n","    for index in layer_indices:\n","        layer_dir = os.path.join(directory, f\"layer_{index}\")\n","        if not os.path.exists(layer_dir):\n","            all_exist = False\n","            break\n","    return all_exist\n","\n","def load_audio_files(directory, layer_indices=[-1]):\n","    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n","    for filename in tqdm(os.listdir(directory)):\n","        if filename.endswith(\".mp3\"):\n","            file_path = os.path.join(directory, filename)\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n","            input_values = inputs[\"input_values\"].to(device)\n","            \n","            with torch.no_grad():\n","                logits, hidden_states = model(input_values)\n","                for index in layer_indices:\n","                    hidden_state = hidden_states[index]\n","                    # creating sub directory for each layer in speaker directory\n","                    layer_dir = os.path.join(directory, f\"layer_{index}\")\n","                    os.makedirs(layer_dir, exist_ok=True)\n","                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n","                    np.save(save_path, hidden_state.cpu().numpy())\n","\n","def process_audio_directory(base_directory, layer_indices=range(25)):\n","    \"\"\"Verarbeitet Audio-Dateien in den angegebenen Verzeichnissen, falls die Ziellayer-Verzeichnisse noch nicht existieren.\"\"\"\n","    for d in os.listdir(base_directory):\n","        dir_path = os.path.join(base_directory, d)\n","        if os.path.isdir(dir_path) and not check_directories_exist(dir_path, layer_indices):\n","            load_audio_files(dir_path, layer_indices)\n","\n","directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_xlrs_finetuned\")\n","\n","process_audio_directory(directory_path)\n"]},{"cell_type":"markdown","metadata":{},"source":["# now we use optuna to optimize the number of parameter used "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 111 speakers: {'speaker_6': 0, 'speaker_156': 1, 'speaker_22': 2, 'speaker_19': 3, 'speaker_91': 4, 'speaker_27': 5, 'speaker_94': 6, 'speaker_34': 7, 'speaker_97': 8, 'speaker_100': 9, 'speaker_36': 10, 'speaker_128': 11, 'speaker_134': 12, 'speaker_68': 13, 'speaker_9': 14, 'speaker_17': 15, 'speaker_73': 16, 'speaker_42': 17, 'speaker_52': 18, 'speaker_151': 19, 'speaker_150': 20, 'speaker_141': 21, 'speaker_82': 22, 'speaker_130': 23, 'speaker_75': 24, 'speaker_58': 25, 'speaker_74': 26, 'speaker_104': 27, 'speaker_47': 28, 'speaker_135': 29, 'speaker_71': 30, 'speaker_83': 31, 'speaker_116': 32, 'speaker_99': 33, 'speaker_108': 34, 'speaker_31': 35, 'speaker_106': 36, 'speaker_28': 37, 'speaker_65': 38, 'speaker_48': 39, 'speaker_49': 40, 'speaker_53': 41, 'speaker_3': 42, 'speaker_63': 43, 'speaker_138': 44, 'speaker_98': 45, 'speaker_92': 46, 'speaker_123': 47, 'speaker_32': 48, 'speaker_10': 49, 'speaker_155': 50, 'speaker_153': 51, 'speaker_23': 52, 'speaker_59': 53, 'speaker_56': 54, 'speaker_101': 55, 'speaker_26': 56, 'speaker_30': 57, 'speaker_140': 58, 'speaker_35': 59, 'speaker_93': 60, 'speaker_66': 61, 'speaker_62': 62, 'speaker_137': 63, 'speaker_125': 64, 'speaker_157': 65, 'speaker_13': 66, 'speaker_152': 67, 'speaker_25': 68, 'speaker_89': 69, 'speaker_118': 70, 'speaker_16': 71, 'speaker_70': 72, 'speaker_144': 73, 'speaker_102': 74, 'speaker_43': 75, 'speaker_96': 76, 'speaker_131': 77, 'speaker_87': 78, 'speaker_39': 79, 'speaker_8': 80, 'speaker_51': 81, 'speaker_115': 82, 'speaker_158': 83, 'speaker_107': 84, 'speaker_119': 85, 'speaker_77': 86, 'speaker_2': 87, 'speaker_46': 88, 'speaker_84': 89, 'speaker_126': 90, 'speaker_85': 91, 'speaker_109': 92, 'speaker_11': 93, 'speaker_129': 94, 'speaker_86': 95, 'speaker_121': 96, 'speaker_41': 97, 'speaker_113': 98, 'speaker_76': 99, 'speaker_127': 100, 'speaker_154': 101, 'speaker_120': 102, 'speaker_78': 103, 'speaker_122': 104, 'speaker_117': 105, 'speaker_64': 106, 'speaker_133': 107, 'speaker_80': 108, 'speaker_124': 109, 'speaker_50': 110}\n","Total files in train: 8880\n","Loaded 111 speakers: {'speaker_6': 0, 'speaker_156': 1, 'speaker_22': 2, 'speaker_19': 3, 'speaker_91': 4, 'speaker_27': 5, 'speaker_94': 6, 'speaker_34': 7, 'speaker_97': 8, 'speaker_100': 9, 'speaker_36': 10, 'speaker_128': 11, 'speaker_134': 12, 'speaker_68': 13, 'speaker_9': 14, 'speaker_17': 15, 'speaker_73': 16, 'speaker_42': 17, 'speaker_52': 18, 'speaker_151': 19, 'speaker_150': 20, 'speaker_141': 21, 'speaker_82': 22, 'speaker_130': 23, 'speaker_75': 24, 'speaker_58': 25, 'speaker_74': 26, 'speaker_104': 27, 'speaker_47': 28, 'speaker_135': 29, 'speaker_71': 30, 'speaker_83': 31, 'speaker_116': 32, 'speaker_99': 33, 'speaker_108': 34, 'speaker_31': 35, 'speaker_106': 36, 'speaker_28': 37, 'speaker_65': 38, 'speaker_48': 39, 'speaker_49': 40, 'speaker_53': 41, 'speaker_3': 42, 'speaker_63': 43, 'speaker_138': 44, 'speaker_98': 45, 'speaker_92': 46, 'speaker_123': 47, 'speaker_32': 48, 'speaker_10': 49, 'speaker_155': 50, 'speaker_153': 51, 'speaker_23': 52, 'speaker_59': 53, 'speaker_56': 54, 'speaker_101': 55, 'speaker_26': 56, 'speaker_30': 57, 'speaker_140': 58, 'speaker_35': 59, 'speaker_93': 60, 'speaker_66': 61, 'speaker_62': 62, 'speaker_137': 63, 'speaker_125': 64, 'speaker_157': 65, 'speaker_13': 66, 'speaker_152': 67, 'speaker_25': 68, 'speaker_89': 69, 'speaker_118': 70, 'speaker_16': 71, 'speaker_70': 72, 'speaker_144': 73, 'speaker_102': 74, 'speaker_43': 75, 'speaker_96': 76, 'speaker_131': 77, 'speaker_87': 78, 'speaker_39': 79, 'speaker_8': 80, 'speaker_51': 81, 'speaker_115': 82, 'speaker_158': 83, 'speaker_107': 84, 'speaker_119': 85, 'speaker_77': 86, 'speaker_2': 87, 'speaker_46': 88, 'speaker_84': 89, 'speaker_126': 90, 'speaker_85': 91, 'speaker_109': 92, 'speaker_11': 93, 'speaker_129': 94, 'speaker_86': 95, 'speaker_121': 96, 'speaker_41': 97, 'speaker_113': 98, 'speaker_76': 99, 'speaker_127': 100, 'speaker_154': 101, 'speaker_120': 102, 'speaker_78': 103, 'speaker_122': 104, 'speaker_117': 105, 'speaker_64': 106, 'speaker_133': 107, 'speaker_80': 108, 'speaker_124': 109, 'speaker_50': 110}\n","Total files in validate: 1110\n","Loaded 111 speakers: {'speaker_6': 0, 'speaker_156': 1, 'speaker_22': 2, 'speaker_19': 3, 'speaker_91': 4, 'speaker_27': 5, 'speaker_94': 6, 'speaker_34': 7, 'speaker_97': 8, 'speaker_100': 9, 'speaker_36': 10, 'speaker_128': 11, 'speaker_134': 12, 'speaker_68': 13, 'speaker_9': 14, 'speaker_17': 15, 'speaker_73': 16, 'speaker_42': 17, 'speaker_52': 18, 'speaker_151': 19, 'speaker_150': 20, 'speaker_141': 21, 'speaker_82': 22, 'speaker_130': 23, 'speaker_75': 24, 'speaker_58': 25, 'speaker_74': 26, 'speaker_104': 27, 'speaker_47': 28, 'speaker_135': 29, 'speaker_71': 30, 'speaker_83': 31, 'speaker_116': 32, 'speaker_99': 33, 'speaker_108': 34, 'speaker_31': 35, 'speaker_106': 36, 'speaker_28': 37, 'speaker_65': 38, 'speaker_48': 39, 'speaker_49': 40, 'speaker_53': 41, 'speaker_3': 42, 'speaker_63': 43, 'speaker_138': 44, 'speaker_98': 45, 'speaker_92': 46, 'speaker_123': 47, 'speaker_32': 48, 'speaker_10': 49, 'speaker_155': 50, 'speaker_153': 51, 'speaker_23': 52, 'speaker_59': 53, 'speaker_56': 54, 'speaker_101': 55, 'speaker_26': 56, 'speaker_30': 57, 'speaker_140': 58, 'speaker_35': 59, 'speaker_93': 60, 'speaker_66': 61, 'speaker_62': 62, 'speaker_137': 63, 'speaker_125': 64, 'speaker_157': 65, 'speaker_13': 66, 'speaker_152': 67, 'speaker_25': 68, 'speaker_89': 69, 'speaker_118': 70, 'speaker_16': 71, 'speaker_70': 72, 'speaker_144': 73, 'speaker_102': 74, 'speaker_43': 75, 'speaker_96': 76, 'speaker_131': 77, 'speaker_87': 78, 'speaker_39': 79, 'speaker_8': 80, 'speaker_51': 81, 'speaker_115': 82, 'speaker_158': 83, 'speaker_107': 84, 'speaker_119': 85, 'speaker_77': 86, 'speaker_2': 87, 'speaker_46': 88, 'speaker_84': 89, 'speaker_126': 90, 'speaker_85': 91, 'speaker_109': 92, 'speaker_11': 93, 'speaker_129': 94, 'speaker_86': 95, 'speaker_121': 96, 'speaker_41': 97, 'speaker_113': 98, 'speaker_76': 99, 'speaker_127': 100, 'speaker_154': 101, 'speaker_120': 102, 'speaker_78': 103, 'speaker_122': 104, 'speaker_117': 105, 'speaker_64': 106, 'speaker_133': 107, 'speaker_80': 108, 'speaker_124': 109, 'speaker_50': 110}\n","Total files in test: 1110\n","Number of unique speakers: 111\n","Labels in train dataset: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110]\n","Labels in test dataset: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 59, 59, 59, 59, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 73, 73, 73, 73, 73, 73, 73, 73, 73, 73, 74, 74, 74, 74, 74, 74, 74, 74, 74, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 76, 76, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 77, 77, 77, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 79, 79, 79, 79, 79, 79, 79, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 81, 81, 81, 81, 81, 81, 81, 81, 81, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 83, 83, 83, 83, 83, 83, 83, 83, 83, 83, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 92, 92, 92, 92, 92, 92, 92, 92, 92, 92, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 94, 94, 94, 94, 94, 94, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 97, 97, 97, 97, 97, 97, 97, 97, 97, 97, 98, 98, 98, 98, 98, 98, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 103, 103, 103, 103, 103, 103, 103, 103, 103, 103, 104, 104, 104, 104, 104, 104, 104, 104, 104, 104, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 106, 106, 106, 106, 106, 106, 106, 106, 106, 106, 107, 107, 107, 107, 107, 107, 107, 107, 107, 107, 108, 108, 108, 108, 108, 108, 108, 108, 108, 108, 109, 109, 109, 109, 109, 109, 109, 109, 109, 109, 110, 110, 110, 110, 110, 110, 110, 110, 110, 110]\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","All labels are valid.\n","All labels are valid.\n","All labels are valid.\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_677326/2381888018.py:139: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n","  accuracy_metric = load_metric(\"accuracy\")\n","/home/rag/base_venv/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n","[I 2024-06-18 13:32:37,615] A new study created in memory with name: no-name-7d154c77-f6c2-45e6-9c2a-f7d86fc663cb\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","/home/rag/base_venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv1d(input, weight, bias, self.stride,\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 30:25, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708000</td>\n","      <td>4.701856</td>\n","      <td>0.016216</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.702800</td>\n","      <td>4.689518</td>\n","      <td>0.024324</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.693300</td>\n","      <td>4.675404</td>\n","      <td>0.036036</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.681900</td>\n","      <td>4.656057</td>\n","      <td>0.065766</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.669400</td>\n","      <td>4.632829</td>\n","      <td>0.074775</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.654600</td>\n","      <td>4.609973</td>\n","      <td>0.105405</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.631400</td>\n","      <td>4.581596</td>\n","      <td>0.116216</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.613900</td>\n","      <td>4.551846</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.588800</td>\n","      <td>4.521278</td>\n","      <td>0.116216</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.565200</td>\n","      <td>4.481288</td>\n","      <td>0.118919</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.529200</td>\n","      <td>4.450294</td>\n","      <td>0.102703</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.504800</td>\n","      <td>4.412663</td>\n","      <td>0.117117</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.482200</td>\n","      <td>4.380915</td>\n","      <td>0.105405</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.453700</td>\n","      <td>4.346789</td>\n","      <td>0.114414</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.423000</td>\n","      <td>4.313585</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.389800</td>\n","      <td>4.275049</td>\n","      <td>0.122523</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.357900</td>\n","      <td>4.236281</td>\n","      <td>0.109910</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.328100</td>\n","      <td>4.204026</td>\n","      <td>0.115315</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.302700</td>\n","      <td>4.170482</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.257400</td>\n","      <td>4.138823</td>\n","      <td>0.130631</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.235400</td>\n","      <td>4.108301</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.210900</td>\n","      <td>4.079746</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.165000</td>\n","      <td>4.053044</td>\n","      <td>0.151351</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.148400</td>\n","      <td>4.027901</td>\n","      <td>0.154955</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.127300</td>\n","      <td>3.999754</td>\n","      <td>0.160360</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.095600</td>\n","      <td>3.975323</td>\n","      <td>0.173874</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.078000</td>\n","      <td>3.956824</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.057800</td>\n","      <td>3.932323</td>\n","      <td>0.176577</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.033200</td>\n","      <td>3.913092</td>\n","      <td>0.196396</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.013800</td>\n","      <td>3.891020</td>\n","      <td>0.193694</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.988600</td>\n","      <td>3.871554</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.975600</td>\n","      <td>3.854910</td>\n","      <td>0.218919</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.957100</td>\n","      <td>3.836238</td>\n","      <td>0.227928</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.921500</td>\n","      <td>3.825710</td>\n","      <td>0.232432</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.927600</td>\n","      <td>3.806956</td>\n","      <td>0.230631</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.895700</td>\n","      <td>3.791841</td>\n","      <td>0.227027</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.905100</td>\n","      <td>3.779815</td>\n","      <td>0.223423</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.867400</td>\n","      <td>3.766569</td>\n","      <td>0.242342</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.897200</td>\n","      <td>3.755154</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.826800</td>\n","      <td>3.746001</td>\n","      <td>0.247748</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.874100</td>\n","      <td>3.738703</td>\n","      <td>0.246847</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.840700</td>\n","      <td>3.730690</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.819100</td>\n","      <td>3.723104</td>\n","      <td>0.252252</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.837300</td>\n","      <td>3.719742</td>\n","      <td>0.257658</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.804400</td>\n","      <td>3.715192</td>\n","      <td>0.264865</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.830000</td>\n","      <td>3.709055</td>\n","      <td>0.252252</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.812600</td>\n","      <td>3.704573</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.793300</td>\n","      <td>3.702345</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.813300</td>\n","      <td>3.700446</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.810200</td>\n","      <td>3.700019</td>\n","      <td>0.256757</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 14:04:04,138] Trial 0 finished with value: 3.700018882751465 and parameters: {'num_layers': 6}. Best is trial 0 with value: 3.700018882751465.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 45:21, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708800</td>\n","      <td>4.705050</td>\n","      <td>0.013514</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.705800</td>\n","      <td>4.699969</td>\n","      <td>0.016216</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.703000</td>\n","      <td>4.690352</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.699600</td>\n","      <td>4.679365</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.691800</td>\n","      <td>4.664335</td>\n","      <td>0.040541</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.676300</td>\n","      <td>4.648576</td>\n","      <td>0.045946</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.667800</td>\n","      <td>4.626786</td>\n","      <td>0.053153</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.654100</td>\n","      <td>4.604692</td>\n","      <td>0.073874</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.639800</td>\n","      <td>4.573865</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.617000</td>\n","      <td>4.549234</td>\n","      <td>0.071171</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.600200</td>\n","      <td>4.517234</td>\n","      <td>0.070270</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.573700</td>\n","      <td>4.482484</td>\n","      <td>0.071171</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.538100</td>\n","      <td>4.448764</td>\n","      <td>0.071171</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.521600</td>\n","      <td>4.414910</td>\n","      <td>0.069369</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.483200</td>\n","      <td>4.377725</td>\n","      <td>0.069369</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.452100</td>\n","      <td>4.345842</td>\n","      <td>0.075676</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.431600</td>\n","      <td>4.320053</td>\n","      <td>0.072973</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.391900</td>\n","      <td>4.279597</td>\n","      <td>0.088288</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.349400</td>\n","      <td>4.242125</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.336400</td>\n","      <td>4.213527</td>\n","      <td>0.095495</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.282900</td>\n","      <td>4.179115</td>\n","      <td>0.094595</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.255000</td>\n","      <td>4.148315</td>\n","      <td>0.113514</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.232000</td>\n","      <td>4.119372</td>\n","      <td>0.120721</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.202100</td>\n","      <td>4.095745</td>\n","      <td>0.109009</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.171900</td>\n","      <td>4.065402</td>\n","      <td>0.112613</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.145200</td>\n","      <td>4.038147</td>\n","      <td>0.112613</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.129100</td>\n","      <td>4.025397</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.098000</td>\n","      <td>3.997353</td>\n","      <td>0.120721</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.094200</td>\n","      <td>3.977555</td>\n","      <td>0.125225</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.075800</td>\n","      <td>3.963279</td>\n","      <td>0.122523</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.043500</td>\n","      <td>3.943785</td>\n","      <td>0.138739</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.018800</td>\n","      <td>3.930295</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.020300</td>\n","      <td>3.922332</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.974800</td>\n","      <td>3.897749</td>\n","      <td>0.132432</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.995500</td>\n","      <td>3.886584</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.952400</td>\n","      <td>3.874014</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.952200</td>\n","      <td>3.851353</td>\n","      <td>0.142342</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.945100</td>\n","      <td>3.848419</td>\n","      <td>0.138739</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.946800</td>\n","      <td>3.836332</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.924800</td>\n","      <td>3.838434</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.922900</td>\n","      <td>3.823117</td>\n","      <td>0.140541</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.906200</td>\n","      <td>3.817342</td>\n","      <td>0.142342</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.892200</td>\n","      <td>3.807722</td>\n","      <td>0.148649</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.895000</td>\n","      <td>3.804473</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.888200</td>\n","      <td>3.799090</td>\n","      <td>0.151351</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.893000</td>\n","      <td>3.794405</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.890900</td>\n","      <td>3.787852</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.864400</td>\n","      <td>3.791140</td>\n","      <td>0.146847</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.875500</td>\n","      <td>3.788598</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.851300</td>\n","      <td>3.787920</td>\n","      <td>0.150450</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 14:50:20,789] Trial 1 finished with value: 3.7878518104553223 and parameters: {'num_layers': 17}. Best is trial 0 with value: 3.700018882751465.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 28:31, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.707000</td>\n","      <td>4.701768</td>\n","      <td>0.018919</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.704500</td>\n","      <td>4.690088</td>\n","      <td>0.023423</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.693000</td>\n","      <td>4.676493</td>\n","      <td>0.047748</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.684200</td>\n","      <td>4.657732</td>\n","      <td>0.064865</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.673500</td>\n","      <td>4.634727</td>\n","      <td>0.090991</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.654000</td>\n","      <td>4.605322</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.634800</td>\n","      <td>4.574787</td>\n","      <td>0.120721</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.615800</td>\n","      <td>4.539820</td>\n","      <td>0.151351</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.592000</td>\n","      <td>4.506039</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.565600</td>\n","      <td>4.470708</td>\n","      <td>0.133333</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.526400</td>\n","      <td>4.433433</td>\n","      <td>0.149550</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.507200</td>\n","      <td>4.393464</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.462400</td>\n","      <td>4.352920</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.446900</td>\n","      <td>4.315761</td>\n","      <td>0.174775</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.410200</td>\n","      <td>4.277171</td>\n","      <td>0.170270</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.387800</td>\n","      <td>4.246192</td>\n","      <td>0.174775</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.342600</td>\n","      <td>4.206381</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.315400</td>\n","      <td>4.171531</td>\n","      <td>0.163063</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.279400</td>\n","      <td>4.136498</td>\n","      <td>0.181982</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.242700</td>\n","      <td>4.102133</td>\n","      <td>0.181982</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.218900</td>\n","      <td>4.073676</td>\n","      <td>0.190090</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.188200</td>\n","      <td>4.042323</td>\n","      <td>0.202703</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.159300</td>\n","      <td>4.012592</td>\n","      <td>0.206306</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.134100</td>\n","      <td>3.986319</td>\n","      <td>0.202703</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.099800</td>\n","      <td>3.961750</td>\n","      <td>0.227027</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.101800</td>\n","      <td>3.937200</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.056900</td>\n","      <td>3.916283</td>\n","      <td>0.213514</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.048300</td>\n","      <td>3.891902</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.026600</td>\n","      <td>3.871851</td>\n","      <td>0.214414</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.013200</td>\n","      <td>3.851511</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.971800</td>\n","      <td>3.834013</td>\n","      <td>0.224324</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.957400</td>\n","      <td>3.814631</td>\n","      <td>0.233333</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.944800</td>\n","      <td>3.799175</td>\n","      <td>0.237838</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.949300</td>\n","      <td>3.783537</td>\n","      <td>0.238739</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.928000</td>\n","      <td>3.767664</td>\n","      <td>0.242342</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.913800</td>\n","      <td>3.753765</td>\n","      <td>0.243243</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.867800</td>\n","      <td>3.742715</td>\n","      <td>0.246847</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.884300</td>\n","      <td>3.732322</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.869200</td>\n","      <td>3.721836</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.868200</td>\n","      <td>3.711885</td>\n","      <td>0.257658</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.876600</td>\n","      <td>3.703568</td>\n","      <td>0.251351</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.831600</td>\n","      <td>3.695764</td>\n","      <td>0.258559</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.850700</td>\n","      <td>3.690546</td>\n","      <td>0.253153</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.840200</td>\n","      <td>3.685672</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.817700</td>\n","      <td>3.678678</td>\n","      <td>0.262162</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.822200</td>\n","      <td>3.674911</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.806500</td>\n","      <td>3.671673</td>\n","      <td>0.263063</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.832800</td>\n","      <td>3.669478</td>\n","      <td>0.260360</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.792800</td>\n","      <td>3.668038</td>\n","      <td>0.261261</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.808500</td>\n","      <td>3.667564</td>\n","      <td>0.262162</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 15:19:47,137] Trial 2 finished with value: 3.6675639152526855 and parameters: {'num_layers': 4}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 58:51, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709100</td>\n","      <td>4.709498</td>\n","      <td>0.009009</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.707500</td>\n","      <td>4.706626</td>\n","      <td>0.004505</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.701200</td>\n","      <td>4.701430</td>\n","      <td>0.008108</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.694800</td>\n","      <td>4.697382</td>\n","      <td>0.018919</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.687500</td>\n","      <td>4.687127</td>\n","      <td>0.034234</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.672000</td>\n","      <td>4.682913</td>\n","      <td>0.021622</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.661200</td>\n","      <td>4.663531</td>\n","      <td>0.040541</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.639400</td>\n","      <td>4.634953</td>\n","      <td>0.048649</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.625400</td>\n","      <td>4.609554</td>\n","      <td>0.050450</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.602500</td>\n","      <td>4.575008</td>\n","      <td>0.057658</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.570100</td>\n","      <td>4.536061</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.537800</td>\n","      <td>4.502331</td>\n","      <td>0.093694</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.506500</td>\n","      <td>4.461547</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.467900</td>\n","      <td>4.427675</td>\n","      <td>0.102703</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.434800</td>\n","      <td>4.394350</td>\n","      <td>0.094595</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.407000</td>\n","      <td>4.357834</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.361300</td>\n","      <td>4.318393</td>\n","      <td>0.127027</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.317900</td>\n","      <td>4.275436</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.284600</td>\n","      <td>4.244633</td>\n","      <td>0.136036</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.264700</td>\n","      <td>4.216039</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.223600</td>\n","      <td>4.192859</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.194500</td>\n","      <td>4.149289</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.170700</td>\n","      <td>4.129191</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.131900</td>\n","      <td>4.090184</td>\n","      <td>0.166667</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.112200</td>\n","      <td>4.065620</td>\n","      <td>0.185586</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.074400</td>\n","      <td>4.050172</td>\n","      <td>0.182883</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.077600</td>\n","      <td>4.028419</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.044500</td>\n","      <td>3.996400</td>\n","      <td>0.196396</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.011600</td>\n","      <td>3.980334</td>\n","      <td>0.205405</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>3.993900</td>\n","      <td>3.969758</td>\n","      <td>0.203604</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.988900</td>\n","      <td>3.950012</td>\n","      <td>0.214414</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.961200</td>\n","      <td>3.929278</td>\n","      <td>0.223423</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.938200</td>\n","      <td>3.903035</td>\n","      <td>0.227928</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.943600</td>\n","      <td>3.892821</td>\n","      <td>0.222523</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.900600</td>\n","      <td>3.883996</td>\n","      <td>0.225225</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.889800</td>\n","      <td>3.859256</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.874900</td>\n","      <td>3.842371</td>\n","      <td>0.235135</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.861500</td>\n","      <td>3.829768</td>\n","      <td>0.237838</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.836100</td>\n","      <td>3.817925</td>\n","      <td>0.239640</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.844600</td>\n","      <td>3.814267</td>\n","      <td>0.238739</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.824600</td>\n","      <td>3.786792</td>\n","      <td>0.260360</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.805500</td>\n","      <td>3.789843</td>\n","      <td>0.251351</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.805500</td>\n","      <td>3.768992</td>\n","      <td>0.262162</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.805000</td>\n","      <td>3.768307</td>\n","      <td>0.263063</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.779900</td>\n","      <td>3.759068</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.770400</td>\n","      <td>3.760789</td>\n","      <td>0.252252</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.773700</td>\n","      <td>3.751913</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.776800</td>\n","      <td>3.748971</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.767500</td>\n","      <td>3.750349</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.779900</td>\n","      <td>3.749176</td>\n","      <td>0.256757</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:13]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 16:19:43,081] Trial 3 finished with value: 3.748971462249756 and parameters: {'num_layers': 24}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 30:59, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708300</td>\n","      <td>4.701194</td>\n","      <td>0.020721</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.706800</td>\n","      <td>4.690593</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.694800</td>\n","      <td>4.677840</td>\n","      <td>0.038739</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.685200</td>\n","      <td>4.661039</td>\n","      <td>0.051351</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.680800</td>\n","      <td>4.644998</td>\n","      <td>0.071171</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.662800</td>\n","      <td>4.621366</td>\n","      <td>0.109910</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.646500</td>\n","      <td>4.598451</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.634000</td>\n","      <td>4.574369</td>\n","      <td>0.138739</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.616400</td>\n","      <td>4.547435</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.592600</td>\n","      <td>4.515973</td>\n","      <td>0.144144</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.560800</td>\n","      <td>4.485220</td>\n","      <td>0.146847</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.544100</td>\n","      <td>4.453067</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.504800</td>\n","      <td>4.417603</td>\n","      <td>0.140541</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.496100</td>\n","      <td>4.386225</td>\n","      <td>0.154955</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.463000</td>\n","      <td>4.357003</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.442700</td>\n","      <td>4.325265</td>\n","      <td>0.157658</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.412300</td>\n","      <td>4.292655</td>\n","      <td>0.167568</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.378600</td>\n","      <td>4.259297</td>\n","      <td>0.164865</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.352100</td>\n","      <td>4.226234</td>\n","      <td>0.183784</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.315700</td>\n","      <td>4.193959</td>\n","      <td>0.172973</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.295000</td>\n","      <td>4.167820</td>\n","      <td>0.174775</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.268300</td>\n","      <td>4.137499</td>\n","      <td>0.190991</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.244000</td>\n","      <td>4.110568</td>\n","      <td>0.188288</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.216800</td>\n","      <td>4.082216</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.185400</td>\n","      <td>4.055642</td>\n","      <td>0.209910</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.181700</td>\n","      <td>4.033346</td>\n","      <td>0.208108</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.146400</td>\n","      <td>4.013234</td>\n","      <td>0.209009</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.133000</td>\n","      <td>3.986117</td>\n","      <td>0.225225</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.106300</td>\n","      <td>3.968717</td>\n","      <td>0.228829</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.101800</td>\n","      <td>3.945478</td>\n","      <td>0.228829</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.057800</td>\n","      <td>3.925459</td>\n","      <td>0.223423</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.037400</td>\n","      <td>3.907660</td>\n","      <td>0.227027</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.025900</td>\n","      <td>3.891247</td>\n","      <td>0.231532</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.033700</td>\n","      <td>3.877691</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>4.007700</td>\n","      <td>3.860911</td>\n","      <td>0.244144</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.996200</td>\n","      <td>3.845925</td>\n","      <td>0.253153</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.952600</td>\n","      <td>3.833790</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.964100</td>\n","      <td>3.820328</td>\n","      <td>0.262162</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.957500</td>\n","      <td>3.809428</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.946900</td>\n","      <td>3.798950</td>\n","      <td>0.263964</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.945800</td>\n","      <td>3.788476</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.915800</td>\n","      <td>3.779425</td>\n","      <td>0.260360</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.931000</td>\n","      <td>3.774375</td>\n","      <td>0.253153</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.920800</td>\n","      <td>3.769133</td>\n","      <td>0.263063</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.900700</td>\n","      <td>3.761356</td>\n","      <td>0.263063</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.898400</td>\n","      <td>3.758069</td>\n","      <td>0.264865</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.883700</td>\n","      <td>3.754281</td>\n","      <td>0.266667</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.906800</td>\n","      <td>3.752023</td>\n","      <td>0.266667</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.874500</td>\n","      <td>3.750479</td>\n","      <td>0.267568</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.885200</td>\n","      <td>3.749874</td>\n","      <td>0.267568</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 16:51:38,106] Trial 4 finished with value: 3.7498741149902344 and parameters: {'num_layers': 4}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 54:36, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.711200</td>\n","      <td>4.705831</td>\n","      <td>0.016216</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.708400</td>\n","      <td>4.702398</td>\n","      <td>0.015315</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.702900</td>\n","      <td>4.696335</td>\n","      <td>0.027027</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.700100</td>\n","      <td>4.687635</td>\n","      <td>0.039640</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.693400</td>\n","      <td>4.676097</td>\n","      <td>0.051351</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.685400</td>\n","      <td>4.661163</td>\n","      <td>0.056757</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.675700</td>\n","      <td>4.637342</td>\n","      <td>0.081982</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.659500</td>\n","      <td>4.614371</td>\n","      <td>0.103604</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.642600</td>\n","      <td>4.588152</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.625800</td>\n","      <td>4.560373</td>\n","      <td>0.122523</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.594300</td>\n","      <td>4.524986</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.571900</td>\n","      <td>4.488932</td>\n","      <td>0.145946</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.543400</td>\n","      <td>4.448090</td>\n","      <td>0.146847</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.517500</td>\n","      <td>4.409671</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.482500</td>\n","      <td>4.372098</td>\n","      <td>0.145946</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.444500</td>\n","      <td>4.327139</td>\n","      <td>0.141441</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.410600</td>\n","      <td>4.289383</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.373700</td>\n","      <td>4.254826</td>\n","      <td>0.124324</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.326500</td>\n","      <td>4.214815</td>\n","      <td>0.129730</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.293900</td>\n","      <td>4.179906</td>\n","      <td>0.136036</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.254600</td>\n","      <td>4.153387</td>\n","      <td>0.138739</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.240600</td>\n","      <td>4.119223</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.202800</td>\n","      <td>4.091010</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.174900</td>\n","      <td>4.065492</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.162400</td>\n","      <td>4.043022</td>\n","      <td>0.151351</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.114100</td>\n","      <td>4.011876</td>\n","      <td>0.162162</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.099800</td>\n","      <td>3.989797</td>\n","      <td>0.168468</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.079100</td>\n","      <td>3.964779</td>\n","      <td>0.183784</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.025100</td>\n","      <td>3.944906</td>\n","      <td>0.190991</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.028600</td>\n","      <td>3.927418</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.013300</td>\n","      <td>3.913224</td>\n","      <td>0.190090</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.981900</td>\n","      <td>3.888771</td>\n","      <td>0.206306</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.974200</td>\n","      <td>3.880123</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.938400</td>\n","      <td>3.862991</td>\n","      <td>0.195495</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.939300</td>\n","      <td>3.838701</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.913600</td>\n","      <td>3.829768</td>\n","      <td>0.205405</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.902900</td>\n","      <td>3.814790</td>\n","      <td>0.218018</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.905200</td>\n","      <td>3.811102</td>\n","      <td>0.221622</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.891100</td>\n","      <td>3.791110</td>\n","      <td>0.219820</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.878600</td>\n","      <td>3.777987</td>\n","      <td>0.226126</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.858500</td>\n","      <td>3.768918</td>\n","      <td>0.234234</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.839000</td>\n","      <td>3.761950</td>\n","      <td>0.238739</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.831100</td>\n","      <td>3.756822</td>\n","      <td>0.235135</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.822000</td>\n","      <td>3.746464</td>\n","      <td>0.248649</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.840800</td>\n","      <td>3.743724</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.812600</td>\n","      <td>3.733363</td>\n","      <td>0.250450</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.821400</td>\n","      <td>3.736216</td>\n","      <td>0.248649</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.802200</td>\n","      <td>3.731967</td>\n","      <td>0.248649</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.808900</td>\n","      <td>3.732638</td>\n","      <td>0.249550</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.812500</td>\n","      <td>3.731489</td>\n","      <td>0.250450</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:13]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 17:47:18,181] Trial 5 finished with value: 3.731489419937134 and parameters: {'num_layers': 21}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 57:02, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709200</td>\n","      <td>4.707587</td>\n","      <td>0.009009</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.708500</td>\n","      <td>4.707574</td>\n","      <td>0.009009</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.704700</td>\n","      <td>4.694062</td>\n","      <td>0.019820</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.700000</td>\n","      <td>4.696763</td>\n","      <td>0.016216</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.693500</td>\n","      <td>4.678085</td>\n","      <td>0.020721</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.682700</td>\n","      <td>4.666332</td>\n","      <td>0.023423</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.665600</td>\n","      <td>4.651262</td>\n","      <td>0.022523</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.655000</td>\n","      <td>4.635927</td>\n","      <td>0.028829</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.643700</td>\n","      <td>4.618079</td>\n","      <td>0.027928</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.631300</td>\n","      <td>4.601048</td>\n","      <td>0.028829</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.599000</td>\n","      <td>4.573476</td>\n","      <td>0.042342</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.579700</td>\n","      <td>4.548142</td>\n","      <td>0.048649</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.560500</td>\n","      <td>4.525317</td>\n","      <td>0.049550</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.546300</td>\n","      <td>4.503197</td>\n","      <td>0.059459</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.525900</td>\n","      <td>4.477688</td>\n","      <td>0.051351</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.480200</td>\n","      <td>4.446368</td>\n","      <td>0.058559</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.468200</td>\n","      <td>4.432839</td>\n","      <td>0.060360</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.445100</td>\n","      <td>4.393147</td>\n","      <td>0.076577</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.422100</td>\n","      <td>4.363751</td>\n","      <td>0.079279</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.388100</td>\n","      <td>4.334110</td>\n","      <td>0.086486</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.359500</td>\n","      <td>4.311521</td>\n","      <td>0.091892</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.343200</td>\n","      <td>4.283757</td>\n","      <td>0.087387</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.326700</td>\n","      <td>4.259099</td>\n","      <td>0.093694</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.292500</td>\n","      <td>4.254761</td>\n","      <td>0.095495</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.269800</td>\n","      <td>4.216080</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.245500</td>\n","      <td>4.194061</td>\n","      <td>0.108108</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.227200</td>\n","      <td>4.172600</td>\n","      <td>0.109009</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.192200</td>\n","      <td>4.148956</td>\n","      <td>0.102703</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.182100</td>\n","      <td>4.138188</td>\n","      <td>0.095495</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.157500</td>\n","      <td>4.104715</td>\n","      <td>0.112613</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.141800</td>\n","      <td>4.088309</td>\n","      <td>0.109009</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.127400</td>\n","      <td>4.071943</td>\n","      <td>0.110811</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.132900</td>\n","      <td>4.054162</td>\n","      <td>0.120721</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.103600</td>\n","      <td>4.041310</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>4.100900</td>\n","      <td>4.023479</td>\n","      <td>0.125225</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>4.102200</td>\n","      <td>4.011960</td>\n","      <td>0.131532</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>4.062900</td>\n","      <td>4.004694</td>\n","      <td>0.132432</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>4.046000</td>\n","      <td>3.982445</td>\n","      <td>0.136937</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>4.035600</td>\n","      <td>3.976648</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>4.028200</td>\n","      <td>3.969609</td>\n","      <td>0.136036</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>4.031900</td>\n","      <td>3.959837</td>\n","      <td>0.145946</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>4.002600</td>\n","      <td>3.947763</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.997100</td>\n","      <td>3.945447</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>4.012700</td>\n","      <td>3.936359</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>4.007800</td>\n","      <td>3.935278</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.995700</td>\n","      <td>3.930329</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.993200</td>\n","      <td>3.925938</td>\n","      <td>0.149550</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.962700</td>\n","      <td>3.924130</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.961000</td>\n","      <td>3.921755</td>\n","      <td>0.154054</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.974100</td>\n","      <td>3.921457</td>\n","      <td>0.154955</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:14]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 18:45:29,463] Trial 6 finished with value: 3.9214565753936768 and parameters: {'num_layers': 23}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 32:29, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709000</td>\n","      <td>4.699757</td>\n","      <td>0.028829</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.703400</td>\n","      <td>4.686362</td>\n","      <td>0.054955</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.691600</td>\n","      <td>4.668060</td>\n","      <td>0.056757</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.680700</td>\n","      <td>4.650777</td>\n","      <td>0.073874</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.672000</td>\n","      <td>4.629227</td>\n","      <td>0.078378</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.648600</td>\n","      <td>4.602532</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.639000</td>\n","      <td>4.578953</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.616600</td>\n","      <td>4.549279</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.593600</td>\n","      <td>4.515528</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.567100</td>\n","      <td>4.485372</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.536600</td>\n","      <td>4.447817</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.519600</td>\n","      <td>4.415506</td>\n","      <td>0.141441</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.487100</td>\n","      <td>4.379423</td>\n","      <td>0.160360</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.462200</td>\n","      <td>4.344904</td>\n","      <td>0.140541</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.429800</td>\n","      <td>4.305915</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.384700</td>\n","      <td>4.271538</td>\n","      <td>0.163063</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.359800</td>\n","      <td>4.236232</td>\n","      <td>0.159459</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.338500</td>\n","      <td>4.204259</td>\n","      <td>0.171171</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.302000</td>\n","      <td>4.172125</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.268200</td>\n","      <td>4.135140</td>\n","      <td>0.181982</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.242000</td>\n","      <td>4.109256</td>\n","      <td>0.176577</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.204300</td>\n","      <td>4.076878</td>\n","      <td>0.189189</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.191800</td>\n","      <td>4.049375</td>\n","      <td>0.193694</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.154700</td>\n","      <td>4.026951</td>\n","      <td>0.205405</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.139300</td>\n","      <td>4.000737</td>\n","      <td>0.215315</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.108200</td>\n","      <td>3.974560</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.073000</td>\n","      <td>3.958592</td>\n","      <td>0.229730</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.068000</td>\n","      <td>3.938963</td>\n","      <td>0.229730</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.047100</td>\n","      <td>3.914848</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.026000</td>\n","      <td>3.894274</td>\n","      <td>0.242342</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.004700</td>\n","      <td>3.878076</td>\n","      <td>0.237838</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.988300</td>\n","      <td>3.857566</td>\n","      <td>0.250450</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.977400</td>\n","      <td>3.842633</td>\n","      <td>0.257658</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.958300</td>\n","      <td>3.828285</td>\n","      <td>0.257658</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.953600</td>\n","      <td>3.813251</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.917000</td>\n","      <td>3.795719</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.932900</td>\n","      <td>3.789813</td>\n","      <td>0.254955</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.915100</td>\n","      <td>3.775078</td>\n","      <td>0.264865</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.882500</td>\n","      <td>3.765811</td>\n","      <td>0.261261</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.893900</td>\n","      <td>3.752859</td>\n","      <td>0.273874</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.877600</td>\n","      <td>3.744928</td>\n","      <td>0.271171</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.884900</td>\n","      <td>3.734122</td>\n","      <td>0.274775</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.827200</td>\n","      <td>3.726365</td>\n","      <td>0.272973</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.865700</td>\n","      <td>3.721431</td>\n","      <td>0.281081</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.834200</td>\n","      <td>3.714890</td>\n","      <td>0.281081</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.862700</td>\n","      <td>3.711708</td>\n","      <td>0.285586</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.826100</td>\n","      <td>3.708059</td>\n","      <td>0.285586</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.825200</td>\n","      <td>3.705020</td>\n","      <td>0.285586</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.828100</td>\n","      <td>3.703939</td>\n","      <td>0.287387</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.853800</td>\n","      <td>3.703343</td>\n","      <td>0.286486</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 19:18:58,614] Trial 7 finished with value: 3.7033426761627197 and parameters: {'num_layers': 5}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 43:51, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708500</td>\n","      <td>4.701094</td>\n","      <td>0.024324</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.707300</td>\n","      <td>4.693042</td>\n","      <td>0.034234</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.700200</td>\n","      <td>4.679426</td>\n","      <td>0.045045</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.688800</td>\n","      <td>4.665529</td>\n","      <td>0.051351</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.679700</td>\n","      <td>4.642776</td>\n","      <td>0.054054</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.663800</td>\n","      <td>4.618524</td>\n","      <td>0.054054</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.648100</td>\n","      <td>4.587119</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.621800</td>\n","      <td>4.557612</td>\n","      <td>0.062162</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.600100</td>\n","      <td>4.522763</td>\n","      <td>0.048649</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.574200</td>\n","      <td>4.486005</td>\n","      <td>0.045045</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.529300</td>\n","      <td>4.445055</td>\n","      <td>0.042342</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.499100</td>\n","      <td>4.408706</td>\n","      <td>0.050450</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.481400</td>\n","      <td>4.372338</td>\n","      <td>0.045045</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.441300</td>\n","      <td>4.339260</td>\n","      <td>0.036036</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.417600</td>\n","      <td>4.304996</td>\n","      <td>0.042342</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.371600</td>\n","      <td>4.273015</td>\n","      <td>0.037838</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.344200</td>\n","      <td>4.246428</td>\n","      <td>0.046847</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.322200</td>\n","      <td>4.217195</td>\n","      <td>0.054054</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.273300</td>\n","      <td>4.190627</td>\n","      <td>0.059459</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.275500</td>\n","      <td>4.166819</td>\n","      <td>0.068468</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.230300</td>\n","      <td>4.151617</td>\n","      <td>0.067568</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.231500</td>\n","      <td>4.124301</td>\n","      <td>0.075676</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.181100</td>\n","      <td>4.093198</td>\n","      <td>0.085586</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.156600</td>\n","      <td>4.078513</td>\n","      <td>0.090991</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.164500</td>\n","      <td>4.061090</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.109200</td>\n","      <td>4.041152</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.109700</td>\n","      <td>4.027841</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.105700</td>\n","      <td>4.007665</td>\n","      <td>0.103604</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.089300</td>\n","      <td>3.988010</td>\n","      <td>0.106306</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.078500</td>\n","      <td>3.975202</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.059200</td>\n","      <td>3.963508</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.026700</td>\n","      <td>3.949201</td>\n","      <td>0.118919</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.016100</td>\n","      <td>3.932996</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.000900</td>\n","      <td>3.922091</td>\n","      <td>0.125225</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>4.007100</td>\n","      <td>3.909119</td>\n","      <td>0.131532</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.992800</td>\n","      <td>3.905665</td>\n","      <td>0.127027</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.979500</td>\n","      <td>3.890434</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.942300</td>\n","      <td>3.873478</td>\n","      <td>0.129730</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.945300</td>\n","      <td>3.871943</td>\n","      <td>0.136937</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.951700</td>\n","      <td>3.857203</td>\n","      <td>0.141441</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.933000</td>\n","      <td>3.854077</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.917700</td>\n","      <td>3.841653</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.915700</td>\n","      <td>3.840430</td>\n","      <td>0.144144</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.895200</td>\n","      <td>3.836864</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.925300</td>\n","      <td>3.829639</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.913500</td>\n","      <td>3.826437</td>\n","      <td>0.155856</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.897600</td>\n","      <td>3.823583</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.899000</td>\n","      <td>3.824484</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.899500</td>\n","      <td>3.823255</td>\n","      <td>0.155856</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.893300</td>\n","      <td>3.822214</td>\n","      <td>0.156757</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 20:03:46,785] Trial 8 finished with value: 3.822213649749756 and parameters: {'num_layers': 14}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 30:34, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708200</td>\n","      <td>4.698780</td>\n","      <td>0.017117</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.700900</td>\n","      <td>4.685322</td>\n","      <td>0.023423</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.693600</td>\n","      <td>4.673082</td>\n","      <td>0.033333</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.683800</td>\n","      <td>4.654863</td>\n","      <td>0.039640</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.674400</td>\n","      <td>4.636420</td>\n","      <td>0.058559</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.656100</td>\n","      <td>4.612448</td>\n","      <td>0.078378</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.638200</td>\n","      <td>4.585474</td>\n","      <td>0.084685</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.622200</td>\n","      <td>4.558756</td>\n","      <td>0.090991</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.605400</td>\n","      <td>4.533115</td>\n","      <td>0.095495</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.578900</td>\n","      <td>4.500781</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.549300</td>\n","      <td>4.468423</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.529400</td>\n","      <td>4.437880</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.499100</td>\n","      <td>4.406584</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.480800</td>\n","      <td>4.373149</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.452300</td>\n","      <td>4.340744</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.428500</td>\n","      <td>4.309666</td>\n","      <td>0.114414</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.391400</td>\n","      <td>4.278243</td>\n","      <td>0.120721</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.368400</td>\n","      <td>4.246907</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.340700</td>\n","      <td>4.216258</td>\n","      <td>0.116216</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.304600</td>\n","      <td>4.185709</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.285600</td>\n","      <td>4.157191</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.250600</td>\n","      <td>4.125033</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.230600</td>\n","      <td>4.101110</td>\n","      <td>0.142342</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.205900</td>\n","      <td>4.076652</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.165300</td>\n","      <td>4.052529</td>\n","      <td>0.146847</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.171700</td>\n","      <td>4.030895</td>\n","      <td>0.149550</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.131500</td>\n","      <td>4.009491</td>\n","      <td>0.154955</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.123300</td>\n","      <td>3.988549</td>\n","      <td>0.155856</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.102400</td>\n","      <td>3.971538</td>\n","      <td>0.155856</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.097900</td>\n","      <td>3.949867</td>\n","      <td>0.157658</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.050100</td>\n","      <td>3.930471</td>\n","      <td>0.158559</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.040500</td>\n","      <td>3.914379</td>\n","      <td>0.160360</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.023100</td>\n","      <td>3.899057</td>\n","      <td>0.163063</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.034200</td>\n","      <td>3.886283</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>4.004800</td>\n","      <td>3.868508</td>\n","      <td>0.164865</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.994200</td>\n","      <td>3.854517</td>\n","      <td>0.166667</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.949300</td>\n","      <td>3.841515</td>\n","      <td>0.175676</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.967500</td>\n","      <td>3.831542</td>\n","      <td>0.177477</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.954500</td>\n","      <td>3.821718</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.950300</td>\n","      <td>3.813076</td>\n","      <td>0.173874</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.952800</td>\n","      <td>3.803458</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.913300</td>\n","      <td>3.794735</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.929800</td>\n","      <td>3.790420</td>\n","      <td>0.177477</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.924100</td>\n","      <td>3.784816</td>\n","      <td>0.183784</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.898000</td>\n","      <td>3.778032</td>\n","      <td>0.184685</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.907400</td>\n","      <td>3.775834</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.896300</td>\n","      <td>3.772277</td>\n","      <td>0.183784</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.914100</td>\n","      <td>3.769900</td>\n","      <td>0.185586</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.868800</td>\n","      <td>3.768263</td>\n","      <td>0.187387</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.889200</td>\n","      <td>3.767844</td>\n","      <td>0.187387</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 20:35:17,473] Trial 9 finished with value: 3.767843723297119 and parameters: {'num_layers': 4}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 38:56, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.711400</td>\n","      <td>4.704685</td>\n","      <td>0.010811</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.706400</td>\n","      <td>4.696667</td>\n","      <td>0.018018</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.699600</td>\n","      <td>4.683206</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.696500</td>\n","      <td>4.673188</td>\n","      <td>0.028829</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.684000</td>\n","      <td>4.656142</td>\n","      <td>0.035135</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.672100</td>\n","      <td>4.636421</td>\n","      <td>0.043243</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.658500</td>\n","      <td>4.613287</td>\n","      <td>0.049550</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.646000</td>\n","      <td>4.590195</td>\n","      <td>0.054054</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.628500</td>\n","      <td>4.563351</td>\n","      <td>0.066667</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.611600</td>\n","      <td>4.534761</td>\n","      <td>0.072072</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.588900</td>\n","      <td>4.506412</td>\n","      <td>0.084685</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.553800</td>\n","      <td>4.465185</td>\n","      <td>0.081081</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.532200</td>\n","      <td>4.431260</td>\n","      <td>0.082883</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.507200</td>\n","      <td>4.398415</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.462600</td>\n","      <td>4.360611</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.433300</td>\n","      <td>4.319417</td>\n","      <td>0.103604</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.409300</td>\n","      <td>4.282621</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.378500</td>\n","      <td>4.253992</td>\n","      <td>0.105405</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.328700</td>\n","      <td>4.217055</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.289700</td>\n","      <td>4.178700</td>\n","      <td>0.108108</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.259700</td>\n","      <td>4.150966</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.243900</td>\n","      <td>4.129045</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.205100</td>\n","      <td>4.100427</td>\n","      <td>0.108108</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.177000</td>\n","      <td>4.068603</td>\n","      <td>0.115315</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.149400</td>\n","      <td>4.045800</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.126500</td>\n","      <td>4.018504</td>\n","      <td>0.120721</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.095500</td>\n","      <td>4.001331</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.082800</td>\n","      <td>3.978869</td>\n","      <td>0.124324</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.051900</td>\n","      <td>3.964559</td>\n","      <td>0.118919</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.039700</td>\n","      <td>3.947324</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.021700</td>\n","      <td>3.935027</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.993000</td>\n","      <td>3.910018</td>\n","      <td>0.135135</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.000300</td>\n","      <td>3.902788</td>\n","      <td>0.135135</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.983700</td>\n","      <td>3.891035</td>\n","      <td>0.136937</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.957500</td>\n","      <td>3.879457</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.923500</td>\n","      <td>3.860929</td>\n","      <td>0.141441</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.949900</td>\n","      <td>3.855437</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.930500</td>\n","      <td>3.842500</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.914800</td>\n","      <td>3.831290</td>\n","      <td>0.148649</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.921700</td>\n","      <td>3.818972</td>\n","      <td>0.160360</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.887700</td>\n","      <td>3.817312</td>\n","      <td>0.163063</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.898400</td>\n","      <td>3.811976</td>\n","      <td>0.158559</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.901400</td>\n","      <td>3.802925</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.872400</td>\n","      <td>3.800495</td>\n","      <td>0.164865</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.878600</td>\n","      <td>3.798083</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.890800</td>\n","      <td>3.795562</td>\n","      <td>0.172973</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.857700</td>\n","      <td>3.795657</td>\n","      <td>0.173874</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.864300</td>\n","      <td>3.789020</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.847200</td>\n","      <td>3.785946</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.841800</td>\n","      <td>3.786099</td>\n","      <td>0.179279</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:12]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 21:15:13,698] Trial 10 finished with value: 3.7859463691711426 and parameters: {'num_layers': 10}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 37:58, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709400</td>\n","      <td>4.701189</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.703200</td>\n","      <td>4.689653</td>\n","      <td>0.030631</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.698100</td>\n","      <td>4.673441</td>\n","      <td>0.041441</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.685300</td>\n","      <td>4.659721</td>\n","      <td>0.048649</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.677700</td>\n","      <td>4.641739</td>\n","      <td>0.054955</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.660700</td>\n","      <td>4.621482</td>\n","      <td>0.056757</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.645300</td>\n","      <td>4.593356</td>\n","      <td>0.053153</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.630000</td>\n","      <td>4.562118</td>\n","      <td>0.063964</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.599100</td>\n","      <td>4.526942</td>\n","      <td>0.055856</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.575900</td>\n","      <td>4.487835</td>\n","      <td>0.067568</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.546600</td>\n","      <td>4.450718</td>\n","      <td>0.080180</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.514200</td>\n","      <td>4.414592</td>\n","      <td>0.077477</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.477400</td>\n","      <td>4.381722</td>\n","      <td>0.061261</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.451700</td>\n","      <td>4.341491</td>\n","      <td>0.069369</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.414700</td>\n","      <td>4.303432</td>\n","      <td>0.074775</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.373200</td>\n","      <td>4.266179</td>\n","      <td>0.068468</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.347700</td>\n","      <td>4.229481</td>\n","      <td>0.067568</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.317600</td>\n","      <td>4.195956</td>\n","      <td>0.062162</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.292700</td>\n","      <td>4.164501</td>\n","      <td>0.069369</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.265500</td>\n","      <td>4.138517</td>\n","      <td>0.074775</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.211700</td>\n","      <td>4.109202</td>\n","      <td>0.084685</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.199800</td>\n","      <td>4.075630</td>\n","      <td>0.104505</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.155100</td>\n","      <td>4.058128</td>\n","      <td>0.087387</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.141200</td>\n","      <td>4.028567</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.099500</td>\n","      <td>4.007237</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.087100</td>\n","      <td>3.983658</td>\n","      <td>0.103604</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.056000</td>\n","      <td>3.961248</td>\n","      <td>0.115315</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.074300</td>\n","      <td>3.954080</td>\n","      <td>0.112613</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.023000</td>\n","      <td>3.928922</td>\n","      <td>0.135135</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.018600</td>\n","      <td>3.912996</td>\n","      <td>0.130631</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.978700</td>\n","      <td>3.899417</td>\n","      <td>0.137838</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.972300</td>\n","      <td>3.883958</td>\n","      <td>0.136937</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.957300</td>\n","      <td>3.871819</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.944100</td>\n","      <td>3.855764</td>\n","      <td>0.149550</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.934000</td>\n","      <td>3.841923</td>\n","      <td>0.154955</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.912100</td>\n","      <td>3.835004</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.910500</td>\n","      <td>3.822140</td>\n","      <td>0.160360</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.876800</td>\n","      <td>3.817548</td>\n","      <td>0.154054</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.887700</td>\n","      <td>3.804981</td>\n","      <td>0.154955</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.880500</td>\n","      <td>3.804445</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.878100</td>\n","      <td>3.798802</td>\n","      <td>0.159459</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.869300</td>\n","      <td>3.788098</td>\n","      <td>0.158559</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.874700</td>\n","      <td>3.777729</td>\n","      <td>0.162162</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.846900</td>\n","      <td>3.773029</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.839800</td>\n","      <td>3.772639</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.854200</td>\n","      <td>3.767493</td>\n","      <td>0.167568</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.832000</td>\n","      <td>3.764653</td>\n","      <td>0.167568</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.845800</td>\n","      <td>3.765082</td>\n","      <td>0.166667</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.825600</td>\n","      <td>3.765153</td>\n","      <td>0.166667</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.837100</td>\n","      <td>3.763418</td>\n","      <td>0.169369</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 21:54:08,970] Trial 11 finished with value: 3.763418436050415 and parameters: {'num_layers': 9}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 27:35, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708200</td>\n","      <td>4.699550</td>\n","      <td>0.009009</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.703400</td>\n","      <td>4.689146</td>\n","      <td>0.023423</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.694300</td>\n","      <td>4.677635</td>\n","      <td>0.054054</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.685400</td>\n","      <td>4.662054</td>\n","      <td>0.063964</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.677500</td>\n","      <td>4.642311</td>\n","      <td>0.058559</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.654000</td>\n","      <td>4.617785</td>\n","      <td>0.085586</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.637300</td>\n","      <td>4.588194</td>\n","      <td>0.099099</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.616500</td>\n","      <td>4.554558</td>\n","      <td>0.103604</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.585300</td>\n","      <td>4.518782</td>\n","      <td>0.130631</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.561800</td>\n","      <td>4.486297</td>\n","      <td>0.145946</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.535300</td>\n","      <td>4.455346</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.504800</td>\n","      <td>4.420938</td>\n","      <td>0.188288</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.474800</td>\n","      <td>4.390060</td>\n","      <td>0.172973</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.458400</td>\n","      <td>4.359210</td>\n","      <td>0.185586</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.428700</td>\n","      <td>4.328526</td>\n","      <td>0.196396</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.400200</td>\n","      <td>4.301608</td>\n","      <td>0.200901</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.373800</td>\n","      <td>4.269468</td>\n","      <td>0.209910</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.367700</td>\n","      <td>4.237639</td>\n","      <td>0.228829</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.327100</td>\n","      <td>4.207494</td>\n","      <td>0.227928</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.297200</td>\n","      <td>4.177919</td>\n","      <td>0.242342</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.278200</td>\n","      <td>4.151407</td>\n","      <td>0.244144</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.253100</td>\n","      <td>4.123743</td>\n","      <td>0.251351</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.238900</td>\n","      <td>4.094543</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.195600</td>\n","      <td>4.069237</td>\n","      <td>0.257658</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.197300</td>\n","      <td>4.044594</td>\n","      <td>0.279279</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.168200</td>\n","      <td>4.018356</td>\n","      <td>0.278378</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.139500</td>\n","      <td>3.996649</td>\n","      <td>0.280180</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.122200</td>\n","      <td>3.976016</td>\n","      <td>0.286486</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.121500</td>\n","      <td>3.956808</td>\n","      <td>0.284685</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.068700</td>\n","      <td>3.934660</td>\n","      <td>0.300000</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.068700</td>\n","      <td>3.915614</td>\n","      <td>0.309009</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.048200</td>\n","      <td>3.894413</td>\n","      <td>0.319820</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.036300</td>\n","      <td>3.877560</td>\n","      <td>0.320721</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.038600</td>\n","      <td>3.862908</td>\n","      <td>0.324324</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>4.030300</td>\n","      <td>3.848288</td>\n","      <td>0.330631</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>4.004800</td>\n","      <td>3.834628</td>\n","      <td>0.327928</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.960700</td>\n","      <td>3.821623</td>\n","      <td>0.341441</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.986100</td>\n","      <td>3.808883</td>\n","      <td>0.338739</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.987600</td>\n","      <td>3.797002</td>\n","      <td>0.346847</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.953700</td>\n","      <td>3.786530</td>\n","      <td>0.344144</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.956500</td>\n","      <td>3.777345</td>\n","      <td>0.347748</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.923500</td>\n","      <td>3.769450</td>\n","      <td>0.351351</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.970800</td>\n","      <td>3.762778</td>\n","      <td>0.359459</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.925100</td>\n","      <td>3.755991</td>\n","      <td>0.361261</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.927500</td>\n","      <td>3.751146</td>\n","      <td>0.360360</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.893000</td>\n","      <td>3.746142</td>\n","      <td>0.357658</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.902600</td>\n","      <td>3.741900</td>\n","      <td>0.361261</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.914000</td>\n","      <td>3.739410</td>\n","      <td>0.363964</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.916900</td>\n","      <td>3.737873</td>\n","      <td>0.363063</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.917700</td>\n","      <td>3.737313</td>\n","      <td>0.362162</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 22:22:43,499] Trial 12 finished with value: 3.7373125553131104 and parameters: {'num_layers': 1}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 36:50, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.711300</td>\n","      <td>4.702909</td>\n","      <td>0.019820</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.702800</td>\n","      <td>4.691449</td>\n","      <td>0.018018</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.696700</td>\n","      <td>4.680123</td>\n","      <td>0.032432</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.686500</td>\n","      <td>4.662714</td>\n","      <td>0.047748</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.675600</td>\n","      <td>4.645768</td>\n","      <td>0.057658</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.660300</td>\n","      <td>4.625079</td>\n","      <td>0.060360</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.646100</td>\n","      <td>4.597983</td>\n","      <td>0.069369</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.628300</td>\n","      <td>4.573708</td>\n","      <td>0.081081</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.606100</td>\n","      <td>4.537744</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.583100</td>\n","      <td>4.512765</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.555200</td>\n","      <td>4.478727</td>\n","      <td>0.096396</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.536400</td>\n","      <td>4.444711</td>\n","      <td>0.090090</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.494700</td>\n","      <td>4.407345</td>\n","      <td>0.104505</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.476200</td>\n","      <td>4.369572</td>\n","      <td>0.104505</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.425600</td>\n","      <td>4.327694</td>\n","      <td>0.105405</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.406200</td>\n","      <td>4.296010</td>\n","      <td>0.101802</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.361300</td>\n","      <td>4.257798</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.335700</td>\n","      <td>4.218211</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.289500</td>\n","      <td>4.181383</td>\n","      <td>0.115315</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.262500</td>\n","      <td>4.157562</td>\n","      <td>0.116216</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.227000</td>\n","      <td>4.123060</td>\n","      <td>0.122523</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.199600</td>\n","      <td>4.087305</td>\n","      <td>0.136937</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.169900</td>\n","      <td>4.063089</td>\n","      <td>0.124324</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.155800</td>\n","      <td>4.035414</td>\n","      <td>0.133333</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.107200</td>\n","      <td>4.006886</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.096700</td>\n","      <td>3.984675</td>\n","      <td>0.136036</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.058000</td>\n","      <td>3.953002</td>\n","      <td>0.146847</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.042600</td>\n","      <td>3.931226</td>\n","      <td>0.163964</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.020900</td>\n","      <td>3.913788</td>\n","      <td>0.163964</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.001800</td>\n","      <td>3.901229</td>\n","      <td>0.167568</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.960300</td>\n","      <td>3.878235</td>\n","      <td>0.171171</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.964700</td>\n","      <td>3.866065</td>\n","      <td>0.173874</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.927700</td>\n","      <td>3.843729</td>\n","      <td>0.181982</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.932700</td>\n","      <td>3.827183</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.913200</td>\n","      <td>3.819343</td>\n","      <td>0.184685</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.889100</td>\n","      <td>3.802418</td>\n","      <td>0.174775</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.881900</td>\n","      <td>3.788357</td>\n","      <td>0.186486</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.866400</td>\n","      <td>3.785151</td>\n","      <td>0.178378</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.860900</td>\n","      <td>3.771283</td>\n","      <td>0.189189</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.861300</td>\n","      <td>3.759726</td>\n","      <td>0.201802</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.848600</td>\n","      <td>3.762014</td>\n","      <td>0.190991</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.820300</td>\n","      <td>3.747569</td>\n","      <td>0.201802</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.859200</td>\n","      <td>3.745272</td>\n","      <td>0.201802</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.826900</td>\n","      <td>3.739848</td>\n","      <td>0.200901</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.819700</td>\n","      <td>3.735447</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.801500</td>\n","      <td>3.730178</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.808700</td>\n","      <td>3.727229</td>\n","      <td>0.204505</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.812000</td>\n","      <td>3.726861</td>\n","      <td>0.209009</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.802900</td>\n","      <td>3.726120</td>\n","      <td>0.204505</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.816900</td>\n","      <td>3.725167</td>\n","      <td>0.206306</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 23:00:31,014] Trial 13 finished with value: 3.7251672744750977 and parameters: {'num_layers': 8}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 28:11, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.706100</td>\n","      <td>4.703903</td>\n","      <td>0.009009</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.706100</td>\n","      <td>4.694532</td>\n","      <td>0.025225</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.700200</td>\n","      <td>4.684687</td>\n","      <td>0.025225</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.694500</td>\n","      <td>4.672815</td>\n","      <td>0.040541</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.683300</td>\n","      <td>4.657837</td>\n","      <td>0.059459</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.667500</td>\n","      <td>4.637659</td>\n","      <td>0.069369</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.653000</td>\n","      <td>4.613770</td>\n","      <td>0.081081</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.636400</td>\n","      <td>4.587174</td>\n","      <td>0.086486</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.613700</td>\n","      <td>4.554393</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.587800</td>\n","      <td>4.518972</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.562500</td>\n","      <td>4.488193</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.530500</td>\n","      <td>4.452956</td>\n","      <td>0.177477</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.499900</td>\n","      <td>4.413753</td>\n","      <td>0.172973</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.477100</td>\n","      <td>4.382334</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.447800</td>\n","      <td>4.349380</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.411100</td>\n","      <td>4.316898</td>\n","      <td>0.196396</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.384300</td>\n","      <td>4.282884</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.380400</td>\n","      <td>4.249700</td>\n","      <td>0.207207</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.337500</td>\n","      <td>4.218507</td>\n","      <td>0.218919</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.306100</td>\n","      <td>4.185792</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.283700</td>\n","      <td>4.155612</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.249000</td>\n","      <td>4.125178</td>\n","      <td>0.214414</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.251400</td>\n","      <td>4.097623</td>\n","      <td>0.234234</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.197400</td>\n","      <td>4.069285</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.196400</td>\n","      <td>4.044600</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.167500</td>\n","      <td>4.020468</td>\n","      <td>0.236937</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.140600</td>\n","      <td>3.995631</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.117300</td>\n","      <td>3.973833</td>\n","      <td>0.246847</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.119800</td>\n","      <td>3.951961</td>\n","      <td>0.247748</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.061300</td>\n","      <td>3.929255</td>\n","      <td>0.268468</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.056700</td>\n","      <td>3.910424</td>\n","      <td>0.267568</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.038900</td>\n","      <td>3.891144</td>\n","      <td>0.270270</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.027500</td>\n","      <td>3.875459</td>\n","      <td>0.270270</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.039100</td>\n","      <td>3.859749</td>\n","      <td>0.275676</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>4.023800</td>\n","      <td>3.844757</td>\n","      <td>0.276577</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>4.004400</td>\n","      <td>3.829292</td>\n","      <td>0.275676</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.952300</td>\n","      <td>3.815722</td>\n","      <td>0.282883</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.977700</td>\n","      <td>3.801773</td>\n","      <td>0.287387</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.975700</td>\n","      <td>3.790547</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.949700</td>\n","      <td>3.780377</td>\n","      <td>0.293694</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.959400</td>\n","      <td>3.771248</td>\n","      <td>0.296396</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.917500</td>\n","      <td>3.762505</td>\n","      <td>0.305405</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.963400</td>\n","      <td>3.755863</td>\n","      <td>0.303604</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.909300</td>\n","      <td>3.749403</td>\n","      <td>0.300901</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.916100</td>\n","      <td>3.744261</td>\n","      <td>0.301802</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.887600</td>\n","      <td>3.739595</td>\n","      <td>0.302703</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.892100</td>\n","      <td>3.735936</td>\n","      <td>0.303604</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.906700</td>\n","      <td>3.733393</td>\n","      <td>0.303604</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.909800</td>\n","      <td>3.732018</td>\n","      <td>0.303604</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.905300</td>\n","      <td>3.731528</td>\n","      <td>0.304505</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-18 23:29:36,913] Trial 14 finished with value: 3.73152756690979 and parameters: {'num_layers': 1}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 42:55, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709900</td>\n","      <td>4.704793</td>\n","      <td>0.020721</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.708400</td>\n","      <td>4.697919</td>\n","      <td>0.020721</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.703300</td>\n","      <td>4.688953</td>\n","      <td>0.033333</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.692200</td>\n","      <td>4.674824</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.687900</td>\n","      <td>4.661367</td>\n","      <td>0.032432</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.673300</td>\n","      <td>4.643546</td>\n","      <td>0.053153</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.663300</td>\n","      <td>4.622330</td>\n","      <td>0.065766</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.648600</td>\n","      <td>4.597552</td>\n","      <td>0.070270</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.636800</td>\n","      <td>4.571906</td>\n","      <td>0.093694</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.614500</td>\n","      <td>4.540740</td>\n","      <td>0.081982</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.593100</td>\n","      <td>4.512674</td>\n","      <td>0.101802</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.557300</td>\n","      <td>4.472299</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.534200</td>\n","      <td>4.439543</td>\n","      <td>0.096396</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.513200</td>\n","      <td>4.399415</td>\n","      <td>0.106306</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.479700</td>\n","      <td>4.359084</td>\n","      <td>0.118919</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.432100</td>\n","      <td>4.321082</td>\n","      <td>0.112613</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.401900</td>\n","      <td>4.283432</td>\n","      <td>0.124324</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.361400</td>\n","      <td>4.243414</td>\n","      <td>0.131532</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.346800</td>\n","      <td>4.208804</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.301800</td>\n","      <td>4.172599</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.262500</td>\n","      <td>4.135621</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.219100</td>\n","      <td>4.106503</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.213100</td>\n","      <td>4.077826</td>\n","      <td>0.146847</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.191400</td>\n","      <td>4.045872</td>\n","      <td>0.148649</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.140700</td>\n","      <td>4.022203</td>\n","      <td>0.148649</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.114300</td>\n","      <td>3.997066</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.102900</td>\n","      <td>3.975573</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.054900</td>\n","      <td>3.951432</td>\n","      <td>0.151351</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.043000</td>\n","      <td>3.930485</td>\n","      <td>0.157658</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.030000</td>\n","      <td>3.920993</td>\n","      <td>0.159459</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.998500</td>\n","      <td>3.905012</td>\n","      <td>0.156757</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.976100</td>\n","      <td>3.884415</td>\n","      <td>0.169369</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.974200</td>\n","      <td>3.863930</td>\n","      <td>0.171171</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.945200</td>\n","      <td>3.854432</td>\n","      <td>0.163964</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.949200</td>\n","      <td>3.843564</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.929100</td>\n","      <td>3.826243</td>\n","      <td>0.182883</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.905300</td>\n","      <td>3.815974</td>\n","      <td>0.184685</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.896900</td>\n","      <td>3.806139</td>\n","      <td>0.171171</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.902700</td>\n","      <td>3.797343</td>\n","      <td>0.183784</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.883700</td>\n","      <td>3.789473</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.859900</td>\n","      <td>3.780736</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.854900</td>\n","      <td>3.773842</td>\n","      <td>0.184685</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.854900</td>\n","      <td>3.767096</td>\n","      <td>0.191892</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.840900</td>\n","      <td>3.760921</td>\n","      <td>0.191892</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.835200</td>\n","      <td>3.754269</td>\n","      <td>0.199099</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.828200</td>\n","      <td>3.751574</td>\n","      <td>0.197297</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.826100</td>\n","      <td>3.750077</td>\n","      <td>0.193694</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.836500</td>\n","      <td>3.747722</td>\n","      <td>0.195495</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.837100</td>\n","      <td>3.747310</td>\n","      <td>0.193694</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.835600</td>\n","      <td>3.746245</td>\n","      <td>0.195495</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:12]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 00:13:29,518] Trial 15 finished with value: 3.7462451457977295 and parameters: {'num_layers': 13}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 33:47, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.707400</td>\n","      <td>4.699555</td>\n","      <td>0.019820</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.702300</td>\n","      <td>4.687501</td>\n","      <td>0.036937</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.693800</td>\n","      <td>4.672675</td>\n","      <td>0.034234</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.684000</td>\n","      <td>4.654934</td>\n","      <td>0.052252</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.675200</td>\n","      <td>4.633548</td>\n","      <td>0.086486</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.656800</td>\n","      <td>4.612518</td>\n","      <td>0.075676</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.638200</td>\n","      <td>4.582642</td>\n","      <td>0.091892</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.615900</td>\n","      <td>4.549713</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.596000</td>\n","      <td>4.518878</td>\n","      <td>0.094595</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.568500</td>\n","      <td>4.479667</td>\n","      <td>0.117117</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.535400</td>\n","      <td>4.445903</td>\n","      <td>0.131532</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.518500</td>\n","      <td>4.415766</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.486800</td>\n","      <td>4.379254</td>\n","      <td>0.130631</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.459900</td>\n","      <td>4.345335</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.429800</td>\n","      <td>4.309250</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.402700</td>\n","      <td>4.273847</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.366600</td>\n","      <td>4.241851</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.337900</td>\n","      <td>4.211224</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.323300</td>\n","      <td>4.179004</td>\n","      <td>0.151351</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.275600</td>\n","      <td>4.149167</td>\n","      <td>0.158559</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.259700</td>\n","      <td>4.124237</td>\n","      <td>0.147748</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.239200</td>\n","      <td>4.097694</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.197100</td>\n","      <td>4.073090</td>\n","      <td>0.154054</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.175200</td>\n","      <td>4.045795</td>\n","      <td>0.167568</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.156700</td>\n","      <td>4.023756</td>\n","      <td>0.158559</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.127000</td>\n","      <td>3.998389</td>\n","      <td>0.167568</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.116300</td>\n","      <td>3.977654</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.087500</td>\n","      <td>3.956051</td>\n","      <td>0.168468</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.060500</td>\n","      <td>3.939850</td>\n","      <td>0.176577</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.046200</td>\n","      <td>3.919793</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.026400</td>\n","      <td>3.904283</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.018700</td>\n","      <td>3.891810</td>\n","      <td>0.186486</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.988500</td>\n","      <td>3.871947</td>\n","      <td>0.184685</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.962900</td>\n","      <td>3.856869</td>\n","      <td>0.189189</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.968900</td>\n","      <td>3.843201</td>\n","      <td>0.190090</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.942500</td>\n","      <td>3.827956</td>\n","      <td>0.198198</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.940900</td>\n","      <td>3.818000</td>\n","      <td>0.195495</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.911600</td>\n","      <td>3.809484</td>\n","      <td>0.195495</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.942300</td>\n","      <td>3.796911</td>\n","      <td>0.200901</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.877800</td>\n","      <td>3.786632</td>\n","      <td>0.204505</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.919200</td>\n","      <td>3.782578</td>\n","      <td>0.201802</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.881300</td>\n","      <td>3.775565</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.871900</td>\n","      <td>3.766673</td>\n","      <td>0.207207</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.890300</td>\n","      <td>3.764417</td>\n","      <td>0.205405</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.850800</td>\n","      <td>3.756884</td>\n","      <td>0.213514</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.882400</td>\n","      <td>3.754113</td>\n","      <td>0.215315</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.857600</td>\n","      <td>3.751042</td>\n","      <td>0.207207</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.845700</td>\n","      <td>3.749022</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.855800</td>\n","      <td>3.747017</td>\n","      <td>0.213514</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.860800</td>\n","      <td>3.746233</td>\n","      <td>0.214414</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 00:48:21,171] Trial 16 finished with value: 3.746232748031616 and parameters: {'num_layers': 6}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 40:19, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709500</td>\n","      <td>4.705660</td>\n","      <td>0.010811</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.708400</td>\n","      <td>4.696444</td>\n","      <td>0.021622</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.703400</td>\n","      <td>4.689759</td>\n","      <td>0.021622</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.693600</td>\n","      <td>4.676877</td>\n","      <td>0.042342</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.685200</td>\n","      <td>4.663540</td>\n","      <td>0.043243</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.672300</td>\n","      <td>4.643582</td>\n","      <td>0.048649</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.666700</td>\n","      <td>4.625642</td>\n","      <td>0.060360</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.647100</td>\n","      <td>4.602217</td>\n","      <td>0.068468</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.623700</td>\n","      <td>4.570015</td>\n","      <td>0.073874</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.604000</td>\n","      <td>4.531774</td>\n","      <td>0.072973</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.574300</td>\n","      <td>4.490301</td>\n","      <td>0.080180</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.543800</td>\n","      <td>4.451092</td>\n","      <td>0.076577</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.509200</td>\n","      <td>4.412146</td>\n","      <td>0.088288</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.473400</td>\n","      <td>4.369435</td>\n","      <td>0.083784</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.435500</td>\n","      <td>4.326591</td>\n","      <td>0.091892</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.395500</td>\n","      <td>4.290410</td>\n","      <td>0.091892</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.357500</td>\n","      <td>4.257566</td>\n","      <td>0.087387</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.322500</td>\n","      <td>4.219739</td>\n","      <td>0.081982</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.297000</td>\n","      <td>4.190453</td>\n","      <td>0.089189</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.275900</td>\n","      <td>4.142042</td>\n","      <td>0.089189</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.216500</td>\n","      <td>4.127437</td>\n","      <td>0.076577</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.195600</td>\n","      <td>4.094165</td>\n","      <td>0.081982</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.162200</td>\n","      <td>4.078122</td>\n","      <td>0.078378</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.152600</td>\n","      <td>4.050106</td>\n","      <td>0.081081</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.131000</td>\n","      <td>4.027257</td>\n","      <td>0.089189</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.115700</td>\n","      <td>3.997495</td>\n","      <td>0.092793</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.054900</td>\n","      <td>3.979020</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.058300</td>\n","      <td>3.968024</td>\n","      <td>0.088288</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.048900</td>\n","      <td>3.948290</td>\n","      <td>0.095495</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.019800</td>\n","      <td>3.934744</td>\n","      <td>0.101802</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.989800</td>\n","      <td>3.921775</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.977000</td>\n","      <td>3.904075</td>\n","      <td>0.103604</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.968300</td>\n","      <td>3.887713</td>\n","      <td>0.108108</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.979800</td>\n","      <td>3.877680</td>\n","      <td>0.105405</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.948800</td>\n","      <td>3.856562</td>\n","      <td>0.121622</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.940100</td>\n","      <td>3.851177</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.919500</td>\n","      <td>3.837167</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.908100</td>\n","      <td>3.835908</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.899400</td>\n","      <td>3.830835</td>\n","      <td>0.125225</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.900000</td>\n","      <td>3.819105</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.885800</td>\n","      <td>3.805226</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.873400</td>\n","      <td>3.807214</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.869500</td>\n","      <td>3.799913</td>\n","      <td>0.130631</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.886900</td>\n","      <td>3.791343</td>\n","      <td>0.135135</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.863100</td>\n","      <td>3.788652</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.835800</td>\n","      <td>3.786612</td>\n","      <td>0.136036</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.839600</td>\n","      <td>3.783809</td>\n","      <td>0.136937</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.847800</td>\n","      <td>3.783262</td>\n","      <td>0.133333</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.859300</td>\n","      <td>3.779862</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.852800</td>\n","      <td>3.778929</td>\n","      <td>0.133333</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:12]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 01:29:45,792] Trial 17 finished with value: 3.7789292335510254 and parameters: {'num_layers': 11}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 35:03, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709500</td>\n","      <td>4.702230</td>\n","      <td>0.016216</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.705000</td>\n","      <td>4.689596</td>\n","      <td>0.023423</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.696900</td>\n","      <td>4.670762</td>\n","      <td>0.038739</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.678200</td>\n","      <td>4.649355</td>\n","      <td>0.056757</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.669700</td>\n","      <td>4.629714</td>\n","      <td>0.086486</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.649200</td>\n","      <td>4.605486</td>\n","      <td>0.109009</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.631500</td>\n","      <td>4.578020</td>\n","      <td>0.113514</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.618100</td>\n","      <td>4.547173</td>\n","      <td>0.097297</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.587800</td>\n","      <td>4.512570</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.561600</td>\n","      <td>4.474004</td>\n","      <td>0.102703</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.535000</td>\n","      <td>4.440886</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.499600</td>\n","      <td>4.401935</td>\n","      <td>0.104505</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.460500</td>\n","      <td>4.366634</td>\n","      <td>0.102703</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.434600</td>\n","      <td>4.325938</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.399000</td>\n","      <td>4.285952</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.374100</td>\n","      <td>4.260342</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.315200</td>\n","      <td>4.218509</td>\n","      <td>0.109009</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.299100</td>\n","      <td>4.180635</td>\n","      <td>0.121622</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.280700</td>\n","      <td>4.152354</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.243700</td>\n","      <td>4.123566</td>\n","      <td>0.116216</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.205100</td>\n","      <td>4.092140</td>\n","      <td>0.121622</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.186200</td>\n","      <td>4.064884</td>\n","      <td>0.117117</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.155700</td>\n","      <td>4.043431</td>\n","      <td>0.121622</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.118200</td>\n","      <td>4.017946</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.089400</td>\n","      <td>3.998925</td>\n","      <td>0.127928</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.077100</td>\n","      <td>3.970514</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.046000</td>\n","      <td>3.957367</td>\n","      <td>0.145045</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.039000</td>\n","      <td>3.927117</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.018900</td>\n","      <td>3.912292</td>\n","      <td>0.154054</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>3.989300</td>\n","      <td>3.891540</td>\n","      <td>0.163063</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.969100</td>\n","      <td>3.879674</td>\n","      <td>0.161261</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.960400</td>\n","      <td>3.859835</td>\n","      <td>0.170270</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.954700</td>\n","      <td>3.844266</td>\n","      <td>0.181982</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.933200</td>\n","      <td>3.829581</td>\n","      <td>0.186486</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.936600</td>\n","      <td>3.815372</td>\n","      <td>0.185586</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.904500</td>\n","      <td>3.815276</td>\n","      <td>0.172973</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.884300</td>\n","      <td>3.796365</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.889300</td>\n","      <td>3.792312</td>\n","      <td>0.189189</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.872800</td>\n","      <td>3.778404</td>\n","      <td>0.194595</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.874400</td>\n","      <td>3.773580</td>\n","      <td>0.198198</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.863200</td>\n","      <td>3.758438</td>\n","      <td>0.201802</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.850500</td>\n","      <td>3.759457</td>\n","      <td>0.204505</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.828400</td>\n","      <td>3.745189</td>\n","      <td>0.209009</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.844100</td>\n","      <td>3.743273</td>\n","      <td>0.208108</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.821700</td>\n","      <td>3.738936</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.852000</td>\n","      <td>3.737384</td>\n","      <td>0.218919</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.846000</td>\n","      <td>3.732962</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.818800</td>\n","      <td>3.731664</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.815400</td>\n","      <td>3.731320</td>\n","      <td>0.210811</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.800600</td>\n","      <td>3.729662</td>\n","      <td>0.209910</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:11]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 02:05:43,769] Trial 18 finished with value: 3.7296619415283203 and parameters: {'num_layers': 7}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 45:25, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.708800</td>\n","      <td>4.706070</td>\n","      <td>0.012613</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.707600</td>\n","      <td>4.699078</td>\n","      <td>0.020721</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.700500</td>\n","      <td>4.688899</td>\n","      <td>0.031532</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.696200</td>\n","      <td>4.678683</td>\n","      <td>0.029730</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.693100</td>\n","      <td>4.663395</td>\n","      <td>0.038739</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.678300</td>\n","      <td>4.646754</td>\n","      <td>0.050450</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.669900</td>\n","      <td>4.625658</td>\n","      <td>0.048649</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.654900</td>\n","      <td>4.600386</td>\n","      <td>0.059459</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.633500</td>\n","      <td>4.576926</td>\n","      <td>0.063964</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.616000</td>\n","      <td>4.548250</td>\n","      <td>0.052252</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.592100</td>\n","      <td>4.520502</td>\n","      <td>0.056757</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.568500</td>\n","      <td>4.484933</td>\n","      <td>0.049550</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.536800</td>\n","      <td>4.447397</td>\n","      <td>0.061261</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.515900</td>\n","      <td>4.405289</td>\n","      <td>0.064865</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.476900</td>\n","      <td>4.372677</td>\n","      <td>0.063063</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.447500</td>\n","      <td>4.329396</td>\n","      <td>0.080180</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.407500</td>\n","      <td>4.296225</td>\n","      <td>0.088288</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.379500</td>\n","      <td>4.262464</td>\n","      <td>0.079279</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.334500</td>\n","      <td>4.234556</td>\n","      <td>0.078378</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.304200</td>\n","      <td>4.191783</td>\n","      <td>0.071171</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.287800</td>\n","      <td>4.167113</td>\n","      <td>0.081982</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.239700</td>\n","      <td>4.141320</td>\n","      <td>0.081081</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.222900</td>\n","      <td>4.121378</td>\n","      <td>0.076577</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.191100</td>\n","      <td>4.085795</td>\n","      <td>0.082883</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.151700</td>\n","      <td>4.075444</td>\n","      <td>0.072072</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.130900</td>\n","      <td>4.048745</td>\n","      <td>0.074775</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.121900</td>\n","      <td>4.024743</td>\n","      <td>0.080180</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.105500</td>\n","      <td>4.004226</td>\n","      <td>0.083784</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.095100</td>\n","      <td>3.986733</td>\n","      <td>0.084685</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.065400</td>\n","      <td>3.978619</td>\n","      <td>0.083784</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.036900</td>\n","      <td>3.948912</td>\n","      <td>0.090991</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.020000</td>\n","      <td>3.943645</td>\n","      <td>0.089189</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.031200</td>\n","      <td>3.928352</td>\n","      <td>0.090090</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>4.003400</td>\n","      <td>3.912281</td>\n","      <td>0.108108</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.992200</td>\n","      <td>3.902322</td>\n","      <td>0.107207</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.977000</td>\n","      <td>3.894078</td>\n","      <td>0.110811</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.953500</td>\n","      <td>3.883801</td>\n","      <td>0.122523</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.956800</td>\n","      <td>3.877382</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.947200</td>\n","      <td>3.863712</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.926900</td>\n","      <td>3.856793</td>\n","      <td>0.116216</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.916300</td>\n","      <td>3.847990</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.937100</td>\n","      <td>3.848338</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.924000</td>\n","      <td>3.838082</td>\n","      <td>0.111712</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.905500</td>\n","      <td>3.838106</td>\n","      <td>0.118919</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.901600</td>\n","      <td>3.829427</td>\n","      <td>0.125225</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.906000</td>\n","      <td>3.829292</td>\n","      <td>0.121622</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.897900</td>\n","      <td>3.825810</td>\n","      <td>0.123423</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.896200</td>\n","      <td>3.824890</td>\n","      <td>0.124324</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.908200</td>\n","      <td>3.821985</td>\n","      <td>0.126126</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.877700</td>\n","      <td>3.821041</td>\n","      <td>0.127928</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:13]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 02:52:07,003] Trial 19 finished with value: 3.8210413455963135 and parameters: {'num_layers': 15}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 29:09, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.709400</td>\n","      <td>4.703187</td>\n","      <td>0.016216</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.706700</td>\n","      <td>4.692450</td>\n","      <td>0.032432</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.699100</td>\n","      <td>4.680313</td>\n","      <td>0.043243</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.690200</td>\n","      <td>4.665735</td>\n","      <td>0.075676</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.679000</td>\n","      <td>4.645260</td>\n","      <td>0.075676</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.663000</td>\n","      <td>4.622732</td>\n","      <td>0.104505</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.650900</td>\n","      <td>4.591664</td>\n","      <td>0.119820</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.632200</td>\n","      <td>4.560389</td>\n","      <td>0.128829</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.602200</td>\n","      <td>4.528654</td>\n","      <td>0.143243</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.583600</td>\n","      <td>4.494323</td>\n","      <td>0.180180</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.552900</td>\n","      <td>4.456782</td>\n","      <td>0.176577</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.521100</td>\n","      <td>4.418965</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.496100</td>\n","      <td>4.380022</td>\n","      <td>0.209009</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.461800</td>\n","      <td>4.342593</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.437600</td>\n","      <td>4.304060</td>\n","      <td>0.191892</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.400900</td>\n","      <td>4.266027</td>\n","      <td>0.193694</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.376000</td>\n","      <td>4.229206</td>\n","      <td>0.199099</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.335200</td>\n","      <td>4.193513</td>\n","      <td>0.204505</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.301200</td>\n","      <td>4.156723</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.288000</td>\n","      <td>4.125582</td>\n","      <td>0.234234</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.246800</td>\n","      <td>4.095469</td>\n","      <td>0.224324</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.214300</td>\n","      <td>4.064282</td>\n","      <td>0.220721</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.198000</td>\n","      <td>4.037582</td>\n","      <td>0.231532</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.169800</td>\n","      <td>4.010011</td>\n","      <td>0.253153</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.135400</td>\n","      <td>3.986445</td>\n","      <td>0.244144</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.105800</td>\n","      <td>3.958881</td>\n","      <td>0.248649</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.097400</td>\n","      <td>3.933682</td>\n","      <td>0.256757</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.078500</td>\n","      <td>3.909411</td>\n","      <td>0.270270</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.039200</td>\n","      <td>3.889627</td>\n","      <td>0.258559</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.034500</td>\n","      <td>3.870953</td>\n","      <td>0.268468</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.012400</td>\n","      <td>3.850789</td>\n","      <td>0.265766</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.011000</td>\n","      <td>3.833052</td>\n","      <td>0.274775</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.979300</td>\n","      <td>3.815063</td>\n","      <td>0.282883</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.960800</td>\n","      <td>3.798844</td>\n","      <td>0.281982</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.935200</td>\n","      <td>3.785297</td>\n","      <td>0.280180</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.938600</td>\n","      <td>3.771327</td>\n","      <td>0.284685</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.928100</td>\n","      <td>3.757941</td>\n","      <td>0.285586</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.932800</td>\n","      <td>3.746340</td>\n","      <td>0.283784</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.901800</td>\n","      <td>3.735167</td>\n","      <td>0.290991</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.881100</td>\n","      <td>3.724722</td>\n","      <td>0.289189</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.869900</td>\n","      <td>3.717049</td>\n","      <td>0.294595</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.855800</td>\n","      <td>3.708536</td>\n","      <td>0.290991</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.884100</td>\n","      <td>3.701191</td>\n","      <td>0.294595</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.849200</td>\n","      <td>3.695266</td>\n","      <td>0.294595</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.862200</td>\n","      <td>3.689084</td>\n","      <td>0.295495</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.879400</td>\n","      <td>3.685849</td>\n","      <td>0.296396</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.820800</td>\n","      <td>3.682183</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.837300</td>\n","      <td>3.679472</td>\n","      <td>0.297297</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.841600</td>\n","      <td>3.677918</td>\n","      <td>0.297297</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.834600</td>\n","      <td>3.677361</td>\n","      <td>0.297297</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 03:22:14,988] Trial 20 finished with value: 3.6773605346679688 and parameters: {'num_layers': 3}. Best is trial 2 with value: 3.6675639152526855.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 29:21, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.707200</td>\n","      <td>4.699301</td>\n","      <td>0.018018</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.704700</td>\n","      <td>4.688194</td>\n","      <td>0.021622</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.693800</td>\n","      <td>4.673615</td>\n","      <td>0.032432</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.686800</td>\n","      <td>4.654483</td>\n","      <td>0.061261</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.671500</td>\n","      <td>4.632442</td>\n","      <td>0.089189</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.655700</td>\n","      <td>4.605697</td>\n","      <td>0.102703</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.638400</td>\n","      <td>4.578365</td>\n","      <td>0.134234</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.622000</td>\n","      <td>4.548279</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.586700</td>\n","      <td>4.511109</td>\n","      <td>0.152252</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.563500</td>\n","      <td>4.473130</td>\n","      <td>0.168468</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.533200</td>\n","      <td>4.435119</td>\n","      <td>0.174775</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.503300</td>\n","      <td>4.398670</td>\n","      <td>0.199099</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.478200</td>\n","      <td>4.359633</td>\n","      <td>0.210811</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.440500</td>\n","      <td>4.318953</td>\n","      <td>0.198198</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.420000</td>\n","      <td>4.281734</td>\n","      <td>0.219820</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.379800</td>\n","      <td>4.245889</td>\n","      <td>0.216216</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.355700</td>\n","      <td>4.209248</td>\n","      <td>0.198198</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.321400</td>\n","      <td>4.172504</td>\n","      <td>0.208108</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.285200</td>\n","      <td>4.140203</td>\n","      <td>0.215315</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.274700</td>\n","      <td>4.106815</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.230500</td>\n","      <td>4.075804</td>\n","      <td>0.244144</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.200200</td>\n","      <td>4.044574</td>\n","      <td>0.253153</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.178500</td>\n","      <td>4.013026</td>\n","      <td>0.254054</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.148000</td>\n","      <td>3.983796</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.111800</td>\n","      <td>3.958192</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.075100</td>\n","      <td>3.930280</td>\n","      <td>0.269369</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.073200</td>\n","      <td>3.906299</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.050400</td>\n","      <td>3.880914</td>\n","      <td>0.274775</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.021700</td>\n","      <td>3.858989</td>\n","      <td>0.280180</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.009600</td>\n","      <td>3.839175</td>\n","      <td>0.278378</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.983500</td>\n","      <td>3.819587</td>\n","      <td>0.278378</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.976200</td>\n","      <td>3.801380</td>\n","      <td>0.287387</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.946000</td>\n","      <td>3.781101</td>\n","      <td>0.287387</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.931300</td>\n","      <td>3.762866</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.906400</td>\n","      <td>3.750342</td>\n","      <td>0.291892</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.897500</td>\n","      <td>3.732751</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.894300</td>\n","      <td>3.718957</td>\n","      <td>0.306306</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.890200</td>\n","      <td>3.708054</td>\n","      <td>0.301802</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.863000</td>\n","      <td>3.697616</td>\n","      <td>0.298198</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.848800</td>\n","      <td>3.685478</td>\n","      <td>0.301802</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.833600</td>\n","      <td>3.676884</td>\n","      <td>0.304505</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.824400</td>\n","      <td>3.667677</td>\n","      <td>0.307207</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.846000</td>\n","      <td>3.659199</td>\n","      <td>0.321622</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.804800</td>\n","      <td>3.652884</td>\n","      <td>0.317117</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.825500</td>\n","      <td>3.647245</td>\n","      <td>0.318919</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.839500</td>\n","      <td>3.643790</td>\n","      <td>0.318018</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.786100</td>\n","      <td>3.640165</td>\n","      <td>0.318018</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.802000</td>\n","      <td>3.636725</td>\n","      <td>0.318919</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.806400</td>\n","      <td>3.635040</td>\n","      <td>0.321622</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.794400</td>\n","      <td>3.634566</td>\n","      <td>0.321622</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 03:52:31,467] Trial 21 finished with value: 3.634565830230713 and parameters: {'num_layers': 3}. Best is trial 21 with value: 3.634565830230713.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 29:11, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.707200</td>\n","      <td>4.699294</td>\n","      <td>0.018018</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.704600</td>\n","      <td>4.688156</td>\n","      <td>0.021622</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.693800</td>\n","      <td>4.673570</td>\n","      <td>0.034234</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.686800</td>\n","      <td>4.654453</td>\n","      <td>0.061261</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.671500</td>\n","      <td>4.632446</td>\n","      <td>0.089189</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.655700</td>\n","      <td>4.605681</td>\n","      <td>0.100901</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.638300</td>\n","      <td>4.578270</td>\n","      <td>0.133333</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.621900</td>\n","      <td>4.548192</td>\n","      <td>0.139640</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.586700</td>\n","      <td>4.511121</td>\n","      <td>0.153153</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.563500</td>\n","      <td>4.473117</td>\n","      <td>0.168468</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.533200</td>\n","      <td>4.435079</td>\n","      <td>0.173874</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.503200</td>\n","      <td>4.398549</td>\n","      <td>0.199099</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.478200</td>\n","      <td>4.359606</td>\n","      <td>0.211712</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.440500</td>\n","      <td>4.318936</td>\n","      <td>0.200000</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.420000</td>\n","      <td>4.281748</td>\n","      <td>0.219820</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.379800</td>\n","      <td>4.245950</td>\n","      <td>0.216216</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.355700</td>\n","      <td>4.209342</td>\n","      <td>0.198198</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.321500</td>\n","      <td>4.172609</td>\n","      <td>0.208108</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.285300</td>\n","      <td>4.140340</td>\n","      <td>0.216216</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.274900</td>\n","      <td>4.106958</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.230700</td>\n","      <td>4.075962</td>\n","      <td>0.242342</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.200400</td>\n","      <td>4.044775</td>\n","      <td>0.254054</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.178700</td>\n","      <td>4.013245</td>\n","      <td>0.254054</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.148200</td>\n","      <td>3.984004</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.112000</td>\n","      <td>3.958380</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.075300</td>\n","      <td>3.930449</td>\n","      <td>0.269369</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.073300</td>\n","      <td>3.906449</td>\n","      <td>0.258559</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.050600</td>\n","      <td>3.881070</td>\n","      <td>0.274775</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.021800</td>\n","      <td>3.859131</td>\n","      <td>0.281081</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.009700</td>\n","      <td>3.839262</td>\n","      <td>0.278378</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>3.983600</td>\n","      <td>3.819668</td>\n","      <td>0.279279</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.976300</td>\n","      <td>3.801459</td>\n","      <td>0.286486</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.946100</td>\n","      <td>3.781185</td>\n","      <td>0.286486</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.931400</td>\n","      <td>3.762951</td>\n","      <td>0.293694</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.906400</td>\n","      <td>3.750421</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.897500</td>\n","      <td>3.732842</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.894400</td>\n","      <td>3.719043</td>\n","      <td>0.306306</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.890300</td>\n","      <td>3.708138</td>\n","      <td>0.302703</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.863100</td>\n","      <td>3.697693</td>\n","      <td>0.298198</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.848900</td>\n","      <td>3.685554</td>\n","      <td>0.301802</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.833700</td>\n","      <td>3.676960</td>\n","      <td>0.303604</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.824500</td>\n","      <td>3.667761</td>\n","      <td>0.307207</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.846000</td>\n","      <td>3.659285</td>\n","      <td>0.319820</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.804800</td>\n","      <td>3.652960</td>\n","      <td>0.317117</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.825600</td>\n","      <td>3.647326</td>\n","      <td>0.318919</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.839600</td>\n","      <td>3.643871</td>\n","      <td>0.318018</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.786200</td>\n","      <td>3.640239</td>\n","      <td>0.318018</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.802100</td>\n","      <td>3.636806</td>\n","      <td>0.318919</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.806500</td>\n","      <td>3.635117</td>\n","      <td>0.320721</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.794500</td>\n","      <td>3.634643</td>\n","      <td>0.320721</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:09]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 04:22:35,358] Trial 22 finished with value: 3.634643077850342 and parameters: {'num_layers': 3}. Best is trial 21 with value: 3.634565830230713.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 28:26, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.706700</td>\n","      <td>4.700283</td>\n","      <td>0.010811</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.704900</td>\n","      <td>4.688091</td>\n","      <td>0.022523</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.695600</td>\n","      <td>4.674660</td>\n","      <td>0.027928</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.689100</td>\n","      <td>4.657594</td>\n","      <td>0.049550</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.674200</td>\n","      <td>4.637477</td>\n","      <td>0.083784</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.659200</td>\n","      <td>4.612431</td>\n","      <td>0.138739</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.640500</td>\n","      <td>4.583565</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.618200</td>\n","      <td>4.553049</td>\n","      <td>0.179279</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.596200</td>\n","      <td>4.521086</td>\n","      <td>0.206306</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.576700</td>\n","      <td>4.487890</td>\n","      <td>0.199099</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.544100</td>\n","      <td>4.456224</td>\n","      <td>0.201802</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.520200</td>\n","      <td>4.422111</td>\n","      <td>0.208108</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.491700</td>\n","      <td>4.388116</td>\n","      <td>0.238739</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.471500</td>\n","      <td>4.353439</td>\n","      <td>0.222523</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.446000</td>\n","      <td>4.320489</td>\n","      <td>0.237838</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.416000</td>\n","      <td>4.286528</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.401100</td>\n","      <td>4.253952</td>\n","      <td>0.234234</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.360700</td>\n","      <td>4.222486</td>\n","      <td>0.236036</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.332600</td>\n","      <td>4.186555</td>\n","      <td>0.241441</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.301800</td>\n","      <td>4.152401</td>\n","      <td>0.260360</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.272400</td>\n","      <td>4.121102</td>\n","      <td>0.261261</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.252200</td>\n","      <td>4.089577</td>\n","      <td>0.269369</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.226100</td>\n","      <td>4.059015</td>\n","      <td>0.272973</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.180500</td>\n","      <td>4.028825</td>\n","      <td>0.272973</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.159000</td>\n","      <td>4.001961</td>\n","      <td>0.266667</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.143700</td>\n","      <td>3.976625</td>\n","      <td>0.274775</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.098500</td>\n","      <td>3.948562</td>\n","      <td>0.284685</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.116500</td>\n","      <td>3.924656</td>\n","      <td>0.293694</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.073600</td>\n","      <td>3.902022</td>\n","      <td>0.289189</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.070800</td>\n","      <td>3.879324</td>\n","      <td>0.303604</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.023400</td>\n","      <td>3.858323</td>\n","      <td>0.295495</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>4.009600</td>\n","      <td>3.839548</td>\n","      <td>0.307207</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>4.009800</td>\n","      <td>3.819437</td>\n","      <td>0.307207</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.967900</td>\n","      <td>3.800153</td>\n","      <td>0.313514</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.956400</td>\n","      <td>3.782989</td>\n","      <td>0.316216</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.945700</td>\n","      <td>3.768972</td>\n","      <td>0.314414</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.923700</td>\n","      <td>3.753229</td>\n","      <td>0.323423</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.921400</td>\n","      <td>3.739704</td>\n","      <td>0.320721</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.926100</td>\n","      <td>3.726725</td>\n","      <td>0.326126</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.878200</td>\n","      <td>3.714749</td>\n","      <td>0.328829</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.894800</td>\n","      <td>3.705456</td>\n","      <td>0.326126</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.871800</td>\n","      <td>3.698106</td>\n","      <td>0.327027</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.858000</td>\n","      <td>3.687911</td>\n","      <td>0.327027</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.869000</td>\n","      <td>3.681520</td>\n","      <td>0.332432</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.885000</td>\n","      <td>3.676747</td>\n","      <td>0.331532</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.840300</td>\n","      <td>3.671725</td>\n","      <td>0.335135</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.846600</td>\n","      <td>3.667381</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.845800</td>\n","      <td>3.664528</td>\n","      <td>0.334234</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.839100</td>\n","      <td>3.662749</td>\n","      <td>0.333333</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.862900</td>\n","      <td>3.662410</td>\n","      <td>0.335135</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 04:52:02,033] Trial 23 finished with value: 3.662409543991089 and parameters: {'num_layers': 2}. Best is trial 21 with value: 3.634565830230713.\n","/home/rag/base_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-xls-r-300m and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/home/rag/base_venv/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11100' max='11100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11100/11100 28:54, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>222</td>\n","      <td>4.707500</td>\n","      <td>4.698718</td>\n","      <td>0.018919</td>\n","    </tr>\n","    <tr>\n","      <td>444</td>\n","      <td>4.703100</td>\n","      <td>4.685509</td>\n","      <td>0.033333</td>\n","    </tr>\n","    <tr>\n","      <td>666</td>\n","      <td>4.695700</td>\n","      <td>4.670607</td>\n","      <td>0.047748</td>\n","    </tr>\n","    <tr>\n","      <td>888</td>\n","      <td>4.679300</td>\n","      <td>4.651435</td>\n","      <td>0.055856</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>4.668700</td>\n","      <td>4.627486</td>\n","      <td>0.081982</td>\n","    </tr>\n","    <tr>\n","      <td>1332</td>\n","      <td>4.649000</td>\n","      <td>4.597727</td>\n","      <td>0.098198</td>\n","    </tr>\n","    <tr>\n","      <td>1554</td>\n","      <td>4.627400</td>\n","      <td>4.565370</td>\n","      <td>0.118018</td>\n","    </tr>\n","    <tr>\n","      <td>1776</td>\n","      <td>4.603800</td>\n","      <td>4.536026</td>\n","      <td>0.148649</td>\n","    </tr>\n","    <tr>\n","      <td>1998</td>\n","      <td>4.582400</td>\n","      <td>4.498976</td>\n","      <td>0.150450</td>\n","    </tr>\n","    <tr>\n","      <td>2220</td>\n","      <td>4.553200</td>\n","      <td>4.462032</td>\n","      <td>0.163964</td>\n","    </tr>\n","    <tr>\n","      <td>2442</td>\n","      <td>4.520000</td>\n","      <td>4.423743</td>\n","      <td>0.165766</td>\n","    </tr>\n","    <tr>\n","      <td>2664</td>\n","      <td>4.488200</td>\n","      <td>4.389455</td>\n","      <td>0.164865</td>\n","    </tr>\n","    <tr>\n","      <td>2886</td>\n","      <td>4.464100</td>\n","      <td>4.354001</td>\n","      <td>0.163964</td>\n","    </tr>\n","    <tr>\n","      <td>3108</td>\n","      <td>4.440600</td>\n","      <td>4.316078</td>\n","      <td>0.181081</td>\n","    </tr>\n","    <tr>\n","      <td>3330</td>\n","      <td>4.411800</td>\n","      <td>4.280097</td>\n","      <td>0.190991</td>\n","    </tr>\n","    <tr>\n","      <td>3552</td>\n","      <td>4.379800</td>\n","      <td>4.246301</td>\n","      <td>0.199099</td>\n","    </tr>\n","    <tr>\n","      <td>3774</td>\n","      <td>4.360900</td>\n","      <td>4.213735</td>\n","      <td>0.189189</td>\n","    </tr>\n","    <tr>\n","      <td>3996</td>\n","      <td>4.317300</td>\n","      <td>4.181690</td>\n","      <td>0.191892</td>\n","    </tr>\n","    <tr>\n","      <td>4218</td>\n","      <td>4.289800</td>\n","      <td>4.150507</td>\n","      <td>0.197297</td>\n","    </tr>\n","    <tr>\n","      <td>4440</td>\n","      <td>4.263900</td>\n","      <td>4.118230</td>\n","      <td>0.205405</td>\n","    </tr>\n","    <tr>\n","      <td>4662</td>\n","      <td>4.233700</td>\n","      <td>4.089144</td>\n","      <td>0.213514</td>\n","    </tr>\n","    <tr>\n","      <td>4884</td>\n","      <td>4.212800</td>\n","      <td>4.057606</td>\n","      <td>0.212613</td>\n","    </tr>\n","    <tr>\n","      <td>5106</td>\n","      <td>4.191200</td>\n","      <td>4.031340</td>\n","      <td>0.219820</td>\n","    </tr>\n","    <tr>\n","      <td>5328</td>\n","      <td>4.148500</td>\n","      <td>4.003383</td>\n","      <td>0.228829</td>\n","    </tr>\n","    <tr>\n","      <td>5550</td>\n","      <td>4.125100</td>\n","      <td>3.976451</td>\n","      <td>0.242342</td>\n","    </tr>\n","    <tr>\n","      <td>5772</td>\n","      <td>4.111700</td>\n","      <td>3.953743</td>\n","      <td>0.237838</td>\n","    </tr>\n","    <tr>\n","      <td>5994</td>\n","      <td>4.074600</td>\n","      <td>3.931357</td>\n","      <td>0.246847</td>\n","    </tr>\n","    <tr>\n","      <td>6216</td>\n","      <td>4.088700</td>\n","      <td>3.909763</td>\n","      <td>0.254054</td>\n","    </tr>\n","    <tr>\n","      <td>6438</td>\n","      <td>4.050100</td>\n","      <td>3.886426</td>\n","      <td>0.259459</td>\n","    </tr>\n","    <tr>\n","      <td>6660</td>\n","      <td>4.046300</td>\n","      <td>3.869372</td>\n","      <td>0.265766</td>\n","    </tr>\n","    <tr>\n","      <td>6882</td>\n","      <td>4.003900</td>\n","      <td>3.851006</td>\n","      <td>0.267568</td>\n","    </tr>\n","    <tr>\n","      <td>7104</td>\n","      <td>3.993200</td>\n","      <td>3.832940</td>\n","      <td>0.255856</td>\n","    </tr>\n","    <tr>\n","      <td>7326</td>\n","      <td>3.994600</td>\n","      <td>3.815448</td>\n","      <td>0.271171</td>\n","    </tr>\n","    <tr>\n","      <td>7548</td>\n","      <td>3.952200</td>\n","      <td>3.801003</td>\n","      <td>0.273874</td>\n","    </tr>\n","    <tr>\n","      <td>7770</td>\n","      <td>3.948900</td>\n","      <td>3.784320</td>\n","      <td>0.276577</td>\n","    </tr>\n","    <tr>\n","      <td>7992</td>\n","      <td>3.935900</td>\n","      <td>3.771408</td>\n","      <td>0.278378</td>\n","    </tr>\n","    <tr>\n","      <td>8214</td>\n","      <td>3.919000</td>\n","      <td>3.758341</td>\n","      <td>0.288288</td>\n","    </tr>\n","    <tr>\n","      <td>8436</td>\n","      <td>3.914700</td>\n","      <td>3.746286</td>\n","      <td>0.281982</td>\n","    </tr>\n","    <tr>\n","      <td>8658</td>\n","      <td>3.916700</td>\n","      <td>3.735657</td>\n","      <td>0.290991</td>\n","    </tr>\n","    <tr>\n","      <td>8880</td>\n","      <td>3.878000</td>\n","      <td>3.724333</td>\n","      <td>0.287387</td>\n","    </tr>\n","    <tr>\n","      <td>9102</td>\n","      <td>3.893900</td>\n","      <td>3.715793</td>\n","      <td>0.291892</td>\n","    </tr>\n","    <tr>\n","      <td>9324</td>\n","      <td>3.860800</td>\n","      <td>3.708762</td>\n","      <td>0.286486</td>\n","    </tr>\n","    <tr>\n","      <td>9546</td>\n","      <td>3.856900</td>\n","      <td>3.700595</td>\n","      <td>0.290991</td>\n","    </tr>\n","    <tr>\n","      <td>9768</td>\n","      <td>3.871200</td>\n","      <td>3.694114</td>\n","      <td>0.290991</td>\n","    </tr>\n","    <tr>\n","      <td>9990</td>\n","      <td>3.885100</td>\n","      <td>3.689937</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>10212</td>\n","      <td>3.845500</td>\n","      <td>3.685937</td>\n","      <td>0.291892</td>\n","    </tr>\n","    <tr>\n","      <td>10434</td>\n","      <td>3.843200</td>\n","      <td>3.682016</td>\n","      <td>0.293694</td>\n","    </tr>\n","    <tr>\n","      <td>10656</td>\n","      <td>3.847500</td>\n","      <td>3.679405</td>\n","      <td>0.292793</td>\n","    </tr>\n","    <tr>\n","      <td>10878</td>\n","      <td>3.832500</td>\n","      <td>3.677732</td>\n","      <td>0.293694</td>\n","    </tr>\n","    <tr>\n","      <td>11100</td>\n","      <td>3.862700</td>\n","      <td>3.677327</td>\n","      <td>0.293694</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["[I 2024-06-19 05:21:50,192] Trial 24 finished with value: 3.6773271560668945 and parameters: {'num_layers': 2}. Best is trial 21 with value: 3.634565830230713.\n"]},{"name":"stdout","output_type":"stream","text":["Best hyperparameters: {'num_layers': 3}\n"]},{"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 251\u001b[0m\n\u001b[1;32m    248\u001b[0m best_model_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/best_model_xlsr_finetuning_optuna_layer_optimized\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(best_model_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 251\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(best_model_dir)\n\u001b[1;32m    252\u001b[0m processor\u001b[38;5;241m.\u001b[39msave_pretrained(best_model_dir)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"]}],"source":["import os\n","import sys\n","import torch\n","import librosa\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data import Dataset\n","from transformers import Wav2Vec2Processor, Wav2Vec2Model, Trainer, TrainingArguments, TrainerCallback, Wav2Vec2FeatureExtractor, Wav2Vec2Config\n","import math\n","from datasets import load_metric\n","from datetime import datetime\n","import torch.nn as nn\n","import optuna\n","\n","# Load the processor\n","processor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","\n","# Define the custom dataset class using pandas\n","class LocalAudioDataset(Dataset):\n","    def __init__(self, csv_file, processor, subset, noise_factor=0.0):\n","        self.processor = processor\n","        self.data = pd.read_csv(csv_file)\n","        self.data = self.data[self.data['subset'] == subset]\n","        self.speaker_ids = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n","        self.data['label'] = self.data['label'].map(self.speaker_ids)\n","        self.noise_factor = noise_factor\n","        \n","        print(f\"Loaded {len(self.speaker_ids)} speakers: {self.speaker_ids}\")\n","        print(f\"Total files in {subset}: {len(self.data)}\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx, retry_count=0):\n","        file_path = self.data.iloc[idx]['path']\n","        label = self.data.iloc[idx]['label']\n","        \n","        try:\n","            audio, sr = librosa.load(file_path, sr=16000)\n","            audio = librosa.to_mono(audio)\n","            audio = self._pad_or_truncate(audio, max_length=16000)\n","            if self.noise_factor > 0:\n","                audio = self._add_noise(audio)\n","            input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.squeeze(0)\n","            return {\"input_values\": input_values, \"labels\": label}\n","        except Exception as e:\n","            if retry_count < 3:  # Retry up to 3 times\n","                return self.__getitem__((idx + 1) % len(self), retry_count + 1)\n","            else:\n","                print(f\"Error loading {file_path}: {e}\", file=sys.stderr)\n","                raise e  # Raise exception if retry limit is reached\n","\n","    def _pad_or_truncate(self, audio, max_length):\n","        if len(audio) < max_length:\n","            pad_size = max_length - len(audio)\n","            audio = np.pad(audio, (0, pad_size), 'constant', constant_values=(0, 0))\n","        else:\n","            audio = audio[:max_length]\n","        return audio\n","\n","    def _add_noise(self, audio):\n","        noise = np.random.randn(len(audio))\n","        augmented_audio = audio + self.noise_factor * noise\n","        augmented_audio = augmented_audio.astype(type(audio[0]))\n","        return augmented_audio\n","\n","# Paths to dataset CSV file\n","csv_file = 'dataset_large.csv'\n","train_dataset = LocalAudioDataset(csv_file, processor, 'train')\n","validate_dataset = LocalAudioDataset(csv_file, processor, 'validate')\n","test_dataset = LocalAudioDataset(csv_file, processor, 'test')\n","\n","num_speakers = len(train_dataset.speaker_ids)\n","print(f\"Number of unique speakers: {num_speakers}\")\n","\n","print(f\"Labels in train dataset: {train_dataset.data['label'].tolist()}\")\n","print(f\"Labels in test dataset: {test_dataset.data['label'].tolist()}\")\n","\n","# Define a custom classification head on top of the base Wav2Vec2 model\n","class Wav2Vec2ClassificationHead(nn.Module):\n","    def __init__(self, config, num_labels):\n","        super().__init__()\n","        self.dropout = nn.Dropout(config.hidden_dropout)\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take the mean of the hidden states of the first token\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","# Define the full model by combining Wav2Vec2Model with the classification head\n","class Wav2Vec2ForCustomClassification(nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-xls-r-300m\")\n","        self.classifier = Wav2Vec2ClassificationHead(self.wav2vec2.config, num_labels)\n","\n","    def forward(self, input_values, attention_mask=None, labels=None):\n","        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n","        hidden_states = outputs.last_hidden_state\n","        logits = self.classifier(hidden_states)\n","        \n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n","        \n","        return (loss, logits) if loss is not None else logits\n","\n","# Instantiate the model with the custom classification head\n","model = Wav2Vec2ForCustomClassification(num_labels=num_speakers)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","model = model.to(device)\n","\n","def validate_labels(dataset):\n","    for item in dataset:\n","        label = item['labels']\n","        if label >= num_speakers or label < 0:\n","            print(f\"Invalid label {label} for item: {item}\")\n","            raise ValueError(f\"Invalid label {label} found in dataset.\")\n","    print(\"All labels are valid.\")\n","\n","validate_labels(train_dataset)\n","validate_labels(validate_dataset)\n","validate_labels(test_dataset)\n","\n","batch_size = 8\n","steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n","logging_steps = steps_per_epoch // 5\n","eval_steps = steps_per_epoch // 5\n","\n","accuracy_metric = load_metric(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return accuracy_metric.compute(predictions=predictions, references=labels)\n","\n","log_dir = \"/home/rag/experimental_trial/results/training_logs\"\n","os.makedirs(log_dir, exist_ok=True)\n","timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","log_file = os.path.join(log_dir, f\"training_logxlsr_finetuning_optimizing_layers_{timestamp}.csv\")\n","with open(log_file, \"w\") as f:\n","    f.write(\"Timestamp,Step,Training Loss,Validation Loss,Accuracy\\n\")\n","\n","class SaveMetricsCallback(TrainerCallback):\n","    def on_log(self, args, state, control, logs=None, **kwargs):\n","        if logs is not None:\n","            with open(log_file, \"a\") as f:\n","                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","                step = state.global_step\n","                training_loss = logs.get(\"loss\", \"\")\n","                validation_loss = logs.get(\"eval_loss\", \"\")\n","                accuracy = logs.get(\"eval_accuracy\", \"\")\n","                f.write(f\"{timestamp},{step},{training_loss},{validation_loss},{accuracy}\\n\")\n","\n","class EarlyStoppingCallback(TrainerCallback):\n","    def __init__(self, early_stopping_patience=100, early_stopping_threshold=0.0):\n","        self.early_stopping_patience = early_stopping_patience\n","        self.early_stopping_threshold = early_stopping_threshold\n","        self.best_metric = None\n","        self.patience_counter = 0\n","\n","    def on_evaluate(self, args, state, control, **kwargs):\n","        metric = kwargs.get(\"metrics\", {}).get(\"eval_loss\")\n","        if metric is None:\n","            return\n","        \n","        if self.best_metric is None or metric < self.best_metric - self.early_stopping_threshold:\n","            self.best_metric = metric\n","            self.patience_counter = 0\n","        else:\n","            self.patience_counter += 1\n","        \n","        if self.patience_counter >= self.early_stopping_patience:\n","            print(f\"Early stopping at step {state.global_step}\")\n","            control.should_training_stop = True\n","\n","# Define the Optuna objective function\n","def objective(trial):\n","    # Suggest the number of layers\n","    num_layers = trial.suggest_int('num_layers', 1, 24)\n","\n","    # Load the model configuration with the suggested number of layers\n","    config = Wav2Vec2Config.from_pretrained(\"facebook/wav2vec2-xls-r-300m\", num_labels=num_speakers)\n","    config.num_hidden_layers = num_layers\n","\n","    # Instantiate the model with the custom classification head\n","    model = Wav2Vec2ForCustomClassification(num_labels=num_speakers)\n","    model.wav2vec2.encoder.layers = model.wav2vec2.encoder.layers[:num_layers]\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    # Ensure 'no_cuda' parameter aligns with device availability\n","    training_args = TrainingArguments(\n","        output_dir=\"./results\",\n","        group_by_length=True,\n","        per_device_train_batch_size=batch_size,\n","        evaluation_strategy=\"steps\",\n","        num_train_epochs=10,  # Use 10 epochs\n","        save_steps=logging_steps,\n","        eval_steps=eval_steps,\n","        logging_steps=logging_steps,\n","        learning_rate=5e-6,  # Fixed learning rate\n","        save_total_limit=2,\n","        no_cuda=not torch.cuda.is_available(),\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"eval_loss\",\n","        greater_is_better=False,\n","        save_strategy=\"steps\"\n","    )\n","\n","    # Add early stopping callback to the trainer\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=validate_dataset,\n","        tokenizer=processor,\n","        compute_metrics=compute_metrics,\n","        callbacks=[SaveMetricsCallback(), EarlyStoppingCallback()]\n","    )\n","\n","    # Train and evaluate\n","    trainer.train()\n","    \n","    metrics = trainer.evaluate(validate_dataset)\n","    \n","    # Return the evaluation loss for Optuna to minimize\n","    return metrics[\"eval_loss\"]\n","\n","# Create an Optuna study and optimize the objective function\n","study = optuna.create_study(direction=\"minimize\")\n","study.optimize(objective, n_trials=25)\n","\n","# Print the best hyperparameters found\n","print(f\"Best hyperparameters: {study.best_params}\")\n","\n","# Save the best model and processor\n","best_model_dir = \"./results/best_model_xlsr_finetuning_optuna_layer_optimized\"\n","os.makedirs(best_model_dir, exist_ok=True)\n","\n","trainer.save_model(best_model_dir)\n","processor.save_pretrained(best_model_dir)\n","\n","print(f\"Best model saved to {best_model_dir}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN+yvvv6PVerTjdhNTDh9OE","gpuType":"T4","mount_file_id":"1WM9o59Rmu2-KuQu0mfCOrhA_e8D1TdAj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"08103e90e897474fa82bf032221e11fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c0b00113d9d4d17a9324aac1a1b7812":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8512d416bd84a0e8ccff509c4aa998b","max":212,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfa5aba7c0b84a5bad48b6505ecb5246","value":212}},"0c11e4c65a9c42deb48b73e70e421873":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13dcfcdb1bb548be992630d634303121":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"202535d409cb4df4b9f36f79a56a444f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ed129a6b9834f12a8c8b2155e09ca23","placeholder":"​","style":"IPY_MODEL_a27dff3aa8a34489bd6cd47ebabc7d27","value":" 1.57k/1.57k [00:00&lt;00:00, 90.4kB/s]"}},"300ac59f66cd4ba19a8e05558766d56e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"362b4b76ecf74aefbd85b27a41daf87d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"402a6e29fa5d4216842716be09bdec0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ed129a6b9834f12a8c8b2155e09ca23":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee8cc88f6f8429da17d627c5bcfbaee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f89d5eddaa542498f9e22f51dd0379e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f00321cd01948d081f90d5fb0399ac7","IPY_MODEL_6c38047dff0b49c3917b73465362d015","IPY_MODEL_202535d409cb4df4b9f36f79a56a444f"],"layout":"IPY_MODEL_402a6e29fa5d4216842716be09bdec0b"}},"6c38047dff0b49c3917b73465362d015":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2f7f474e9b491ca0172dc850ab005d","max":1568,"min":0,"orientation":"horizontal","style":"IPY_MODEL_862426b3ac80408584610f2067df9c55","value":1568}},"6e9ae6e783a04f1492df54d198e9a844":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73cfdb8db08f4c62a0199b21da0890d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e9ae6e783a04f1492df54d198e9a844","placeholder":"​","style":"IPY_MODEL_300ac59f66cd4ba19a8e05558766d56e","value":" 212/212 [00:00&lt;00:00, 15.3kB/s]"}},"7f8c3d92d1cb4e1db403794993823d65":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8497181edc074e65b048e1f70a664a46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862426b3ac80408584610f2067df9c55":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ecdc35e441c4e0b9515446383caf7c1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f00321cd01948d081f90d5fb0399ac7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f8c3d92d1cb4e1db403794993823d65","placeholder":"​","style":"IPY_MODEL_362b4b76ecf74aefbd85b27a41daf87d","value":"config.json: 100%"}},"91a74ab240a54735a18da7e1cf8cde5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_922f60e005c242a29e00de5b56d9eff2","IPY_MODEL_0c0b00113d9d4d17a9324aac1a1b7812","IPY_MODEL_73cfdb8db08f4c62a0199b21da0890d6"],"layout":"IPY_MODEL_8ecdc35e441c4e0b9515446383caf7c1"}},"922f60e005c242a29e00de5b56d9eff2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a93d7b6215cb42e790b2bb08c7fa910c","placeholder":"​","style":"IPY_MODEL_5ee8cc88f6f8429da17d627c5bcfbaee","value":"preprocessor_config.json: 100%"}},"92d3680ff8ff49c0a3f21a13201bfecc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c11e4c65a9c42deb48b73e70e421873","placeholder":"​","style":"IPY_MODEL_d1f43d48550f49cc89cb1305ad3120d6","value":" 1.27G/1.27G [00:12&lt;00:00, 167MB/s]"}},"93dc414ffd09438db90797e3d50c5883":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8497181edc074e65b048e1f70a664a46","placeholder":"​","style":"IPY_MODEL_13dcfcdb1bb548be992630d634303121","value":"pytorch_model.bin: 100%"}},"9d2f7f474e9b491ca0172dc850ab005d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e5d01709684425988f881a95f1ff96e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f62ab26c3d144c7c850a8eb6e35dfc64","max":1269737156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b147449ceb4a4f289a7fac9283e497f4","value":1269737156}},"a27dff3aa8a34489bd6cd47ebabc7d27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a93d7b6215cb42e790b2bb08c7fa910c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b147449ceb4a4f289a7fac9283e497f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1f43d48550f49cc89cb1305ad3120d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd399b2c6d3a4d4ab667b6c91753f6e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93dc414ffd09438db90797e3d50c5883","IPY_MODEL_9e5d01709684425988f881a95f1ff96e","IPY_MODEL_92d3680ff8ff49c0a3f21a13201bfecc"],"layout":"IPY_MODEL_08103e90e897474fa82bf032221e11fb"}},"dfa5aba7c0b84a5bad48b6505ecb5246":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f62ab26c3d144c7c850a8eb6e35dfc64":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8512d416bd84a0e8ccff509c4aa998b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
