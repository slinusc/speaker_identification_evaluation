{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperModel\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the processor and model for Whisper\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "model = WhisperModel.from_pretrained(\"openai/whisper-large\", output_hidden_states=True)\n",
    "model.to(device)\n",
    "\n",
    "def check_directories_exist(directory, layer_indices):\n",
    "    \"\"\"Prüft, ob die benötigten Verzeichnisse für jede Schicht bereits existieren.\"\"\"\n",
    "    all_exist = True\n",
    "    for index in layer_indices:\n",
    "        layer_dir = os.path.join(directory, f\"layer_{index}\")\n",
    "        if not os.path.exists(layer_dir):\n",
    "            all_exist = False\n",
    "            break\n",
    "    return all_exist\n",
    "\n",
    "def load_audio_files(input_directory, output_directory, layer_indices=[-1]):\n",
    "    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n",
    "    for filename in tqdm(os.listdir(input_directory)):\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            audio, sr = librosa.load(file_path, sr=16000)\n",
    "            inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "            input_values = inputs[\"input_features\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.encoder(input_values)\n",
    "                for index in layer_indices:\n",
    "                    hidden_states = outputs.hidden_states[index]\n",
    "                    mean_pooled_hidden_states = hidden_states.mean(dim=1)  # Mean Pooling über die Zeitdimension\n",
    "                    # creating sub directory for each layer in output directory\n",
    "                    layer_dir = os.path.join(output_directory, f\"layer_{index}\")\n",
    "                    os.makedirs(layer_dir, exist_ok=True)\n",
    "                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n",
    "                    np.save(save_path, mean_pooled_hidden_states.cpu().numpy())\n",
    "\n",
    "def process_audio_directory(input_base_directory, output_base_directory, layer_indices=range(25)):\n",
    "    \"\"\"Verarbeitet Audio-Dateien in den angegebenen Verzeichnissen und speichert die Ergebnisse im Zielverzeichnis.\"\"\"\n",
    "    for d in os.listdir(input_base_directory):\n",
    "        input_dir_path = os.path.join(input_base_directory, d)\n",
    "        output_dir_path = os.path.join(output_base_directory, d)\n",
    "        if os.path.isdir(input_dir_path) and not check_directories_exist(output_dir_path, layer_indices):\n",
    "            load_audio_files(input_dir_path, output_dir_path, layer_indices)\n",
    "\n",
    "input_directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_backup\")\n",
    "output_directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_whisper\")\n",
    "process_audio_directory(input_directory_path, output_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we finetune whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import WhisperProcessor, WhisperModel, Trainer, TrainingArguments, TrainerCallback, WhisperConfig\n",
    "import math\n",
    "from datasets import load_metric\n",
    "from datetime import datetime\n",
    "\n",
    "class WhisperForSequenceClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(WhisperForSequenceClassification, self).__init__()\n",
    "        self.whisper = WhisperModel.from_pretrained(\"openai/whisper-large\")\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        outputs = self.whisper(input_values).last_hidden_state\n",
    "        pooled_output = outputs.mean(dim=1)  # Mean Pooling über die Zeitdimension\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "        \n",
    "        return (loss, logits) if loss is not None else logits\n",
    "\n",
    "# Initialize the processor and model for Whisper\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# Define the custom dataset class using pandas\n",
    "class LocalAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, processor, subset):\n",
    "        self.processor = processor\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data = self.data[self.data['subset'] == subset]\n",
    "        self.speaker_ids = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n",
    "        self.data['label'] = self.data['label'].map(self.speaker_ids)\n",
    "        \n",
    "        print(f\"Loaded {len(self.speaker_ids)} speakers: {self.speaker_ids}\")\n",
    "        print(f\"Total files in {subset}: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data.iloc[idx]['path']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=16000)\n",
    "            audio = librosa.to_mono(audio)\n",
    "            audio = self._pad_or_truncate(audio, max_length=16000)\n",
    "            input_values = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\").input_values.squeeze(0)\n",
    "            return {\"input_values\": input_values, \"labels\": label}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\", file=sys.stderr)\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "    def _pad_or_truncate(self, audio, max_length):\n",
    "        if len(audio) < max_length:\n",
    "            pad_size = max_length - len(audio)\n",
    "            audio = np.pad(audio, (0, pad_size), 'constant', constant_values=(0, 0))\n",
    "        else:\n",
    "            audio = audio[:max_length]\n",
    "        return audio\n",
    "\n",
    "# Paths to dataset CSV file\n",
    "csv_file = 'dataset_large.csv'\n",
    "train_dataset = LocalAudioDataset(csv_file, processor, 'train')\n",
    "validate_dataset = LocalAudioDataset(csv_file, processor, 'validate')\n",
    "test_dataset = LocalAudioDataset(csv_file, processor, 'test')\n",
    "\n",
    "num_speakers = len(train_dataset.speaker_ids)\n",
    "config = WhisperConfig.from_pretrained(\"openai/whisper-large\", num_labels=num_speakers)\n",
    "model = WhisperForSequenceClassification(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "def validate_labels(dataset):\n",
    "    for item in dataset:\n",
    "        label = item['labels']\n",
    "        if label >= num_speakers or label < 0:\n",
    "            print(f\"Invalid label {label} for item: {item}\")\n",
    "            raise ValueError(f\"Invalid label {label} found in dataset.\")\n",
    "    print(\"All labels are valid.\")\n",
    "\n",
    "validate_labels(train_dataset)\n",
    "validate_labels(validate_dataset)\n",
    "validate_labels(test_dataset)\n",
    "\n",
    "batch_size = 8\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "logging_steps = steps_per_epoch // 5\n",
    "eval_steps = steps_per_epoch // 5\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "log_dir = \"/home/rag/experimental_trial/results/training_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(log_dir, f\"training_log_100_epochs_5_layer{timestamp}.csv\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    f.write(\"Timestamp,Step,Training Loss,Validation Loss,Accuracy\\n\")\n",
    "\n",
    "class SaveMetricsCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            with open(log_file, \"a\") as f:\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                step = state.global_step\n",
    "                training_loss = logs.get(\"loss\", \"\")\n",
    "                validation_loss = logs.get(\"eval_loss\", \"\")\n",
    "                accuracy = logs.get(\"eval_accuracy\", \"\")\n",
    "                f.write(f\"{timestamp},{step},{training_loss},{validation_loss},{accuracy}\\n\")\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=100, early_stopping_threshold=0.0):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.best_metric = None\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metric = kwargs.get(\"metrics\", {}).get(\"eval_loss\")\n",
    "        if metric is None:\n",
    "            return\n",
    "        \n",
    "        if self.best_metric is None or metric < self.best_metric - self.early_stopping_threshold:\n",
    "            self.best_metric = metric\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        if self.patience_counter >= self.early_stopping_patience:\n",
    "            print(f\"Early stopping at step {state.global_step}\")\n",
    "            control.should_training_stop = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=100,\n",
    "    save_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=5e-6,\n",
    "    save_total_limit=2,\n",
    "    no_cuda=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,  # lower eval_loss is better\n",
    "    save_strategy=\"steps\"  # or \"epoch\" if you prefer to save every epoch\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validate_dataset,\n",
    "    tokenizer=processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[SaveMetricsCallback(), EarlyStoppingCallback()]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test set evaluation metrics: {metrics}\")\n",
    "print(\"Training and evaluation completed successfully!\")\n",
    "\n",
    "best_model_dir = \"./results/best_model_100_epochs_5_layer\"\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "\n",
    "trainer.save_model(best_model_dir)\n",
    "processor.save_pretrained(best_model_dir)\n",
    "\n",
    "print(f\"Best model saved to {best_model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparam tuning for whisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we optimize the number of layers used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import Trainer, TrainingArguments, TrainerCallback, WhisperConfig, WhisperModel, WhisperProcessor\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from datasets import load_metric\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set up logging for Optuna\n",
    "log_dir = \"/home/rag/experimental_trial/results/training_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(log_dir, f\"training_log_optuna_optim_whisper{timestamp}.csv\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Add file handler to logger\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Redirect Optuna logging to the file\n",
    "optuna_logger = logging.getLogger(\"optuna\")\n",
    "optuna_logger.addHandler(file_handler)\n",
    "\n",
    "# Load the processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# Define the custom dataset class\n",
    "class LocalAudioDataset(Dataset):\n",
    "    def __init__(self, csv_file, processor, subset, noise_factor=0.0, max_speakers=50):\n",
    "        self.processor = processor\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data = self.data[self.data['subset'] == subset]\n",
    "        \n",
    "        # Limit the number of speakers to max_speakers\n",
    "        speaker_counts = self.data['label'].value_counts()\n",
    "        top_speakers = speaker_counts.nlargest(max_speakers).index\n",
    "        self.data = self.data[self.data['label'].isin(top_speakers)]\n",
    "        \n",
    "        self.speaker_ids = {label: idx for idx, label in enumerate(self.data['label'].unique())}\n",
    "        self.data['label'] = self.data['label'].map(self.speaker_ids)\n",
    "        self.noise_factor = noise_factor\n",
    "        \n",
    "        print(f\"Loaded {len(self.speaker_ids)} speakers: {self.speaker_ids}\")\n",
    "        print(f\"Total files in {subset}: {len(self.data)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data.iloc[idx]['path']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=16000)\n",
    "            audio = librosa.to_mono(audio)\n",
    "            # Use the processor to extract features\n",
    "            inputs = self.processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "            input_values = inputs.input_features.squeeze(0)\n",
    "            return {\"input_values\": input_values, \"labels\": label}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\", file=sys.stderr)\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# Paths to dataset CSV file\n",
    "csv_file = 'dataset_large.csv'\n",
    "train_dataset = LocalAudioDataset(csv_file, processor, 'train', noise_factor=0, max_speakers=111)\n",
    "validate_dataset = LocalAudioDataset(csv_file, processor, 'validate', max_speakers=111)\n",
    "test_dataset = LocalAudioDataset(csv_file, processor, 'test', max_speakers=111)\n",
    "\n",
    "num_speakers = len(train_dataset.speaker_ids)\n",
    "print(f\"Number of unique speakers: {num_speakers}\")\n",
    "\n",
    "print(f\"Labels in train dataset: {train_dataset.data['label'].tolist()}\")\n",
    "print(f\"Labels in test dataset: {test_dataset.data['label'].tolist()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def validate_labels(dataset):\n",
    "    for item in dataset:\n",
    "        label = item['labels']\n",
    "        if label >= num_speakers or label < 0:\n",
    "            print(f\"Invalid label {label} for item: {item}\")\n",
    "            raise ValueError(f\"Invalid label {label} found in dataset.\")\n",
    "    print(\"All labels are valid.\")\n",
    "\n",
    "batch_size = 2\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "logging_steps = steps_per_epoch // 5\n",
    "eval_steps = steps_per_epoch // 5\n",
    "\n",
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class SaveMetricsCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            with open(log_file, \"a\") as f:\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                step = state.global_step\n",
    "                training_loss = logs.get(\"loss\", \"\")\n",
    "                validation_loss = logs.get(\"eval_loss\", \"\")\n",
    "                accuracy = logs.get(\"eval_accuracy\", \"\")\n",
    "                f.write(f\"{timestamp},{step},{training_loss},{validation_loss},{accuracy}\\n\")\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=100, early_stopping_threshold=0.0):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.best_metric = None\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metric = kwargs.get(\"metrics\", {}).get(\"eval_loss\")\n",
    "        if metric is None:\n",
    "            return\n",
    "        \n",
    "        if self.best_metric is None or metric < self.best_metric - self.early_stopping_threshold:\n",
    "            self.best_metric = metric\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "        \n",
    "        if self.patience_counter >= self.early_stopping_patience:\n",
    "            print(f\"Early stopping at step {state.global_step}\")\n",
    "            control.should_training_stop = True\n",
    "\n",
    "# Custom classification head with mean pooling\n",
    "class CustomWhisperForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.whisper = WhisperModel(config)\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.hidden_size = config.d_model\n",
    "        self.num_labels = config.num_labels\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        # Pass input through Whisper encoder\n",
    "        encoder_outputs = self.whisper.encoder(input_values)\n",
    "        hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply pooling\n",
    "        pooled_output = self.pooling(hidden_states.transpose(1, 2)).squeeze(-1)\n",
    "        \n",
    "        # Ensure the pooled output has the correct shape\n",
    "        if pooled_output.dim() == 1:\n",
    "            pooled_output = pooled_output.unsqueeze(0)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return (loss, logits) if loss is not None else (logits,)\n",
    "\n",
    "# Custom data collator for Whisper\n",
    "class DataCollatorForWhisper:\n",
    "    def __call__(self, features):\n",
    "        input_values = torch.stack([f[\"input_values\"] for f in features])\n",
    "        labels = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "        return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# Extend the Trainer class\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_loss_model_dir = \"./results/best_model_loss_2layer_versuch2\"\n",
    "        self.best_accuracy_model_dir = \"./results/best_model_accuracy_versuch2\"\n",
    "        os.makedirs(self.best_loss_model_dir, exist_ok=True)\n",
    "        os.makedirs(self.best_accuracy_model_dir, exist_ok=True)\n",
    "        self.best_eval_loss = float(\"inf\")\n",
    "        self.best_eval_accuracy = 0.0\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        eval_metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        current_eval_loss = eval_metrics[\"eval_loss\"]\n",
    "        current_eval_accuracy = eval_metrics[\"eval_accuracy\"]\n",
    "        \n",
    "        if current_eval_loss < self.best_eval_loss:\n",
    "            self.best_eval_loss = current_eval_loss\n",
    "            self.save_model(self.best_loss_model_dir)\n",
    "            print(f\"Saved best model according to eval_loss: {self.best_eval_loss}\")\n",
    "\n",
    "        if current_eval_accuracy > self.best_eval_accuracy:\n",
    "            self.best_eval_accuracy = current_eval_accuracy\n",
    "            self.save_model(self.best_accuracy_model_dir)\n",
    "            print(f\"Saved best model according to eval_accuracy: {self.best_eval_accuracy}\")\n",
    "\n",
    "        return eval_metrics\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_values = inputs.get(\"input_values\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(input_values=input_values, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest the number of layers\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 24)\n",
    "    \n",
    "    # Load the model configuration with the suggested number of layers\n",
    "    config = WhisperConfig.from_pretrained(\"openai/whisper-large\", num_labels=num_speakers)\n",
    "    config.num_hidden_layers = num_layers\n",
    "    model = CustomWhisperForSequenceClassification(config)\n",
    "    \n",
    "    # Apply the number of hidden layers correctly\n",
    "    model.whisper.encoder.layers = torch.nn.ModuleList(model.whisper.encoder.layers[:num_layers])\n",
    "    \n",
    "    # Transfer the model to the correct device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        group_by_length=False,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=3,\n",
    "        save_steps=logging_steps,\n",
    "        eval_steps=eval_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=1e-5,\n",
    "        save_total_limit=2,\n",
    "        no_cuda=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,  # lower eval_loss is better\n",
    "        save_strategy=\"steps\"  # or \"epoch\" if you prefer to save every epoch\n",
    "    )\n",
    "    \n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validate_dataset,\n",
    "        data_collator=DataCollatorForWhisper(),\n",
    "        tokenizer=processor,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[SaveMetricsCallback(), EarlyStoppingCallback(early_stopping_patience=50)]\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = trainer.evaluate(validate_dataset)\n",
    "    return metrics['eval_loss']\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=11)\n",
    "\n",
    "result_file = os.path.join(log_dir, \"OptunaResult.txt\")\n",
    "with open(result_file, \"w\") as f:\n",
    "    f.write(\"Best trial:\\n\")\n",
    "    trial = study.best_trial\n",
    "    f.write(f\"  Value: {trial.value}\\n\")\n",
    "    f.write(\"  Params:\\n\")\n",
    "    for key, value in trial.params.items():\n",
    "        f.write(f\"    {key}: {value}\\n\")\n",
    "    \n",
    "    f.write(\"\\nAll trials:\\n\")\n",
    "    for i, trial in enumerate(study.trials):\n",
    "        f.write(f\"Trial {i}:\\n\")\n",
    "        f.write(f\"  Value: {trial.value}\\n\")\n",
    "        f.write(\"  Params:\\n\")\n",
    "        for key, value in trial.params.items():\n",
    "            f.write(f\"    {key}: {value}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"Operation finished.\\n\")\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperConfig, WhisperModel\n",
    "from safetensors.torch import load_file as safe_load\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the custom classification head with mean pooling for Whisper\n",
    "class CustomWhisperForSequenceClassification(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.whisper = WhisperModel(config)\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.hidden_size = config.d_model\n",
    "        self.num_labels = config.num_labels\n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.num_labels)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        # Pass input through Whisper encoder\n",
    "        encoder_outputs = self.whisper.encoder(input_values)\n",
    "        hidden_states = encoder_outputs.last_hidden_state\n",
    "        \n",
    "        # Apply pooling\n",
    "        pooled_output = self.pooling(hidden_states.transpose(1, 2)).squeeze(-1)\n",
    "        \n",
    "        # Ensure the pooled output has the correct shape\n",
    "        if pooled_output.dim() == 1:\n",
    "            pooled_output = pooled_output.unsqueeze(0)\n",
    "        \n",
    "        # Pass through classifier\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "        \n",
    "        return (loss, logits) if loss is not None else (logits,)\n",
    "\n",
    "# Path to the fine-tuned model weights file\n",
    "model_path = \"/home/rag/experimental_trial/results/best_model_loss_whisper_110/model.safetensors\"\n",
    "\n",
    "# Load the pre-trained Whisper large model and processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n",
    "\n",
    "# Load the model configuration\n",
    "config = WhisperConfig.from_pretrained(\"openai/whisper-large\", num_labels=110)  # Adjust num_labels as needed\n",
    "\n",
    "# Initialize the custom model with the configuration\n",
    "model = CustomWhisperForSequenceClassification(config)\n",
    "\n",
    "# Load the model weights from safetensors file\n",
    "state_dict = safe_load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "def check_directories_exist(directory, layer_indices):\n",
    "    \"\"\"Prüft, ob die benötigten Verzeichnisse für jede Schicht bereits existieren.\"\"\"\n",
    "    all_exist = True\n",
    "    for index in layer_indices:\n",
    "        layer_dir = os.path.join(directory, f\"layer_{index}\")\n",
    "        if not os.path.exists(layer_dir):\n",
    "            all_exist = False\n",
    "            break\n",
    "    return all_exist\n",
    "\n",
    "def load_audio_files(input_directory, output_directory, layer_indices=[-1]):\n",
    "    \"\"\"Lädt alle MP3-Dateien im angegebenen Verzeichnis und extrahiert die Repräsentationen aus den spezifizierten Schichten.\"\"\"\n",
    "    for filename in tqdm(os.listdir(input_directory)):\n",
    "        if filename.endswith(\".mp3\"):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            audio, sr = librosa.load(file_path, sr=16000)\n",
    "            inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "            input_values = inputs[\"input_features\"].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.whisper.encoder(input_values, output_hidden_states=True)\n",
    "                for index in layer_indices:\n",
    "                    hidden_states = outputs.hidden_states[index]\n",
    "                    mean_pooled_hidden_states = hidden_states.mean(dim=1)  # Mean Pooling über die Zeitdimension\n",
    "                    # creating sub directory for each layer in output directory\n",
    "                    layer_dir = os.path.join(output_directory, f\"layer_{index}\")\n",
    "                    os.makedirs(layer_dir, exist_ok=True)\n",
    "                    save_path = os.path.join(layer_dir, f\"{os.path.splitext(filename)[0]}_layer_{index}.npy\")\n",
    "                    np.save(save_path, mean_pooled_hidden_states.cpu().numpy())\n",
    "\n",
    "def process_audio_directory(input_base_directory, output_base_directory, layer_indices=range(25)):\n",
    "    \"\"\"Verarbeitet Audio-Dateien in den angegebenen Verzeichnissen und speichert die Ergebnisse im Zielverzeichnis.\"\"\"\n",
    "    for d in os.listdir(input_base_directory):\n",
    "        input_dir_path = os.path.join(input_base_directory, d)\n",
    "        output_dir_path = os.path.join(output_base_directory, d)\n",
    "        if os.path.isdir(input_dir_path) and not check_directories_exist(output_dir_path, layer_indices):\n",
    "            load_audio_files(input_dir_path, output_dir_path, layer_indices)\n",
    "\n",
    "input_directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_backup\")\n",
    "output_directory_path = os.path.expanduser(\"/home/rag/experimental_trial/data/all_speakers_whisper_finetuned2\")\n",
    "process_audio_directory(input_directory_path, output_directory_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
